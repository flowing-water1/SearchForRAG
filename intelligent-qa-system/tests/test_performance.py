"""
æ€§èƒ½æµ‹è¯•å¥—ä»¶
æµ‹è¯•ç³»ç»Ÿçš„æ€§èƒ½ã€è´Ÿè½½å’Œå‹åŠ›æ‰¿å—èƒ½åŠ›
ä½¿ç”¨ HKUDS/LightRAG æ¡†æ¶è¿›è¡Œæµ‹è¯•
"""

import unittest
import time
import concurrent.futures
import statistics
from unittest.mock import Mock, patch
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.advanced_logging import setup_logger, record_metric, get_performance_logger
from src.utils.error_handling import ErrorHandler, RetryHandler
from src.utils.helpers import validate_query, safe_json_parse, calculate_confidence


class TestLoggingPerformance(unittest.TestCase):
    """æ—¥å¿—è®°å½•æ€§èƒ½æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.logger = setup_logger("performance_test")
        self.perf_logger = get_performance_logger("performance_test")
    
    def test_basic_logging_performance(self):
        """æµ‹è¯•åŸºç¡€æ—¥å¿—è®°å½•æ€§èƒ½"""
        iterations = 1000
        
        # æµ‹è¯•æ—¥å¿—è®°å½•é€Ÿåº¦
        start_time = time.time()
        for i in range(iterations):
            self.logger.info(f"Test message {i}")
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        self.assertLess(duration, 5.0)  # åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        self.assertGreater(rate, 100)   # æ¯ç§’è‡³å°‘100æ¡æ—¥å¿—
        
        print(f"æ—¥å¿—è®°å½•æ€§èƒ½: {rate:.2f} æ¡/ç§’")
    
    def test_performance_logging_overhead(self):
        """æµ‹è¯•æ€§èƒ½æ—¥å¿—è®°å½•å¼€é”€"""
        iterations = 100
        
        # æµ‹è¯•æ€§èƒ½æ—¥å¿—è®°å½•å¼€é”€
        start_time = time.time()
        for i in range(iterations):
            self.perf_logger.start_operation(f"test_operation_{i}")
            time.sleep(0.001)  # æ¨¡æ‹ŸçŸ­æ“ä½œ
            self.perf_logger.end_operation(success=True)
        end_time = time.time()
        
        duration = end_time - start_time
        overhead = duration - (iterations * 0.001)  # å‡å»æ¨¡æ‹Ÿæ“ä½œæ—¶é—´
        
        # éªŒè¯å¼€é”€
        self.assertLess(overhead, 1.0)  # å¼€é”€åº”è¯¥å°äº1ç§’
        print(f"æ€§èƒ½æ—¥å¿—è®°å½•å¼€é”€: {overhead:.3f}ç§’")
    
    def test_concurrent_logging_performance(self):
        """æµ‹è¯•å¹¶å‘æ—¥å¿—è®°å½•æ€§èƒ½"""
        iterations = 500
        threads = 4
        
        def log_messages(thread_id):
            logger = setup_logger(f"thread_{thread_id}")
            for i in range(iterations):
                logger.info(f"Thread {thread_id} message {i}")
        
        # æµ‹è¯•å¹¶å‘æ—¥å¿—è®°å½•
        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            futures = [executor.submit(log_messages, i) for i in range(threads)]
            concurrent.futures.wait(futures)
        end_time = time.time()
        
        duration = end_time - start_time
        total_messages = iterations * threads
        rate = total_messages / duration
        
        # éªŒè¯å¹¶å‘æ€§èƒ½
        self.assertLess(duration, 10.0)  # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
        self.assertGreater(rate, 50)     # æ¯ç§’è‡³å°‘50æ¡æ—¥å¿—
        
        print(f"å¹¶å‘æ—¥å¿—è®°å½•æ€§èƒ½: {rate:.2f} æ¡/ç§’")
    
    def test_metric_recording_performance(self):
        """æµ‹è¯•æŒ‡æ ‡è®°å½•æ€§èƒ½"""
        iterations = 1000
        
        # æµ‹è¯•æŒ‡æ ‡è®°å½•é€Ÿåº¦
        start_time = time.time()
        for i in range(iterations):
            record_metric(f"test_metric_{i % 10}", float(i))
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½
        self.assertLess(duration, 2.0)   # åº”è¯¥åœ¨2ç§’å†…å®Œæˆ
        self.assertGreater(rate, 200)    # æ¯ç§’è‡³å°‘200ä¸ªæŒ‡æ ‡
        
        print(f"æŒ‡æ ‡è®°å½•æ€§èƒ½: {rate:.2f} ä¸ª/ç§’")


class TestErrorHandlingPerformance(unittest.TestCase):
    """é”™è¯¯å¤„ç†æ€§èƒ½æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.error_handler = ErrorHandler("performance_test")
        self.retry_handler = RetryHandler(max_retries=3, backoff_factor=0.1)
    
    def test_error_handling_performance(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†æ€§èƒ½"""
        iterations = 100
        
        # æµ‹è¯•é”™è¯¯å¤„ç†é€Ÿåº¦
        start_time = time.time()
        for i in range(iterations):
            try:
                raise ValueError(f"Test error {i}")
            except Exception as e:
                self.error_handler.handle_error(e, {"iteration": i})
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½
        self.assertLess(duration, 5.0)   # åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        self.assertGreater(rate, 10)     # æ¯ç§’è‡³å°‘10ä¸ªé”™è¯¯å¤„ç†
        
        print(f"é”™è¯¯å¤„ç†æ€§èƒ½: {rate:.2f} ä¸ª/ç§’")
    
    def test_retry_handler_performance(self):
        """æµ‹è¯•é‡è¯•å¤„ç†å™¨æ€§èƒ½"""
        iterations = 50
        
        def failing_function():
            if failing_function.call_count < 2:
                failing_function.call_count += 1
                raise ValueError("Temporary failure")
            failing_function.call_count = 0
            return "Success"
        
        failing_function.call_count = 0
        
        # æµ‹è¯•é‡è¯•å¤„ç†æ€§èƒ½
        start_time = time.time()
        for i in range(iterations):
            try:
                result = self.retry_handler.retry_with_backoff(failing_function)
            except Exception:
                pass
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½
        self.assertLess(duration, 10.0)  # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
        self.assertGreater(rate, 2)      # æ¯ç§’è‡³å°‘2ä¸ªé‡è¯•å¤„ç†
        
        print(f"é‡è¯•å¤„ç†æ€§èƒ½: {rate:.2f} ä¸ª/ç§’")


class TestUtilityPerformance(unittest.TestCase):
    """å·¥å…·å‡½æ•°æ€§èƒ½æµ‹è¯•"""
    
    def test_query_validation_performance(self):
        """æµ‹è¯•æŸ¥è¯¢éªŒè¯æ€§èƒ½"""
        iterations = 1000
        test_queries = [
            "è¿™æ˜¯ä¸€ä¸ªæ­£å¸¸çš„æŸ¥è¯¢",
            "What is machine learning?",
            "æœºå™¨å­¦ä¹ çš„å‘å±•å†å²å’Œæœªæ¥è¶‹åŠ¿",
            "å¦‚ä½•ä½¿ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æ",
            "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
        ]
        
        # æµ‹è¯•æŸ¥è¯¢éªŒè¯é€Ÿåº¦
        start_time = time.time()
        for i in range(iterations):
            query = test_queries[i % len(test_queries)]
            valid, error = validate_query(query)
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½
        self.assertLess(duration, 1.0)    # åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
        self.assertGreater(rate, 500)     # æ¯ç§’è‡³å°‘500ä¸ªéªŒè¯
        
        print(f"æŸ¥è¯¢éªŒè¯æ€§èƒ½: {rate:.2f} ä¸ª/ç§’")
    
    def test_json_parsing_performance(self):
        """æµ‹è¯•JSONè§£ææ€§èƒ½"""
        iterations = 1000
        test_jsons = [
            '{"key": "value"}',
            '{"name": "test", "value": 123}',
            '{"array": [1, 2, 3], "nested": {"key": "value"}}',
            '{"complex": {"deep": {"nested": "value"}}}',
            'invalid json'
        ]
        
        # æµ‹è¯•JSONè§£æé€Ÿåº¦
        start_time = time.time()
        for i in range(iterations):
            json_str = test_jsons[i % len(test_jsons)]
            result = safe_json_parse(json_str)
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½
        self.assertLess(duration, 1.0)    # åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
        self.assertGreater(rate, 500)     # æ¯ç§’è‡³å°‘500ä¸ªè§£æ
        
        print(f"JSONè§£ææ€§èƒ½: {rate:.2f} ä¸ª/ç§’")
    
    def test_confidence_calculation_performance(self):
        """æµ‹è¯•ç½®ä¿¡åº¦è®¡ç®—æ€§èƒ½"""
        iterations = 10000
        
        # æµ‹è¯•ç½®ä¿¡åº¦è®¡ç®—é€Ÿåº¦
        start_time = time.time()
        for i in range(iterations):
            confidence = calculate_confidence(
                0.8, 500, 0.9, 0.7
            )
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½
        self.assertLess(duration, 1.0)     # åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
        self.assertGreater(rate, 5000)     # æ¯ç§’è‡³å°‘5000ä¸ªè®¡ç®—
        
        print(f"ç½®ä¿¡åº¦è®¡ç®—æ€§èƒ½: {rate:.2f} ä¸ª/ç§’")


class TestConcurrencyPerformance(unittest.TestCase):
    """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
    
    def test_concurrent_error_handling(self):
        """æµ‹è¯•å¹¶å‘é”™è¯¯å¤„ç†"""
        iterations = 50
        threads = 4
        
        def handle_errors(thread_id):
            handler = ErrorHandler(f"thread_{thread_id}")
            for i in range(iterations):
                try:
                    raise ValueError(f"Error from thread {thread_id}, iteration {i}")
                except Exception as e:
                    handler.handle_error(e, {"thread": thread_id, "iteration": i})
        
        # æµ‹è¯•å¹¶å‘é”™è¯¯å¤„ç†
        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            futures = [executor.submit(handle_errors, i) for i in range(threads)]
            concurrent.futures.wait(futures)
        end_time = time.time()
        
        duration = end_time - start_time
        total_errors = iterations * threads
        rate = total_errors / duration
        
        # éªŒè¯å¹¶å‘æ€§èƒ½
        self.assertLess(duration, 10.0)   # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
        self.assertGreater(rate, 5)       # æ¯ç§’è‡³å°‘5ä¸ªé”™è¯¯å¤„ç†
        
        print(f"å¹¶å‘é”™è¯¯å¤„ç†æ€§èƒ½: {rate:.2f} ä¸ª/ç§’")
    
    def test_concurrent_utility_operations(self):
        """æµ‹è¯•å¹¶å‘å·¥å…·æ“ä½œ"""
        iterations = 100
        threads = 4
        
        def utility_operations(thread_id):
            for i in range(iterations):
                # æŸ¥è¯¢éªŒè¯
                validate_query(f"Thread {thread_id} query {i}")
                
                # JSONè§£æ
                safe_json_parse(f'{{"thread": {thread_id}, "iteration": {i}}}')
                
                # ç½®ä¿¡åº¦è®¡ç®—
                calculate_confidence(0.8, 500, 0.9, 0.7)
        
        # æµ‹è¯•å¹¶å‘å·¥å…·æ“ä½œ
        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            futures = [executor.submit(utility_operations, i) for i in range(threads)]
            concurrent.futures.wait(futures)
        end_time = time.time()
        
        duration = end_time - start_time
        total_operations = iterations * threads * 3  # 3 operations per iteration
        rate = total_operations / duration
        
        # éªŒè¯å¹¶å‘æ€§èƒ½
        self.assertLess(duration, 5.0)    # åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        self.assertGreater(rate, 100)     # æ¯ç§’è‡³å°‘100ä¸ªæ“ä½œ
        
        print(f"å¹¶å‘å·¥å…·æ“ä½œæ€§èƒ½: {rate:.2f} ä¸ª/ç§’")


class TestMemoryPerformance(unittest.TestCase):
    """å†…å­˜æ€§èƒ½æµ‹è¯•"""
    
    def test_memory_usage_stability(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨ç¨³å®šæ€§"""
        import gc
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå¤§é‡æ“ä½œ
        logger = setup_logger("memory_test")
        error_handler = ErrorHandler("memory_test")
        
        for i in range(1000):
            logger.info(f"Memory test message {i}")
            
            try:
                raise ValueError(f"Test error {i}")
            except Exception as e:
                error_handler.handle_error(e, {"iteration": i})
            
            # æ¯100æ¬¡è¿­ä»£æ£€æŸ¥å†…å­˜
            if i % 100 == 0:
                gc.collect()
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_growth = current_memory - initial_memory
                
                # å†…å­˜å¢é•¿ä¸åº”è¯¥è¶…è¿‡50MB
                self.assertLess(memory_growth, 50)
        
        # æœ€ç»ˆå†…å­˜æ£€æŸ¥
        final_memory = process.memory_info().rss / 1024 / 1024
        total_growth = final_memory - initial_memory
        
        print(f"å†…å­˜ä½¿ç”¨: åˆå§‹ {initial_memory:.2f}MB, æœ€ç»ˆ {final_memory:.2f}MB, å¢é•¿ {total_growth:.2f}MB")
        
        # æ€»å†…å­˜å¢é•¿ä¸åº”è¯¥è¶…è¿‡100MB
        self.assertLess(total_growth, 100)


class TestThroughputPerformance(unittest.TestCase):
    """ååé‡æ€§èƒ½æµ‹è¯•"""
    
    def test_system_throughput(self):
        """æµ‹è¯•ç³»ç»Ÿååé‡"""
        iterations = 100
        
        # æ¨¡æ‹Ÿå®Œæ•´çš„å¤„ç†æµç¨‹
        logger = setup_logger("throughput_test")
        error_handler = ErrorHandler("throughput_test")
        
        processing_times = []
        
        for i in range(iterations):
            start_time = time.time()
            
            # æ¨¡æ‹ŸæŸ¥è¯¢éªŒè¯
            query = f"æµ‹è¯•æŸ¥è¯¢ {i}"
            valid, error = validate_query(query)
            
            if valid:
                # æ¨¡æ‹ŸJSONå¤„ç†
                json_data = safe_json_parse(f'{{"query": "{query}", "iteration": {i}}}')
                
                # æ¨¡æ‹Ÿç½®ä¿¡åº¦è®¡ç®—
                confidence = calculate_confidence(0.8, 500, 0.9, 0.7)
                
                # è®°å½•æ—¥å¿—
                logger.info(f"å¤„ç†æŸ¥è¯¢ {i}: ç½®ä¿¡åº¦ {confidence:.2f}")
            else:
                # å¤„ç†é”™è¯¯
                error_handler.handle_error(ValueError(error), {"query": query})
            
            processing_time = time.time() - start_time
            processing_times.append(processing_time)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_time = statistics.mean(processing_times)
        throughput = 1.0 / avg_time
        
        # éªŒè¯ååé‡
        self.assertLess(avg_time, 0.1)     # å¹³å‡å¤„ç†æ—¶é—´åº”è¯¥å°äº100ms
        self.assertGreater(throughput, 10)  # æ¯ç§’è‡³å°‘å¤„ç†10ä¸ªè¯·æ±‚
        
        print(f"ç³»ç»Ÿååé‡: {throughput:.2f} è¯·æ±‚/ç§’")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time*1000:.2f}ms")


class TestStressTest(unittest.TestCase):
    """å‹åŠ›æµ‹è¯•"""
    
    def test_high_load_stability(self):
        """æµ‹è¯•é«˜è´Ÿè½½ç¨³å®šæ€§"""
        iterations = 1000
        threads = 8
        
        def stress_worker(worker_id):
            logger = setup_logger(f"stress_worker_{worker_id}")
            error_handler = ErrorHandler(f"stress_worker_{worker_id}")
            
            for i in range(iterations):
                try:
                    # æ¨¡æ‹Ÿå¤æ‚æ“ä½œ
                    validate_query(f"Worker {worker_id} query {i}")
                    safe_json_parse(f'{{"worker": {worker_id}, "iteration": {i}}}')
                    calculate_confidence(0.8, 500, 0.9, 0.7)
                    
                    logger.info(f"Worker {worker_id} processed item {i}")
                    
                    # å¶å°”æŠ›å‡ºé”™è¯¯
                    if i % 50 == 0:
                        raise ValueError(f"Stress test error from worker {worker_id}")
                        
                except Exception as e:
                    error_handler.handle_error(e, {"worker": worker_id, "iteration": i})
        
        # æ‰§è¡Œå‹åŠ›æµ‹è¯•
        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
            futures = [executor.submit(stress_worker, i) for i in range(threads)]
            concurrent.futures.wait(futures)
        end_time = time.time()
        
        duration = end_time - start_time
        total_operations = iterations * threads
        rate = total_operations / duration
        
        # éªŒè¯å‹åŠ›æµ‹è¯•ç»“æœ
        self.assertLess(duration, 60.0)    # åº”è¯¥åœ¨60ç§’å†…å®Œæˆ
        self.assertGreater(rate, 50)       # æ¯ç§’è‡³å°‘50ä¸ªæ“ä½œ
        
        print(f"å‹åŠ›æµ‹è¯•æ€§èƒ½: {rate:.2f} æ“ä½œ/ç§’")
        print(f"æ€»è€—æ—¶: {duration:.2f}ç§’")


def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestLoggingPerformance,
        TestErrorHandlingPerformance,
        TestUtilityPerformance,
        TestConcurrencyPerformance,
        TestMemoryPerformance,
        TestThroughputPerformance,
        TestStressTest
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    print("\n" + "=" * 60)
    if result.wasSuccessful():
        print("ğŸ‰ æ‰€æœ‰æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ éƒ¨åˆ†æ€§èƒ½æµ‹è¯•å¤±è´¥")
    
    return result.wasSuccessful()


if __name__ == "__main__":
    run_performance_tests()