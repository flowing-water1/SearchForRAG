# æµ‹è¯•æ¨¡å—æŠ€æœ¯æ–‡æ¡£

> è¿”å› [é¡¹ç›®æ¦‚è§ˆæ–‡æ¡£](../../TECHNICAL_REFERENCE.md)

## ğŸ“ ç›¸å…³æ–‡æ¡£å¯¼èˆª
- **[æ ¸å¿ƒæ¨¡å—æ–‡æ¡£](../src/core/README.md)** - æŸ¥çœ‹æµ‹è¯•æ¶µç›–çš„æ ¸å¿ƒåŠŸèƒ½
- **[å·¥ä½œæµèŠ‚ç‚¹æ–‡æ¡£](../src/agents/README.md)** - æŸ¥çœ‹æµ‹è¯•çš„å·¥ä½œæµèŠ‚ç‚¹
- **[å·¥å…·æ¨¡å—æ–‡æ¡£](../src/utils/README.md)** - æŸ¥çœ‹æµ‹è¯•çš„å·¥å…·å’Œå®¢æˆ·ç«¯
- **[é¡¹ç›®æ ¹ç›®å½•](../../TECHNICAL_REFERENCE.md)** - è¿”å›é¡¹ç›®å®Œæ•´æ¦‚è§ˆ

## ğŸ”— æµ‹è¯•ä¸ç³»ç»Ÿé›†æˆ
- [é…ç½®ç®¡ç†æµ‹è¯•](../src/core/README.md#1-é…ç½®ç®¡ç†ç³»ç»Ÿ-configpy) - é…ç½®åŠ è½½å’ŒéªŒè¯æµ‹è¯•
- [å·¥ä½œæµæµ‹è¯•](../src/core/README.md#3-åŸºç¡€å·¥ä½œæµ-workflowpy) - ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•
- [é”™è¯¯å¤„ç†æµ‹è¯•](../src/utils/README.md#3-é”™è¯¯å¤„ç†æ¡†æ¶-error_handlingpy) - å¼‚å¸¸å’Œæ¢å¤æœºåˆ¶æµ‹è¯•
- [æ€§èƒ½ç›‘æ§æµ‹è¯•](../src/utils/README.md#4-ç³»ç»Ÿç›‘æ§-system_monitoringpy) - ç³»ç»Ÿå¥åº·å’Œæ€§èƒ½æµ‹è¯•

---

## æ¨¡å—æ¦‚è¿°

æµ‹è¯•æ¨¡å— (tests/) æä¾›äº†æ™ºèƒ½é—®ç­”ç³»ç»Ÿçš„å…¨é¢æµ‹è¯•è¦†ç›–ï¼ŒåŒ…æ‹¬å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•ã€‚æµ‹è¯•å¥—ä»¶ç¡®ä¿ç³»ç»Ÿçš„å¯é æ€§ã€æ€§èƒ½å’Œæ­£ç¡®æ€§ã€‚

### æ¨¡å—ç»“æ„
```
tests/
â”œâ”€â”€ __init__.py                # æµ‹è¯•æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ core/                      # æ ¸å¿ƒæ¨¡å—æµ‹è¯•
â”‚   â””â”€â”€ __init__.py           
â”œâ”€â”€ agents/                    # å·¥ä½œæµèŠ‚ç‚¹æµ‹è¯•
â”‚   â””â”€â”€ __init__.py           
â”œâ”€â”€ utils/                     # å·¥å…·æ¨¡å—æµ‹è¯•
â”‚   â””â”€â”€ __init__.py           
â”œâ”€â”€ test_comprehensive.py     # ç»¼åˆæµ‹è¯•å¥—ä»¶
â”œâ”€â”€ test_performance.py       # æ€§èƒ½æµ‹è¯•å¥—ä»¶
â””â”€â”€ test_workflow.py          # å·¥ä½œæµæµ‹è¯•å¥—ä»¶
```

### æµ‹è¯•åˆ†ç±»
- **å•å…ƒæµ‹è¯•**: æµ‹è¯•å•ä¸ªç»„ä»¶å’Œå‡½æ•°
- **é›†æˆæµ‹è¯•**: æµ‹è¯•æ¨¡å—é—´çš„åä½œ
- **æ€§èƒ½æµ‹è¯•**: æµ‹è¯•ç³»ç»Ÿæ€§èƒ½å’Œè´Ÿè½½
- **ç«¯åˆ°ç«¯æµ‹è¯•**: æµ‹è¯•å®Œæ•´çš„å·¥ä½œæµç¨‹

---

## æµ‹è¯•æ–‡ä»¶è¯¦è§£

### 1. ç»¼åˆæµ‹è¯•å¥—ä»¶ (test_comprehensive.py)

**ä¸»è¦åŠŸèƒ½**: æä¾›å®Œæ•´çš„å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•è¦†ç›–ã€‚

#### æµ‹è¯•ç±»æ¶æ„

```python
#!/usr/bin/env python3
"""
æ™ºèƒ½é—®ç­”ç³»ç»Ÿç»¼åˆæµ‹è¯•å¥—ä»¶
æä¾›å®Œæ•´çš„å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•
"""

import unittest
import asyncio
import tempfile
import json
import time
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path
import sys

# æµ‹è¯•å¯¼å…¥
from src.utils.advanced_logging import (
    setup_logger, get_performance_logger, get_error_tracker,
    audit_log, record_metric, get_system_metrics
)
from src.utils.error_handling import (
    SystemError, ConfigurationError, DatabaseError, NetworkError,
    ErrorHandler, RetryHandler, CircuitBreaker
)
from src.utils.helpers import (
    validate_query, safe_json_parse, calculate_confidence,
    format_sources, generate_session_id, deep_merge_dicts
)
from src.utils.system_monitoring import (
    HealthStatus, HealthCheck, SystemMonitor, ApplicationHealthChecker
)
```

#### é«˜çº§æ—¥å¿—æµ‹è¯•

```python
class TestAdvancedLogging(unittest.TestCase):
    """é«˜çº§æ—¥å¿—è®°å½•æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.logger = setup_logger("test_logging")
        self.perf_logger = get_performance_logger("test_performance")
        self.error_tracker = get_error_tracker("test_error")
    
    def test_basic_logging(self):
        """æµ‹è¯•åŸºç¡€æ—¥å¿—åŠŸèƒ½"""
        # æµ‹è¯•ä¸åŒçº§åˆ«çš„æ—¥å¿—
        self.logger.debug("æµ‹è¯•è°ƒè¯•ä¿¡æ¯")
        self.logger.info("æµ‹è¯•ä¿¡æ¯æ—¥å¿—")
        self.logger.warning("æµ‹è¯•è­¦å‘Šæ—¥å¿—")
        self.logger.error("æµ‹è¯•é”™è¯¯æ—¥å¿—")
        
        # éªŒè¯æ—¥å¿—å™¨é…ç½®
        self.assertIsNotNone(self.logger)
        self.assertEqual(self.logger.name, "test_logging")
    
    def test_performance_logging(self):
        """æµ‹è¯•æ€§èƒ½æ—¥å¿—åŠŸèƒ½"""
        # å¼€å§‹æ€§èƒ½ç›‘æ§
        self.perf_logger.start_operation("test_operation")
        
        # æ¨¡æ‹Ÿæ“ä½œ
        time.sleep(0.1)
        
        # ç»“æŸæ€§èƒ½ç›‘æ§
        duration = self.perf_logger.end_operation(success=True)
        
        # éªŒè¯æ€§èƒ½è®°å½•
        self.assertIsNotNone(duration)
        self.assertGreater(duration, 0.1)
        self.assertLess(duration, 0.2)
    
    def test_error_tracking(self):
        """æµ‹è¯•é”™è¯¯è¿½è¸ªåŠŸèƒ½"""
        test_error = ValueError("æµ‹è¯•é”™è¯¯")
        
        # è¿½è¸ªé”™è¯¯
        error_data = self.error_tracker.track_error(
            test_error, 
            context={"operation": "test"}
        )
        
        # éªŒè¯é”™è¯¯æ•°æ®
        self.assertEqual(error_data["error_type"], "ValueError")
        self.assertEqual(error_data["error_message"], "æµ‹è¯•é”™è¯¯")
        self.assertEqual(error_data["error_count"], 1)
        self.assertIn("context", error_data)
    
    def test_structured_logging(self):
        """æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—"""
        # è®°å½•ç»“æ„åŒ–æ—¥å¿—
        self.logger.info(
            "ç»“æ„åŒ–æ—¥å¿—æµ‹è¯•",
            extra={
                "user_id": "test_user",
                "session_id": "test_session",
                "metrics": {"operation": "test", "duration": 0.1}
            }
        )
        
        # éªŒè¯æ—¥å¿—è®°å½•æˆåŠŸ
        self.assertTrue(True)  # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œåˆ™æµ‹è¯•é€šè¿‡
    
    def test_audit_logging(self):
        """æµ‹è¯•å®¡è®¡æ—¥å¿—"""
        # è®°å½•å®¡è®¡äº‹ä»¶
        audit_log(
            "test_action",
            user_id="test_user",
            details={"resource": "test_resource", "operation": "read"}
        )
        
        # éªŒè¯å®¡è®¡æ—¥å¿—è®°å½•
        self.assertTrue(True)  # éªŒè¯æ— å¼‚å¸¸å‘ç”Ÿ
    
    def test_metrics_recording(self):
        """æµ‹è¯•æŒ‡æ ‡è®°å½•"""
        # è®°å½•ä¸åŒç±»å‹çš„æŒ‡æ ‡
        record_metric("test_counter", 1, type="counter")
        record_metric("test_gauge", 0.85, type="gauge")
        record_metric("test_timer", 1.5, type="timer")
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        metrics = get_system_metrics()
        
        # éªŒè¯æŒ‡æ ‡è®°å½•
        self.assertIsInstance(metrics, dict)
```

#### é”™è¯¯å¤„ç†æµ‹è¯•

```python
class TestErrorHandling(unittest.TestCase):
    """é”™è¯¯å¤„ç†æ¡†æ¶æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.error_handler = ErrorHandler()
        self.retry_handler = RetryHandler()
        self.circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=1)
    
    def test_custom_exceptions(self):
        """æµ‹è¯•è‡ªå®šä¹‰å¼‚å¸¸"""
        # æµ‹è¯•åŸºç¡€ç³»ç»Ÿå¼‚å¸¸
        with self.assertRaises(SystemError):
            raise SystemError(
                "æµ‹è¯•ç³»ç»Ÿé”™è¯¯",
                error_code="TEST_001",
                category=ErrorCategory.SYSTEM,
                severity=ErrorSeverity.HIGH
            )
        
        # æµ‹è¯•é…ç½®é”™è¯¯
        with self.assertRaises(ConfigurationError):
            raise ConfigurationError(
                "é…ç½®æ–‡ä»¶ç¼ºå¤±",
                recovery_suggestions=["æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„", "é‡æ–°ç”Ÿæˆé…ç½®"]
            )
        
        # æµ‹è¯•æ•°æ®åº“é”™è¯¯
        with self.assertRaises(DatabaseError):
            raise DatabaseError(
                "æ•°æ®åº“è¿æ¥å¤±è´¥",
                details={"host": "localhost", "port": 5432}
            )
    
    def test_error_handler(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†å™¨"""
        test_error = ValueError("æµ‹è¯•å€¼é”™è¯¯")
        
        # å¤„ç†é”™è¯¯
        result = self.error_handler.handle_error(
            test_error,
            context={"operation": "test_operation"}
        )
        
        # éªŒè¯å¤„ç†ç»“æœ
        self.assertIsInstance(result, dict)
        self.assertIn("error_type", result)
        self.assertIn("error_message", result)
        self.assertIn("user_message", result)
        self.assertIn("recovery_suggestions", result)
    
    def test_retry_mechanism(self):
        """æµ‹è¯•é‡è¯•æœºåˆ¶"""
        call_count = 0
        
        def flaky_function():
            nonlocal call_count
            call_count += 1
            if call_count < 3:
                raise NetworkError("ä¸´æ—¶ç½‘ç»œé”™è¯¯")
            return "æˆåŠŸ"
        
        # æ‰§è¡Œé‡è¯•
        result = self.retry_handler.retry_with_backoff(
            flaky_function,
            max_retries=3,
            backoff_factor=0.1
        )
        
        # éªŒè¯é‡è¯•ç»“æœ
        self.assertEqual(result, "æˆåŠŸ")
        self.assertEqual(call_count, 3)
    
    def test_circuit_breaker(self):
        """æµ‹è¯•ç†”æ–­å™¨"""
        failure_count = 0
        
        def failing_function():
            nonlocal failure_count
            failure_count += 1
            raise NetworkError("æœåŠ¡ä¸å¯ç”¨")
        
        # æµ‹è¯•ç†”æ–­å™¨ä¿æŠ¤
        for i in range(5):
            try:
                self.circuit_breaker.call(failing_function)
            except Exception:
                pass
        
        # éªŒè¯ç†”æ–­å™¨çŠ¶æ€
        self.assertEqual(self.circuit_breaker.state, CircuitBreakerState.OPEN)
        self.assertEqual(failure_count, 3)  # åªè°ƒç”¨åˆ°ç†”æ–­é˜ˆå€¼
    
    def test_error_decorators(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†è£…é¥°å™¨"""
        
        @handle_errors(reraise=False, return_on_error="é»˜è®¤å€¼")
        def risky_function():
            raise ValueError("æ•…æ„å‡ºé”™")
        
        # æµ‹è¯•è£…é¥°å™¨é”™è¯¯å¤„ç†
        result = risky_function()
        self.assertEqual(result, "é»˜è®¤å€¼")
        
        @retry_on_failure(max_retries=2, backoff_factor=0.1)
        def unstable_function():
            if not hasattr(unstable_function, 'call_count'):
                unstable_function.call_count = 0
            unstable_function.call_count += 1
            
            if unstable_function.call_count < 2:
                raise NetworkError("ç½‘ç»œä¸ç¨³å®š")
            return "ç¨³å®š"
        
        # æµ‹è¯•é‡è¯•è£…é¥°å™¨
        result = unstable_function()
        self.assertEqual(result, "ç¨³å®š")
```

#### è¾…åŠ©å‡½æ•°æµ‹è¯•

```python
class TestHelperFunctions(unittest.TestCase):
    """è¾…åŠ©å‡½æ•°æµ‹è¯•"""
    
    def test_query_validation(self):
        """æµ‹è¯•æŸ¥è¯¢éªŒè¯"""
        # æµ‹è¯•æœ‰æ•ˆæŸ¥è¯¢
        valid_query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
        is_valid, message = validate_query(valid_query)
        self.assertTrue(is_valid)
        self.assertIsNone(message)
        
        # æµ‹è¯•æ— æ•ˆæŸ¥è¯¢
        invalid_queries = [
            "",              # ç©ºæŸ¥è¯¢
            "   ",           # åªæœ‰ç©ºæ ¼
            "ab",            # å¤ªçŸ­
            "a" * 1001,      # å¤ªé•¿
            "<script>alert('xss')</script>"  # æ¶æ„å†…å®¹
        ]
        
        for query in invalid_queries:
            is_valid, message = validate_query(query)
            self.assertFalse(is_valid)
            self.assertIsNotNone(message)
    
    def test_json_parsing(self):
        """æµ‹è¯•JSONè§£æ"""
        # æµ‹è¯•æœ‰æ•ˆJSON
        valid_json = '{"key": "value", "number": 42}'
        result = safe_json_parse(valid_json)
        self.assertEqual(result["key"], "value")
        self.assertEqual(result["number"], 42)
        
        # æµ‹è¯•æ— æ•ˆJSON
        invalid_json = '{"key": value}'  # ç¼ºå°‘å¼•å·
        result = safe_json_parse(invalid_json, default={"error": True})
        self.assertEqual(result["error"], True)
        
        # æµ‹è¯•ç©ºå­—ç¬¦ä¸²
        result = safe_json_parse("", default={})
        self.assertEqual(result, {})
    
    def test_confidence_calculation(self):
        """æµ‹è¯•ç½®ä¿¡åº¦è®¡ç®—"""
        # æµ‹è¯•é«˜ç½®ä¿¡åº¦åœºæ™¯
        confidence = calculate_confidence(
            retrieval_score=0.9,
            content_length=1200,
            entity_coverage=0.8,
            mode_effectiveness=0.9
        )
        self.assertGreater(confidence, 0.8)
        self.assertLessEqual(confidence, 1.0)
        
        # æµ‹è¯•ä½ç½®ä¿¡åº¦åœºæ™¯
        confidence = calculate_confidence(
            retrieval_score=0.3,
            content_length=100,
            entity_coverage=0.2,
            mode_effectiveness=0.4
        )
        self.assertLess(confidence, 0.5)
    
    def test_source_formatting(self):
        """æµ‹è¯•ä¿¡æ¯æºæ ¼å¼åŒ–"""
        sources = [
            {
                "type": "lightrag_knowledge",
                "mode": "hybrid",
                "confidence": 0.85
            },
            {
                "type": "web_search",
                "title": "AIå‘å±•è¶‹åŠ¿",
                "url": "https://example.com",
                "score": 0.92
            }
        ]
        
        formatted = format_sources(sources)
        self.assertIn("æœ¬åœ°çŸ¥è¯†åº“", formatted)
        self.assertIn("ç½‘ç»œæœç´¢", formatted)
        self.assertIn("hybridæ¨¡å¼", formatted)
        self.assertIn("AIå‘å±•è¶‹åŠ¿", formatted)
    
    def test_id_generation(self):
        """æµ‹è¯•IDç”Ÿæˆ"""
        # æµ‹è¯•ä¼šè¯IDç”Ÿæˆ
        session_id1 = generate_session_id()
        session_id2 = generate_session_id()
        
        self.assertIsInstance(session_id1, str)
        self.assertIsInstance(session_id2, str)
        self.assertNotEqual(session_id1, session_id2)  # åº”è¯¥æ˜¯å”¯ä¸€çš„
        
        # æµ‹è¯•æŸ¥è¯¢IDç”Ÿæˆ
        query_id1 = generate_query_id()
        query_id2 = generate_query_id()
        
        self.assertIsInstance(query_id1, str)
        self.assertIsInstance(query_id2, str)
        self.assertNotEqual(query_id1, query_id2)
    
    def test_dict_operations(self):
        """æµ‹è¯•å­—å…¸æ“ä½œ"""
        # æµ‹è¯•æ·±åº¦åˆå¹¶
        dict1 = {"a": 1, "b": {"c": 2, "d": 3}}
        dict2 = {"b": {"d": 4, "e": 5}, "f": 6}
        
        merged = deep_merge_dicts(dict1, dict2)
        
        self.assertEqual(merged["a"], 1)
        self.assertEqual(merged["b"]["c"], 2)
        self.assertEqual(merged["b"]["d"], 4)  # è¢«è¦†ç›–
        self.assertEqual(merged["b"]["e"], 5)  # æ–°å¢
        self.assertEqual(merged["f"], 6)       # æ–°å¢
```

#### ç³»ç»Ÿç›‘æ§æµ‹è¯•

```python
class TestSystemMonitoring(unittest.TestCase):
    """ç³»ç»Ÿç›‘æ§æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.monitor = SystemMonitor()
        self.health_checker = ApplicationHealthChecker()
    
    def test_health_check_creation(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥åˆ›å»º"""
        health_check = HealthCheck(
            name="test_check",
            status=HealthStatus.HEALTHY,
            message="æµ‹è¯•å¥åº·æ£€æŸ¥",
            details={"test": True},
            timestamp=datetime.now(),
            execution_time=0.1
        )
        
        self.assertEqual(health_check.name, "test_check")
        self.assertEqual(health_check.status, HealthStatus.HEALTHY)
        self.assertIn("test", health_check.details)
    
    def test_monitor_registration(self):
        """æµ‹è¯•ç›‘æ§å™¨æ³¨å†Œ"""
        def dummy_check():
            return HealthCheck(
                name="dummy",
                status=HealthStatus.HEALTHY,
                message="è™šæ‹Ÿæ£€æŸ¥",
                details={},
                timestamp=datetime.now(),
                execution_time=0.0
            )
        
        # æ³¨å†Œå¥åº·æ£€æŸ¥
        self.monitor.register_health_check("dummy_check", dummy_check)
        
        # éªŒè¯æ³¨å†Œ
        self.assertIn("dummy_check", self.monitor.health_checks)
    
    def test_system_metrics_collection(self):
        """æµ‹è¯•ç³»ç»ŸæŒ‡æ ‡æ”¶é›†"""
        # æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›†
        self.monitor._collect_system_metrics()
        
        # éªŒè¯æŒ‡æ ‡æ”¶é›†
        self.assertGreater(len(self.monitor.metrics_history), 0)
    
    def test_alert_thresholds(self):
        """æµ‹è¯•å‘Šè­¦é˜ˆå€¼"""
        # è®¾ç½®å‘Šè­¦é˜ˆå€¼
        self.monitor.set_alert_threshold("test_metric", 80.0)
        
        # éªŒè¯é˜ˆå€¼è®¾ç½®
        self.assertEqual(self.monitor.alert_thresholds["test_metric"], 80.0)
        
        # æµ‹è¯•é˜ˆå€¼æ£€æŸ¥
        self.monitor._check_thresholds("test_metric", 85.0)  # è¶…è¿‡é˜ˆå€¼
        self.monitor._check_thresholds("test_metric", 75.0)  # æœªè¶…è¿‡é˜ˆå€¼
```

### 2. æ€§èƒ½æµ‹è¯•å¥—ä»¶ (test_performance.py)

**ä¸»è¦åŠŸèƒ½**: æµ‹è¯•ç³»ç»Ÿçš„æ€§èƒ½ã€è´Ÿè½½å’Œå‹åŠ›æ‰¿å—èƒ½åŠ›ã€‚

#### æ—¥å¿—æ€§èƒ½æµ‹è¯•

```python
class TestLoggingPerformance(unittest.TestCase):
    """æ—¥å¿—è®°å½•æ€§èƒ½æµ‹è¯•"""
    
    def test_basic_logging_performance(self):
        """æµ‹è¯•åŸºç¡€æ—¥å¿—è®°å½•æ€§èƒ½"""
        iterations = 1000
        logger = setup_logger("performance_test")
        
        # æµ‹è¯•æ—¥å¿—è®°å½•é€Ÿåº¦
        start_time = time.time()
        for i in range(iterations):
            logger.info(f"Test message {i}")
        end_time = time.time()
        
        duration = end_time - start_time
        rate = iterations / duration
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        self.assertLess(duration, 5.0)  # åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        self.assertGreater(rate, 100)   # æ¯ç§’è‡³å°‘100æ¡æ—¥å¿—
        
        print(f"æ—¥å¿—è®°å½•æ€§èƒ½: {rate:.2f} æ¡/ç§’")
    
    def test_concurrent_logging_performance(self):
        """æµ‹è¯•å¹¶å‘æ—¥å¿—è®°å½•æ€§èƒ½"""
        import concurrent.futures
        import threading
        
        def log_worker(worker_id, iterations):
            logger = setup_logger(f"worker_{worker_id}")
            for i in range(iterations):
                logger.info(f"Worker {worker_id} message {i}")
        
        # æµ‹è¯•å¹¶å‘æ—¥å¿—è®°å½•
        workers = 5
        iterations_per_worker = 200
        
        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [
                executor.submit(log_worker, i, iterations_per_worker)
                for i in range(workers)
            ]
            concurrent.futures.wait(futures)
        end_time = time.time()
        
        total_logs = workers * iterations_per_worker
        duration = end_time - start_time
        rate = total_logs / duration
        
        print(f"å¹¶å‘æ—¥å¿—æ€§èƒ½: {rate:.2f} æ¡/ç§’ ({workers} çº¿ç¨‹)")
        self.assertGreater(rate, 500)  # å¹¶å‘æ—¶åº”è¯¥æ›´é«˜
```

#### é”™è¯¯å¤„ç†æ€§èƒ½æµ‹è¯•

```python
class TestErrorHandlingPerformance(unittest.TestCase):
    """é”™è¯¯å¤„ç†æ€§èƒ½æµ‹è¯•"""
    
    def test_exception_handling_overhead(self):
        """æµ‹è¯•å¼‚å¸¸å¤„ç†å¼€é”€"""
        iterations = 1000
        
        # æµ‹è¯•æ­£å¸¸æ‰§è¡Œæ—¶é—´
        start_time = time.time()
        for _ in range(iterations):
            result = "normal operation"
        normal_time = time.time() - start_time
        
        # æµ‹è¯•å¼‚å¸¸å¤„ç†æ—¶é—´
        start_time = time.time()
        for _ in range(iterations):
            try:
                raise ValueError("test error")
            except ValueError:
                result = "error handled"
        exception_time = time.time() - start_time
        
        # è®¡ç®—å¼€é”€
        overhead = (exception_time - normal_time) / normal_time
        
        print(f"å¼‚å¸¸å¤„ç†å¼€é”€: {overhead:.2%}")
        self.assertLess(overhead, 10.0)  # å¼€é”€åº”è¯¥å°äº10å€
    
    def test_retry_performance(self):
        """æµ‹è¯•é‡è¯•æœºåˆ¶æ€§èƒ½"""
        retry_handler = RetryHandler()
        call_count = 0
        
        def flaky_function():
            nonlocal call_count
            call_count += 1
            if call_count < 3:
                raise NetworkError("temporary error")
            return "success"
        
        # æµ‹è¯•é‡è¯•æ€§èƒ½
        start_time = time.time()
        result = retry_handler.retry_with_backoff(
            flaky_function,
            max_retries=3,
            backoff_factor=0.01  # å¿«é€Ÿé‡è¯•ç”¨äºæµ‹è¯•
        )
        duration = time.time() - start_time
        
        self.assertEqual(result, "success")
        self.assertLess(duration, 1.0)  # åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
```

#### å†…å­˜å’Œèµ„æºä½¿ç”¨æµ‹è¯•

```python
class TestResourceUsage(unittest.TestCase):
    """èµ„æºä½¿ç”¨æµ‹è¯•"""
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import gc
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss
        
        # æ‰§è¡Œå¤§é‡æ“ä½œ
        large_data = []
        for i in range(10000):
            large_data.append({
                "id": i,
                "data": f"large data string {i}" * 100
            })
        
        peak_memory = process.memory_info().rss
        memory_increase = peak_memory - initial_memory
        
        # æ¸…ç†
        del large_data
        gc.collect()
        
        final_memory = process.memory_info().rss
        memory_released = peak_memory - final_memory
        
        print(f"å†…å­˜å¢åŠ : {memory_increase / 1024 / 1024:.2f} MB")
        print(f"å†…å­˜é‡Šæ”¾: {memory_released / 1024 / 1024:.2f} MB")
        
        # éªŒè¯å†…å­˜ç®¡ç†
        self.assertGreater(memory_released, memory_increase * 0.8)  # è‡³å°‘é‡Šæ”¾80%
```

### 3. å·¥ä½œæµæµ‹è¯•å¥—ä»¶ (test_workflow.py)

**ä¸»è¦åŠŸèƒ½**: æµ‹è¯•å®Œæ•´çš„å·¥ä½œæµç¨‹æ‰§è¡Œã€‚

#### ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•

```python
class TestWorkflowExecution(unittest.TestCase):
    """å·¥ä½œæµæ‰§è¡Œæµ‹è¯•"""
    
    @patch('src.utils.lightrag_client.query_lightrag_sync')
    @patch('src.core.config.config')
    def test_complete_workflow(self, mock_config, mock_lightrag):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµæ‰§è¡Œ"""
        # æ¨¡æ‹Ÿé…ç½®
        mock_config.LLM_API_KEY = "test_key"
        mock_config.TAVILY_API_KEY = "test_key"
        
        # æ¨¡æ‹ŸLightRAGå“åº”
        mock_lightrag.return_value = {
            "success": True,
            "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯...",
            "mode": "local"
        }
        
        # æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œ
        from src.core.workflow import get_workflow
        
        workflow = get_workflow()
        initial_state = {
            "user_query": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "session_id": "test_session"
        }
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = workflow.invoke(initial_state)
        
        # éªŒè¯ç»“æœ
        self.assertIn("final_answer", result)
        self.assertIn("answer_confidence", result)
        self.assertIn("sources", result)
        self.assertGreater(len(result["final_answer"]), 0)
    
    def test_workflow_error_handling(self):
        """æµ‹è¯•å·¥ä½œæµé”™è¯¯å¤„ç†"""
        # æµ‹è¯•å„ç§é”™è¯¯åœºæ™¯
        error_scenarios = [
            {"user_query": "", "expected_error": "æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"},
            {"user_query": "ab", "expected_error": "æŸ¥è¯¢å¤ªçŸ­"},
            {"user_query": "<script>", "expected_error": "æŸ¥è¯¢åŒ…å«å¯ç–‘å†…å®¹"}
        ]
        
        for scenario in error_scenarios:
            with self.subTest(scenario=scenario):
                # æ‰§è¡Œå·¥ä½œæµ
                result = self.execute_workflow_with_error_handling(scenario["user_query"])
                
                # éªŒè¯é”™è¯¯å¤„ç†
                if "error" in result:
                    self.assertIn("error", result)
```

---

## æµ‹è¯•è¿è¡Œå’Œç®¡ç†

### æµ‹è¯•æ‰§è¡Œå‘½ä»¤

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
python -m pytest tests/test_comprehensive.py

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»
python -m pytest tests/test_comprehensive.py::TestAdvancedLogging

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–¹æ³•
python -m pytest tests/test_comprehensive.py::TestAdvancedLogging::test_basic_logging

# å¸¦è¯¦ç»†è¾“å‡ºè¿è¡Œæµ‹è¯•
python -m pytest tests/ -v

# å¸¦è¦†ç›–ç‡è¿è¡Œæµ‹è¯•
python -m pytest tests/ --cov=src

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
python -m pytest tests/ --cov=src --cov-report=html
```

### æµ‹è¯•é…ç½®

**pytest.ini é…ç½®æ–‡ä»¶**
```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
markers =
    slow: marks tests as slow
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    performance: marks tests as performance tests
```

**æµ‹è¯•ç¯å¢ƒé…ç½®**
```python
# tests/conftest.py
import pytest
import tempfile
from pathlib import Path

@pytest.fixture
def temp_dir():
    """ä¸´æ—¶ç›®å½•fixture"""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)

@pytest.fixture
def mock_config():
    """æ¨¡æ‹Ÿé…ç½®fixture"""
    return {
        "LLM_API_KEY": "test_key",
        "LLM_MODEL": "gpt-4",
        "TAVILY_API_KEY": "test_key",
        "LOG_LEVEL": "DEBUG"
    }

@pytest.fixture(autouse=True)
def setup_test_environment(mock_config):
    """è‡ªåŠ¨è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
    os.environ.update(mock_config)
    yield
    # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    for key in mock_config:
        os.environ.pop(key, None)
```

---

## æµ‹è¯•è¦†ç›–ç‡å’Œè´¨é‡

### è¦†ç›–ç‡ç›®æ ‡

| æ¨¡å— | ç›®æ ‡è¦†ç›–ç‡ | å½“å‰è¦†ç›–ç‡ | çŠ¶æ€ |
|------|------------|------------|------|
| src/core/ | 90% | 85% | ğŸŸ¡ è¿›è¡Œä¸­ |
| src/agents/ | 85% | 80% | ğŸŸ¡ è¿›è¡Œä¸­ |
| src/utils/ | 95% | 92% | ğŸŸ¢ è‰¯å¥½ |
| src/frontend/ | 70% | 65% | ğŸŸ¡ è¿›è¡Œä¸­ |

### è´¨é‡æ£€æŸ¥

**ä»£ç è´¨é‡æ£€æŸ¥**
```bash
# è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥
flake8 tests/
pylint tests/
black tests/ --check
mypy tests/
```

**æµ‹è¯•è´¨é‡æŒ‡æ ‡**
- æµ‹è¯•è¦†ç›–ç‡ > 85%
- æµ‹è¯•æ‰§è¡Œæ—¶é—´ < 30ç§’
- æµ‹è¯•æˆåŠŸç‡ = 100%
- æµ‹è¯•ç»´æŠ¤æ€§è¯„åˆ† > 8/10

---

## æŒç»­é›†æˆå’Œè‡ªåŠ¨åŒ–

### GitHub Actions å·¥ä½œæµ

```yaml
# .github/workflows/tests.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      run: |
        pytest tests/ --cov=src --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v2
      with:
        file: ./coverage.xml
```

### æµ‹è¯•è‡ªåŠ¨åŒ–è„šæœ¬

```bash
#!/bin/bash
# test_runner.sh - è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

set -e

echo "ğŸ§ª å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶..."

# 1. è¿è¡Œå•å…ƒæµ‹è¯•
echo "ğŸ“ è¿è¡Œå•å…ƒæµ‹è¯•..."
python -m pytest tests/test_comprehensive.py -v

# 2. è¿è¡Œæ€§èƒ½æµ‹è¯•
echo "âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•..."
python -m pytest tests/test_performance.py -v

# 3. è¿è¡Œå·¥ä½œæµæµ‹è¯•
echo "ğŸ”„ è¿è¡Œå·¥ä½œæµæµ‹è¯•..."
python -m pytest tests/test_workflow.py -v

# 4. ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
echo "ğŸ“Š ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š..."
python -m pytest tests/ --cov=src --cov-report=html --cov-report=term

# 5. è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥
echo "ğŸ” è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥..."
flake8 src/ tests/
black src/ tests/ --check

echo "âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼"
```

---

## æµ‹è¯•æœ€ä½³å®è·µ

### æµ‹è¯•ç¼–å†™æŒ‡å—

1. **æµ‹è¯•å‘½åè§„èŒƒ**
   - æµ‹è¯•ç±»: `Test + è¢«æµ‹è¯•çš„ç±»/æ¨¡å—å`
   - æµ‹è¯•æ–¹æ³•: `test_ + å…·ä½“æµ‹è¯•å†…å®¹`
   - æ¸…æ™°æè¿°æµ‹è¯•æ„å›¾

2. **æµ‹è¯•ç»“æ„**
   - **Arrange**: å‡†å¤‡æµ‹è¯•æ•°æ®å’Œç¯å¢ƒ
   - **Act**: æ‰§è¡Œè¢«æµ‹è¯•çš„æ“ä½œ
   - **Assert**: éªŒè¯ç»“æœ

3. **Mockå’Œä¾èµ–ç®¡ç†**
   - ä½¿ç”¨Mockéš”ç¦»å¤–éƒ¨ä¾èµ–
   - é¿å…çœŸå®çš„ç½‘ç»œè°ƒç”¨å’Œæ•°æ®åº“æ“ä½œ
   - ä¿æŒæµ‹è¯•çš„å¯é‡å¤æ€§

4. **æµ‹è¯•æ•°æ®ç®¡ç†**
   - ä½¿ç”¨fixtureç®¡ç†æµ‹è¯•æ•°æ®
   - é¿å…ç¡¬ç¼–ç æµ‹è¯•æ•°æ®
   - ç¡®ä¿æµ‹è¯•æ•°æ®çš„æ¸…ç†

### æ€§èƒ½æµ‹è¯•æŒ‡å—

1. **åŸºå‡†æµ‹è¯•**
   - å»ºç«‹æ€§èƒ½åŸºå‡†çº¿
   - ç›‘æ§æ€§èƒ½å›å½’
   - è®¾ç½®åˆç†çš„æ€§èƒ½é˜ˆå€¼

2. **è´Ÿè½½æµ‹è¯•**
   - æ¨¡æ‹ŸçœŸå®è´Ÿè½½åœºæ™¯
   - æµ‹è¯•ç³»ç»Ÿçš„æ‰¿è½½èƒ½åŠ›
   - è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ

3. **å‹åŠ›æµ‹è¯•**
   - æµ‹è¯•ç³»ç»Ÿçš„æé™
   - éªŒè¯é”™è¯¯å¤„ç†æœºåˆ¶
   - è¯„ä¼°ç³»ç»Ÿæ¢å¤èƒ½åŠ›

---

## æ•…éšœæ’é™¤

### å¸¸è§æµ‹è¯•é—®é¢˜

**æµ‹è¯•ç¯å¢ƒé—®é¢˜**
```python
# æ£€æŸ¥æµ‹è¯•ç¯å¢ƒé…ç½®
def check_test_environment():
    required_vars = ["LLM_API_KEY", "TEST_DATABASE_URL"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        pytest.skip(f"ç¼ºå°‘ç¯å¢ƒå˜é‡: {missing_vars}")
```

**ä¾èµ–é—®é¢˜**
```python
# æ£€æŸ¥ä¾èµ–å¯ç”¨æ€§
def check_dependencies():
    try:
        import lightrag
        import streamlit
        return True
    except ImportError as e:
        pytest.skip(f"ç¼ºå°‘ä¾èµ–: {e}")
```

**å¼‚æ­¥æµ‹è¯•é—®é¢˜**
```python
# å¼‚æ­¥æµ‹è¯•å¤„ç†
@pytest.mark.asyncio
async def test_async_function():
    result = await async_function()
    assert result is not None
```

### æµ‹è¯•è°ƒè¯•æŠ€å·§

**è¯¦ç»†æ—¥å¿—**
```python
import logging
logging.basicConfig(level=logging.DEBUG)

def test_with_debug():
    logger = logging.getLogger(__name__)
    logger.debug("æµ‹è¯•å¼€å§‹")
    # æµ‹è¯•é€»è¾‘
    logger.debug("æµ‹è¯•ç»“æŸ")
```

**æ–­ç‚¹è°ƒè¯•**
```python
def test_with_breakpoint():
    result = some_function()
    breakpoint()  # Python 3.7+
    assert result == expected
```

---

**ğŸ“ è¯´æ˜**: æœ¬æ–‡æ¡£æä¾›äº†æ™ºèƒ½é—®ç­”ç³»ç»Ÿæµ‹è¯•æ¨¡å—çš„å…¨é¢æŒ‡å—ã€‚æµ‹è¯•å¥—ä»¶ç¡®ä¿ç³»ç»Ÿçš„å¯é æ€§ã€æ€§èƒ½å’Œæ­£ç¡®æ€§ï¼Œæ”¯æŒæŒç»­é›†æˆå’Œè´¨é‡ä¿è¯ã€‚