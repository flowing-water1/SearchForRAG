"""
å¢å¼ºç‰ˆå·¥ä½œæµç¼–æ’ - é›†æˆç»¼åˆé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""

import asyncio
import time
import uuid
import sys
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# --- ç›¸å¯¹å¯¼å…¥å¤„ç† ---
# ä½¿å¾—è„šæœ¬å¯ä»¥ç›´æ¥è¿è¡Œè¿›è¡Œæµ‹è¯•
try:
    from .state import AgentState
    from .config import config
    from ..agents.query_analysis import query_analysis_node
    from ..agents.strategy_route import strategy_route_node
    from ..agents.local_search import local_search_node
    from ..agents.global_search import global_search_node
    from ..agents.hybrid_search import hybrid_search_node
    from ..agents.quality_assessment import quality_assessment_node
    from ..agents.web_search import web_search_node
    from ..agents.answer_generation import answer_generation_node
    from ..utils.simple_logger import get_simple_logger
    from ..utils.lightrag_client import initialize_lightrag
except ImportError:
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    current_dir = Path(__file__).parent
    project_root = current_dir.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from src.core.state import AgentState
    from src.core.config import config
    from src.agents.query_analysis import query_analysis_node
    from src.agents.strategy_route import strategy_route_node
    from src.agents.local_search import local_search_node
    from src.agents.global_search import global_search_node
    from src.agents.hybrid_search import hybrid_search_node
    from src.agents.quality_assessment import quality_assessment_node
    from src.agents.web_search import web_search_node
    from src.agents.answer_generation import answer_generation_node
    # Use direct import to avoid circular dependency
    import logging
    import sys
    
    def get_simple_logger(name: str, level: str = "INFO") -> logging.Logger:
        """Simple logger function to avoid circular imports"""
        logger = logging.getLogger(name)
        if logger.handlers:
            return logger
        
        log_level = getattr(logging, level.upper(), logging.INFO)
        logger.setLevel(log_level)
        
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(log_level)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.propagate = False
        
        return logger
    
    # Import lightrag initialization function
    try:
        from src.utils.lightrag_client import initialize_lightrag
    except ImportError:
        # Fallback if lightrag_client has issues
        def initialize_lightrag():
            print("Warning: LightRAG initialization skipped due to import issues")
            pass

# æ—¥å¿—è®°å½•
logger = get_simple_logger(__name__)

# å®šä¹‰æ”¯æŒçš„èŠ‚ç‚¹åŠå…¶æè¿°
SUPPORTED_NODES = [
    {"name": "query_analysis", "description": "æŸ¥è¯¢åˆ†æèŠ‚ç‚¹", "function": "åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®å®šæŸ¥è¯¢ç±»å‹å¹¶é€‰æ‹©æœ€ä½³LightRAGæ¨¡å¼"},
    {"name": "strategy_route", "description": "ç­–ç•¥è·¯ç”±èŠ‚ç‚¹", "function": "æ ¹æ®æŸ¥è¯¢åˆ†æç»“æœå†³å®šæ£€ç´¢è·¯å¾„"},
    {"name": "local_search", "description": "æœ¬åœ°æ£€ç´¢èŠ‚ç‚¹", "function": "ä¸“é—¨å¤„ç†äº‹å®æ€§æŸ¥è¯¢çš„å‘é‡æ£€ç´¢"},
    {"name": "global_search", "description": "å…¨å±€æ£€ç´¢èŠ‚ç‚¹", "function": "ä¸“é—¨å¤„ç†å…³ç³»æ€§æŸ¥è¯¢çš„å›¾æ£€ç´¢"},
    {"name": "hybrid_search", "description": "æ··åˆæ£€ç´¢èŠ‚ç‚¹", "function": "ä¸“é—¨å¤„ç†å¤æ‚æŸ¥è¯¢çš„æ··åˆæ£€ç´¢"},
    {"name": "quality_assessment", "description": "è´¨é‡è¯„ä¼°èŠ‚ç‚¹", "function": "è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢è¡¥å……"},
    {"name": "web_search", "description": "ç½‘ç»œæœç´¢èŠ‚ç‚¹", "function": "å½“æœ¬åœ°ä¿¡æ¯ä¸è¶³æ—¶ï¼Œä»ç½‘ç»œè·å–è¡¥å……ä¿¡æ¯"},
    {"name": "answer_generation", "description": "ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹", "function": "æ•´åˆæœ¬åœ°å’Œç½‘ç»œä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"}
]

class IntelligentQAWorkflow:
    """
    ä¸€ä¸ªç”¨äºæ™ºèƒ½é—®ç­”ï¼ˆIntelligent QAï¼‰çš„ç¼–æ’å·¥ä½œæµï¼Œ
    è¯¥å·¥ä½œæµç»“åˆäº† LightRAG å’Œ LangGraph çš„åŠŸèƒ½ï¼Œ
    å®ç°äº†ä»æŸ¥è¯¢åˆ†æåˆ°ç­”æ¡ˆç”Ÿæˆçš„å®Œæ•´æµç¨‹ã€‚
    """
    def __init__(self, workflow_id: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½é—®ç­”å·¥ä½œæµã€‚
        
        Args:
            workflow_id (Optional[str]): å·¥ä½œæµçš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä¼šè‡ªåŠ¨ç”Ÿæˆã€‚
        """
        self.workflow_id = workflow_id or f"intelligent-qa-{uuid.uuid4()}"
        self.version = "2.0.0"
        self._graph = self._build_graph()
        self.compiled_graph = self._compile_graph()
        logger.info(f"âœ… æ™ºèƒ½é—®ç­”å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ {self.workflow_id}")
        logger.info("æ³¨æ„ï¼šLightRAGå°†åœ¨ç¬¬ä¸€æ¬¡æŸ¥è¯¢æ—¶å»¶è¿Ÿåˆå§‹åŒ–")

    def _build_graph(self) -> StateGraph:
        """
        æ„å»ºå·¥ä½œæµçš„å›¾ç»“æ„ï¼ŒåŒ…æ‹¬æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ã€‚
        
        Returns:
            StateGraph: æ„å»ºå®Œæˆçš„å·¥ä½œæµå›¾ã€‚
        """
        graph = StateGraph(AgentState)
        self._add_nodes(graph)
        self._add_edges(graph)
        return graph

    def _add_nodes(self, graph: StateGraph):
        """
        å‘å›¾ä¸­æ·»åŠ æ‰€æœ‰å¤„ç†èŠ‚ç‚¹ã€‚
        """
        logger.info("æ·»åŠ å·¥ä½œæµèŠ‚ç‚¹...")
        graph.add_node("query_analysis", query_analysis_node)
        graph.add_node("strategy_route", strategy_route_node)
        graph.add_node("local_search", local_search_node)
        graph.add_node("global_search", global_search_node)
        graph.add_node("hybrid_search", hybrid_search_node)
        graph.add_node("quality_assessment", quality_assessment_node)
        graph.add_node("web_search", web_search_node)
        graph.add_node("answer_generation", answer_generation_node)
        logger.info("æ‰€æœ‰èŠ‚ç‚¹å·²æ·»åŠ å®Œæˆ")

    def _add_edges(self, graph: StateGraph):
        """
        å®šä¹‰èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å’Œæ¡ä»¶è·¯ç”±ã€‚
        """
        logger.info("é…ç½®æ™ºèƒ½æ£€ç´¢å·¥ä½œæµè·¯ç”±...")
        graph.set_entry_point("query_analysis")
        graph.add_edge("query_analysis", "strategy_route")

        graph.add_conditional_edges(
            "strategy_route",
            self._route_to_search_node,
            {
                "local_search": "local_search",
                "global_search": "global_search",
                "hybrid_search": "hybrid_search"
            }
        )

        graph.add_edge("local_search", "quality_assessment")
        graph.add_edge("global_search", "quality_assessment")
        graph.add_edge("hybrid_search", "quality_assessment")

        graph.add_conditional_edges(
            "quality_assessment",
            self._should_use_web_search,
            {
                "web_search": "web_search",
                "answer_generation": "answer_generation"
            }
        )

        graph.add_edge("web_search", "answer_generation")
        graph.add_edge("answer_generation", END)
        logger.info("æ™ºèƒ½æ£€ç´¢å·¥ä½œæµè·¯ç”±é…ç½®å®Œæˆ")

    def _route_to_search_node(self, state: AgentState) -> str:
        """æ ¹æ®ç­–ç•¥è·¯ç”±èŠ‚ç‚¹çš„å†³ç­–é€‰æ‹©ä¸‹ä¸€ä¸ªæ£€ç´¢èŠ‚ç‚¹ã€‚"""
        route_decision = state.get("lightrag_mode")
        logger.info(f"ğŸ”€ ç­–ç•¥è·¯ç”±: {state.get('query_type')} â†’ {route_decision} â†’ {route_decision}_search")
        if route_decision == "local":
            return "local_search"
        elif route_decision == "global":
            return "global_search"
        elif route_decision == "hybrid":
            return "hybrid_search"
        else:
            logger.warning(f"æœªçŸ¥çš„è·¯ç”±å†³ç­– '{route_decision}', é»˜è®¤ä½¿ç”¨ local_searchã€‚")
            return "local_search"

    def _should_use_web_search(self, state: AgentState) -> str:
        """
        æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœå†³å®šæ˜¯å¦éœ€è¦è¿›è¡Œç½‘ç»œæœç´¢ã€‚
        """
        if state.get("need_web_search", False):
            logger.info("ğŸ” è´¨é‡è¯„ä¼°åˆ¤å®šéœ€è¦ç½‘ç»œæœç´¢è¡¥å……")
            return "web_search"
        else:
            logger.info("âœ… è´¨é‡è¯„ä¼°è®¤ä¸ºæœ¬åœ°ä¿¡æ¯å·²è¶³å¤Ÿ")
            return "answer_generation"

    def _compile_graph(self):
        """
        ç¼–è¯‘å·¥ä½œæµå›¾ï¼Œä½¿å…¶å¯æ‰§è¡Œã€‚
        """
        logger.info("ç¼–è¯‘å·¥ä½œæµ...")
        try:
            # ä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹æ¥æ”¯æŒæµå¼å¤„ç†å’Œä¼šè¯ç®¡ç†
            checkpointer = MemorySaver()
            compiled_graph = self._graph.compile(checkpointer=checkpointer)
            logger.info("âœ… å·¥ä½œæµç¼–è¯‘æˆåŠŸ")
            # å¯é€‰ï¼šç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡ä»¥ä¾›è°ƒè¯•
            try:
                image_bytes = compiled_graph.get_graph().draw_mermaid_png()
                with open("graph.png", "wb") as f:
                    f.write(image_bytes)
                logger.info("âœ… å·¥ä½œæµå¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜: graph.png")
            except Exception as viz_error:
                logger.warning(f"âš ï¸  æ— æ³•ç”Ÿæˆå·¥ä½œæµå¯è§†åŒ–å›¾ç‰‡: {viz_error}")
            return compiled_graph
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¼–è¯‘å¤±è´¥: {e}")
            raise

    def query(self, query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰§è¡ŒæŸ¥è¯¢ã€‚
        """
        return asyncio.run(self.query_async(query_text, config))

    async def query_async(self, query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        å¼‚æ­¥æ‰§è¡ŒæŸ¥è¯¢ã€‚
        """
        start_time = time.time()
        query_id = f"query-{uuid.uuid4()}"
        thread_id = (config or {}).get("configurable", {}).get("thread_id", f"thread-{uuid.uuid4()}")

        initial_state = {
            "user_query": query_text,
            "session_id": thread_id,
            "query_id": query_id
        }

        run_config = {"configurable": {"thread_id": thread_id}}
        
        logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢ {query_id}: {query_text[:100]}...")
        final_state = await self.compiled_graph.ainvoke(initial_state, config=run_config)
        
        execution_time = time.time() - start_time
        logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ {query_id} ({execution_time:.2f}s)")
        
        # æ•´ç†è·¯ç”±è·¯å¾„
        route_taken = []
        
        # å°è¯•å¤šç§æ–¹å¼æå–è·¯ç”±ä¿¡æ¯
        logger.info(f"è°ƒè¯•ï¼šfinal_stateé”®: {list(final_state.keys())}")
        
        # æ–¹æ³•1ï¼šä»messagesä¸­æå–
        if 'messages' in final_state and final_state['messages']:
            for message in final_state['messages']:
                if hasattr(message, 'name') and message.name:
                    route_taken.append(message.name)
        
        # æ–¹æ³•2ï¼šä»æ‰§è¡Œå†å²ä¸­æå–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'execution_history' in final_state:
            route_taken.extend(final_state['execution_history'])
        
        # æ–¹æ³•3ï¼šä»å…¶ä»–å¯èƒ½çš„å­—æ®µæå–
        for key in ['node_sequence', 'path_taken', 'execution_path']:
            if key in final_state and final_state[key]:
                route_taken.extend(final_state[key])
        
        # å¦‚æœä»ç„¶ä¸ºç©ºï¼Œå°è¯•ä»å·¥ä½œæµå†³ç­–ä¸­æ¨æ–­
        if not route_taken:
            if final_state.get('lightrag_mode') == 'local':
                route_taken = ['query_analysis', 'strategy_route', 'local_search', 'quality_assessment']
            elif final_state.get('lightrag_mode') == 'global':
                route_taken = ['query_analysis', 'strategy_route', 'global_search', 'quality_assessment']
            elif final_state.get('lightrag_mode') == 'hybrid':
                route_taken = ['query_analysis', 'strategy_route', 'hybrid_search', 'quality_assessment']
            
            # æ·»åŠ æœ€ç»ˆæ­¥éª¤
            if final_state.get('need_web_search'):
                route_taken.append('web_search')
            route_taken.append('answer_generation')
        
        logger.info(f"è°ƒè¯•ï¼šæå–çš„è·¯ç”±: {route_taken}")

        # ç®€åŒ–æœ€ç»ˆè¾“å‡º
        return {
            "answer": final_state.get("final_answer"),
            "sources": final_state.get("sources"),
            "quality_score": final_state.get("confidence_score"),
            "answer_confidence": final_state.get("answer_confidence"),
            "execution_time": execution_time,
            "query_id": query_id,
            "route_taken": route_taken
        }

    def query_stream(self, query_text: str, config: Optional[Dict] = None):
        """
        æµå¼æ‰§è¡ŒæŸ¥è¯¢ã€‚
        """
        return asyncio.run(self.query_stream_async(query_text, config))

    async def query_stream_async(self, query_text: str, config: Optional[Dict] = None):
        """
        å¼‚æ­¥æµå¼æ‰§è¡ŒæŸ¥è¯¢ã€‚
        """
        thread_id = (config or {}).get("configurable", {}).get("thread_id", f"thread-{uuid.uuid4()}")
        run_config = {"configurable": {"thread_id": thread_id}}
        initial_state = {"user_query": query_text, "session_id": thread_id}
        
        async for chunk in self.compiled_graph.astream(initial_state, run_config):
            yield chunk

# --- å…¨å±€å®ä¾‹å’Œä¾¿æ·å‡½æ•° ---

_workflow_instance: Optional[IntelligentQAWorkflow] = None

def get_workflow() -> IntelligentQAWorkflow:
    """è·å–å…¨å±€å·¥ä½œæµå®ä¾‹ (å•ä¾‹æ¨¡å¼)ã€‚"""
    global _workflow_instance
    if _workflow_instance is None:
        logger.info("åˆ›å»ºæ–°çš„å…¨å±€å·¥ä½œæµå®ä¾‹...")
        _workflow_instance = IntelligentQAWorkflow()
    return _workflow_instance

def query(query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„åŒæ­¥æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return workflow.query(query_text, config)

async def query_async(query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„å¼‚æ­¥æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return await workflow.query_async(query_text, config)

def query_stream(query_text: str, config: Optional[Dict] = None):
    """ä¾¿æ·çš„æµå¼æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return workflow.query_stream(query_text, config)
    
def get_workflow_info() -> Dict[str, Any]:
    """è·å–å·¥ä½œæµçš„å…ƒæ•°æ®ä¿¡æ¯ã€‚"""
    workflow = get_workflow()
    return {
        "id": workflow.workflow_id,
        "version": workflow.version,
        "nodes": SUPPORTED_NODES,
        "graph_type": "StateGraph",
        "framework": "LangGraph"
    }

if __name__ == '__main__':
    # ç”¨äºç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    print("è¿è¡Œå·¥ä½œæµæµ‹è¯•...")
    print("âœ… ä½¿ç”¨å·²é¢„å¤„ç†çš„LightRAGæ•°æ®è¿›è¡Œæµ‹è¯•")

    # åŸºäºdocsç›®å½•å†…å®¹çš„æµ‹è¯•æŸ¥è¯¢ï¼ˆæ•°æ®å·²é¢„å¤„ç†ï¼‰
    test_queries = [
        {
            "query": "OpenAIåœ¨2024å¹´ç­¹é›†äº†å¤šå°‘èµ„é‡‘ï¼Ÿ",
            "expected_strategy": "local_search",
            "description": "äº‹å®æ€§æŸ¥è¯¢æµ‹è¯• - OpenAIèèµ„"
        },
        {
            "query": "Amazonå¯¹Anthropicæ€»å…±æŠ•èµ„äº†å¤šå°‘é’±ï¼Ÿ",
            "expected_strategy": "local_search", 
            "description": "äº‹å®æ€§æŸ¥è¯¢æµ‹è¯• - AmazonæŠ•èµ„"
        },
        {
            "query": "Amazonå’ŒAnthropicçš„åˆä½œå…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
            "expected_strategy": "global_search", 
            "description": "å…³ç³»æ€§æŸ¥è¯¢æµ‹è¯• - åˆä½œå…³ç³»"
        },
        {
            "query": "Databricksçš„æœ€æ–°ä¼°å€¼æ˜¯å¤šå°‘ï¼Ÿ",
            "expected_strategy": "local_search",
            "description": "äº‹å®æ€§æŸ¥è¯¢æµ‹è¯• - Databricksä¼°å€¼"
        },
        {
            "query": "åˆ†æè¿™äº›AIå…¬å¸çš„èèµ„æƒ…å†µå’ŒæŠ•èµ„å…³ç³»",
            "expected_strategy": "hybrid_search",
            "description": "å¤æ‚æŸ¥è¯¢æµ‹è¯• - èèµ„åˆ†æ"
        },
        {
            "query": "è¿™äº›å…¬å¸ä¹‹é—´å­˜åœ¨ä»€ä¹ˆæŠ•èµ„å’Œåˆä½œå…³ç³»ï¼Ÿ",
            "expected_strategy": "global_search",
            "description": "å…³ç³»æ€§æŸ¥è¯¢æµ‹è¯• - æŠ•èµ„å…³ç³»"
        }
    ]
    
    print("\n=== å¤šç­–ç•¥æ£€ç´¢æµ‹è¯• ===")
    for i, test_case in enumerate(test_queries, 1):
        print(f"\n--- æµ‹è¯• {i}: {test_case['description']} ---")
        print(f"æŸ¥è¯¢: {test_case['query']}")
        print(f"æœŸæœ›ç­–ç•¥: {test_case['expected_strategy']}")
        
        try:
            result = query(test_case['query'])
            
            # åˆ†æè·¯ç”±ç»“æœ
            route_taken = result.get('route_taken', [])
            actual_strategy = "æœªçŸ¥"
            
            if 'local_search' in route_taken:
                actual_strategy = "local_search"
            elif 'global_search' in route_taken:
                actual_strategy = "global_search"
            elif 'hybrid_search' in route_taken:
                actual_strategy = "hybrid_search"
            
            print(f"å®é™…ç­–ç•¥: {actual_strategy}")
            print(f"æ‰§è¡Œæ—¶é—´: {result.get('execution_time', 0):.2f}s")
            print(f"ç½®ä¿¡åº¦: {result.get('quality_score', 'N/A')}")
            print(f"å®Œæ•´è·¯ç”±: {' -> '.join(route_taken)}")
            
            # æ˜¾ç¤ºç­”æ¡ˆçš„å‰200ä¸ªå­—ç¬¦
            answer = result.get('answer', 'æ— ç­”æ¡ˆ')
            print(f"ç­”æ¡ˆæ‘˜è¦: {answer[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæœŸæœ›
            if actual_strategy == test_case['expected_strategy']:
                print("âœ… è·¯ç”±ç­–ç•¥ç¬¦åˆé¢„æœŸ")
            else:
                print("âŒ è·¯ç”±ç­–ç•¥ä¸ç¬¦åˆé¢„æœŸ")
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æœ¬åœ°æ£€ç´¢ï¼ˆæ–°å¢ï¼‰
            sources = result.get('sources', [])
            web_search_used = any('web_search' in str(source).lower() for source in sources) if sources else False
            need_web_search = result.get('route_taken', [])
            web_in_route = 'web_search' in need_web_search
            
            if web_in_route:
                print("âš ï¸ ä½¿ç”¨äº†ç½‘ç»œæœç´¢è¡¥å……")
                # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæœ¬åœ°æ£€ç´¢è´¨é‡ä¸è¶³
                confidence = result.get('quality_score', 0)
                print(f"   æœ¬åœ°æ£€ç´¢ç½®ä¿¡åº¦: {confidence}")
                if confidence < 0.7:
                    print("   åŸå› : æœ¬åœ°å†…å®¹è´¨é‡ä¸è¶³ï¼Œéœ€è¦ç½‘ç»œè¡¥å……")
                else:
                    print("   åŸå› : å…¶ä»–å› ç´ è§¦å‘ç½‘ç»œæœç´¢")
            else:
                print("âœ… å®Œå…¨ä½¿ç”¨æœ¬åœ°æ£€ç´¢ï¼Œæ— éœ€ç½‘ç»œæœç´¢")
            
            # ç‰¹åˆ«æ£€æŸ¥é¢„æœŸçš„å…³é”®ä¿¡æ¯æ˜¯å¦å­˜åœ¨
            answer_lower = answer.lower()
            query_lower = test_case['query'].lower()
            
            expected_keywords = {
                "openai": ["6.6", "billion", "157", "thrive"],
                "amazon": ["8", "billion", "anthropic", "4 billion"],  
                "databricks": ["62", "billion", "10 billion", "series j"],
                "åˆä½œ": ["amazon", "anthropic", "aws", "partnership"],
                "æŠ•èµ„": ["amazon", "anthropic", "thrive", "funding"]
            }
            
            found_keywords = False
            for category, keywords in expected_keywords.items():
                if category in query_lower:
                    found_count = sum(1 for kw in keywords if kw in answer_lower)
                    if found_count >= 2:  # è‡³å°‘æ‰¾åˆ°2ä¸ªå…³é”®è¯
                        print(f"âœ… æ‰¾åˆ°æœŸæœ›çš„å…³é”®ä¿¡æ¯ ({category}): {found_count}/{len(keywords)} ä¸ªå…³é”®è¯")
                        found_keywords = True
                        break
            
            if not found_keywords:
                print("âš ï¸ æœªæ‰¾åˆ°æ˜æ˜¾çš„æœŸæœ›å…³é”®ä¿¡æ¯")
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        print("-" * 80)

    # ä¿ç•™åŸæœ‰çš„å•ä¸ªæŸ¥è¯¢æµ‹è¯•
    print("\n=== åŸæœ‰æµ‹è¯•ä¿æŒä¸å˜ ===")
    original_test_query = "LangGraphæ˜¯ä»€ä¹ˆï¼Ÿå®ƒå’ŒLangChainæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
    
    # åŒæ­¥æŸ¥è¯¢æµ‹è¯•
    print("\n--- åŒæ­¥æŸ¥è¯¢æµ‹è¯• ---")
    sync_result = query(original_test_query)
    print(f"ç­”æ¡ˆ: {sync_result['answer']}")
    print(f"æ¥æº: {sync_result['sources']}")
    print(f"è·¯ç”±: {sync_result['route_taken']}")

    # å¼‚æ­¥æŸ¥è¯¢æµ‹è¯•
    async def main_async():
        print("\n--- å¼‚æ­¥æŸ¥è¯¢æµ‹è¯• ---")
        async_result = await query_async(original_test_query)
        print(f"ç­”æ¡ˆ: {async_result['answer']}")

    asyncio.run(main_async())

    # æµå¼æŸ¥è¯¢æµ‹è¯•
    print("\n--- æµå¼æŸ¥è¯¢æµ‹è¯• ---")
    async def test_stream():
        workflow = get_workflow()
        async for step in workflow.query_stream_async(original_test_query):
            print(step)
            print("-" * 20)
    
    asyncio.run(test_stream())