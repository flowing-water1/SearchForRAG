"""
å¢å¼ºç‰ˆå·¥ä½œæµç¼–æ’ - é›†æˆç»¼åˆé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""

import asyncio
import time
import uuid
import sys
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Union

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# --- ç›¸å¯¹å¯¼å…¥å¤„ç† ---
# ä½¿å¾—è„šæœ¬å¯ä»¥ç›´æ¥è¿è¡Œè¿›è¡Œæµ‹è¯•
try:
    from .state import AgentState
    from .config import config
    from ..agents.query_analysis import query_analysis_node
    from ..agents.strategy_route import strategy_route_node
    from ..agents.local_search import local_search_node
    from ..agents.global_search import global_search_node
    from ..agents.hybrid_search import hybrid_search_node
    from ..agents.quality_assessment import quality_assessment_node
    from ..agents.web_search import web_search_node
    from ..agents.answer_generation import answer_generation_node
    from ..utils.simple_logger import get_simple_logger
    from ..utils.lightrag_client import initialize_lightrag
except ImportError:
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    current_dir = Path(__file__).parent
    project_root = current_dir.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from src.core.state import AgentState
    from src.core.config import config
    from src.agents.query_analysis import query_analysis_node
    from src.agents.strategy_route import strategy_route_node
    from src.agents.local_search import local_search_node
    from src.agents.global_search import global_search_node
    from src.agents.hybrid_search import hybrid_search_node
    from src.agents.quality_assessment import quality_assessment_node
    from src.agents.web_search import web_search_node
    from src.agents.answer_generation import answer_generation_node
    # Use direct import to avoid circular dependency
    import logging
    import sys
    
    def get_simple_logger(name: str, level: str = "INFO") -> logging.Logger:
        """Simple logger function to avoid circular imports"""
        logger = logging.getLogger(name)
        if logger.handlers:
            return logger
        
        log_level = getattr(logging, level.upper(), logging.INFO)
        logger.setLevel(log_level)
        
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(log_level)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.propagate = False
        
        return logger
    
    # Import lightrag initialization function
    try:
        from src.utils.lightrag_client import initialize_lightrag
    except ImportError:
        # Fallback if lightrag_client has issues
        def initialize_lightrag():
            print("Warning: LightRAG initialization skipped due to import issues")
            pass

# æ—¥å¿—è®°å½•
logger = get_simple_logger(__name__)

# äº‹ä»¶å¾ªç¯å®‰å…¨æ‰§è¡Œå‡½æ•°
def safe_run_async(coro):
    """
    å®‰å…¨åœ°è¿è¡Œå¼‚æ­¥åç¨‹ï¼Œä½¿ç”¨nest_asyncioæ”¯æŒåµŒå¥—äº‹ä»¶å¾ªç¯ã€‚
    
    Args:
        coro: è¦æ‰§è¡Œçš„åç¨‹
        
    Returns:
        åç¨‹çš„æ‰§è¡Œç»“æœ
        
    Raises:
        Exception: å¦‚æœåç¨‹æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯
    """
    try:
        # å°è¯•å¯¼å…¥å¹¶åº”ç”¨nest_asyncioæ¥æ”¯æŒåµŒå¥—äº‹ä»¶å¾ªç¯
        import nest_asyncio
        nest_asyncio.apply()
        logger.info("âœ… å·²åº”ç”¨nest_asyncioæ”¯æŒåµŒå¥—äº‹ä»¶å¾ªç¯")
    except ImportError:
        logger.warning("âš ï¸ nest_asyncioæœªå®‰è£…ï¼Œå›é€€åˆ°çº¿ç¨‹æ± æ–¹æ³•")
        
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨çº¿ç¨‹æ± 
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
            loop = asyncio.get_running_loop()
            logger.warning("âš ï¸ æ£€æµ‹åˆ°å·²è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œ")
            
            import concurrent.futures
            
            def run_in_thread():
                try:
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        result = new_loop.run_until_complete(coro)
                        logger.info("âœ… çº¿ç¨‹æ± ä¸­çš„å¼‚æ­¥ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
                        return result
                    finally:
                        new_loop.close()
                        asyncio.set_event_loop(None)
                except Exception as e:
                    logger.error(f"âŒ çº¿ç¨‹æ± ä¸­çš„å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                    raise
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_in_thread)
                return future.result(timeout=300)
                
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨asyncio.run
            logger.info("âœ… åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡")
            return asyncio.run(coro)
    
    # å¦‚æœæˆåŠŸå¯¼å…¥nest_asyncioï¼Œç›´æ¥ä½¿ç”¨asyncio.run
    try:
        logger.info("âœ… ä½¿ç”¨nest_asyncioæ”¯æŒçš„asyncio.runæ‰§è¡Œ")
        return asyncio.run(coro)
    except Exception as e:
        logger.error(f"âŒ nest_asyncioæ‰§è¡Œå¤±è´¥: {e}")
        raise

# å®šä¹‰æ”¯æŒçš„èŠ‚ç‚¹åŠå…¶æè¿°
SUPPORTED_NODES = [
    {"name": "query_analysis", "description": "æŸ¥è¯¢åˆ†æèŠ‚ç‚¹", "function": "åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®å®šæŸ¥è¯¢ç±»å‹å¹¶é€‰æ‹©æœ€ä½³LightRAGæ¨¡å¼"},
    {"name": "strategy_route", "description": "ç­–ç•¥è·¯ç”±èŠ‚ç‚¹", "function": "æ ¹æ®æŸ¥è¯¢åˆ†æç»“æœå†³å®šæ£€ç´¢è·¯å¾„"},
    {"name": "local_search", "description": "æœ¬åœ°æ£€ç´¢èŠ‚ç‚¹", "function": "ä¸“é—¨å¤„ç†äº‹å®æ€§æŸ¥è¯¢çš„å‘é‡æ£€ç´¢"},
    {"name": "global_search", "description": "å…¨å±€æ£€ç´¢èŠ‚ç‚¹", "function": "ä¸“é—¨å¤„ç†å…³ç³»æ€§æŸ¥è¯¢çš„å›¾æ£€ç´¢"},
    {"name": "hybrid_search", "description": "æ··åˆæ£€ç´¢èŠ‚ç‚¹", "function": "ä¸“é—¨å¤„ç†å¤æ‚æŸ¥è¯¢çš„æ··åˆæ£€ç´¢"},
    {"name": "quality_assessment", "description": "è´¨é‡è¯„ä¼°èŠ‚ç‚¹", "function": "è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢è¡¥å……"},
    {"name": "web_search", "description": "ç½‘ç»œæœç´¢èŠ‚ç‚¹", "function": "å½“æœ¬åœ°ä¿¡æ¯ä¸è¶³æ—¶ï¼Œä»ç½‘ç»œè·å–è¡¥å……ä¿¡æ¯"},
    {"name": "answer_generation", "description": "ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹", "function": "æ•´åˆæœ¬åœ°å’Œç½‘ç»œä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"}
]

class IntelligentQAWorkflow:
    """
    ä¸€ä¸ªç”¨äºæ™ºèƒ½é—®ç­”ï¼ˆIntelligent QAï¼‰çš„ç¼–æ’å·¥ä½œæµï¼Œ
    è¯¥å·¥ä½œæµç»“åˆäº† LightRAG å’Œ LangGraph çš„åŠŸèƒ½ï¼Œ
    å®ç°äº†ä»æŸ¥è¯¢åˆ†æåˆ°ç­”æ¡ˆç”Ÿæˆçš„å®Œæ•´æµç¨‹ã€‚
    """
    def __init__(self, workflow_id: Optional[str] = None):
        """
        åˆå§‹åŒ–æ™ºèƒ½é—®ç­”å·¥ä½œæµã€‚
        
        Args:
            workflow_id (Optional[str]): å·¥ä½œæµçš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä¼šè‡ªåŠ¨ç”Ÿæˆã€‚
        """
        self.workflow_id = workflow_id or f"intelligent-qa-{uuid.uuid4()}"
        self.version = "2.0.0"
        self._graph = self._build_graph()
        self.compiled_graph = self._compile_graph()
        logger.info(f"âœ… æ™ºèƒ½é—®ç­”å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ {self.workflow_id}")
        logger.info("æ³¨æ„ï¼šLightRAGå°†åœ¨ç¬¬ä¸€æ¬¡æŸ¥è¯¢æ—¶å»¶è¿Ÿåˆå§‹åŒ–")

    def _build_graph(self) -> StateGraph:
        """
        æ„å»ºå·¥ä½œæµçš„å›¾ç»“æ„ï¼ŒåŒ…æ‹¬æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ã€‚
        
        Returns:
            StateGraph: æ„å»ºå®Œæˆçš„å·¥ä½œæµå›¾ã€‚
        """
        graph = StateGraph(AgentState)
        self._add_nodes(graph)
        self._add_edges(graph)
        return graph

    def _add_nodes(self, graph: StateGraph):
        """
        å‘å›¾ä¸­æ·»åŠ æ‰€æœ‰å¤„ç†èŠ‚ç‚¹ã€‚
        """
        logger.info("æ·»åŠ å·¥ä½œæµèŠ‚ç‚¹...")
        graph.add_node("query_analysis", query_analysis_node)
        graph.add_node("strategy_route", strategy_route_node)
        graph.add_node("local_search", local_search_node)
        graph.add_node("global_search", global_search_node)
        graph.add_node("hybrid_search", hybrid_search_node)
        graph.add_node("quality_assessment", quality_assessment_node)
        graph.add_node("web_search", web_search_node)
        graph.add_node("answer_generation", answer_generation_node)
        logger.info("æ‰€æœ‰èŠ‚ç‚¹å·²æ·»åŠ å®Œæˆ")

    def _add_edges(self, graph: StateGraph):
        """
        å®šä¹‰èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å’Œæ¡ä»¶è·¯ç”±ã€‚
        """
        logger.info("é…ç½®æ™ºèƒ½æ£€ç´¢å·¥ä½œæµè·¯ç”±...")
        graph.set_entry_point("query_analysis")
        graph.add_edge("query_analysis", "strategy_route")

        graph.add_conditional_edges(
            "strategy_route",
            self._route_to_search_node,
            {
                "local_search": "local_search",
                "global_search": "global_search",
                "hybrid_search": "hybrid_search"
            }
        )

        graph.add_edge("local_search", "quality_assessment")
        graph.add_edge("global_search", "quality_assessment")
        graph.add_edge("hybrid_search", "quality_assessment")

        graph.add_conditional_edges(
            "quality_assessment",
            self._should_use_web_search,
            {
                "web_search": "web_search",
                "answer_generation": "answer_generation"
            }
        )

        graph.add_edge("web_search", "answer_generation")
        graph.add_edge("answer_generation", END)
        logger.info("æ™ºèƒ½æ£€ç´¢å·¥ä½œæµè·¯ç”±é…ç½®å®Œæˆ")

    def _route_to_search_node(self, state: AgentState) -> str:
        """æ ¹æ®ç­–ç•¥è·¯ç”±èŠ‚ç‚¹çš„å†³ç­–é€‰æ‹©ä¸‹ä¸€ä¸ªæ£€ç´¢èŠ‚ç‚¹ã€‚"""
        route_decision = state.get("lightrag_mode")
        logger.info(f"ğŸ”€ ç­–ç•¥è·¯ç”±: {state.get('query_type')} â†’ {route_decision} â†’ {route_decision}_search")
        if route_decision == "local":
            return "local_search"
        elif route_decision == "global":
            return "global_search"
        elif route_decision == "hybrid":
            return "hybrid_search"
        else:
            logger.warning(f"æœªçŸ¥çš„è·¯ç”±å†³ç­– '{route_decision}', é»˜è®¤ä½¿ç”¨ local_searchã€‚")
            return "local_search"

    def _should_use_web_search(self, state: AgentState) -> str:
        """
        æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœå†³å®šæ˜¯å¦éœ€è¦è¿›è¡Œç½‘ç»œæœç´¢ã€‚
        """
        if state.get("need_web_search", False):
            logger.info("ğŸ” è´¨é‡è¯„ä¼°åˆ¤å®šéœ€è¦ç½‘ç»œæœç´¢è¡¥å……")
            return "web_search"
        else:
            logger.info("âœ… è´¨é‡è¯„ä¼°è®¤ä¸ºæœ¬åœ°ä¿¡æ¯å·²è¶³å¤Ÿ")
            return "answer_generation"

    def _compile_graph(self):
        """
        ç¼–è¯‘å·¥ä½œæµå›¾ï¼Œä½¿å…¶å¯æ‰§è¡Œã€‚
        """
        logger.info("ç¼–è¯‘å·¥ä½œæµ...")
        try:
            # ä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹æ¥æ”¯æŒæµå¼å¤„ç†å’Œä¼šè¯ç®¡ç†
            checkpointer = MemorySaver()
            compiled_graph = self._graph.compile(checkpointer=checkpointer)
            logger.info("âœ… å·¥ä½œæµç¼–è¯‘æˆåŠŸ")
            # å¯é€‰ï¼šç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡ä»¥ä¾›è°ƒè¯•
            try:
                image_bytes = compiled_graph.get_graph().draw_mermaid_png()
                with open("graph.png", "wb") as f:
                    f.write(image_bytes)
                logger.info("âœ… å·¥ä½œæµå¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜: graph.png")
            except Exception as viz_error:
                logger.warning(f"âš ï¸  æ— æ³•ç”Ÿæˆå·¥ä½œæµå¯è§†åŒ–å›¾ç‰‡: {viz_error}")
            return compiled_graph
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¼–è¯‘å¤±è´¥: {e}")
            raise

    def _analyze_node_data(self, node_name: str, node_data: dict, step_info: dict):
        """
        åˆ†æä¸åŒèŠ‚ç‚¹çš„å…³é”®ä¿¡æ¯å¹¶æ‰“å°è°ƒè¯•è¾“å‡ºã€‚
        
        Args:
            node_name (str): èŠ‚ç‚¹åç§°
            node_data (dict): èŠ‚ç‚¹æ•°æ®
            step_info (dict): æ­¥éª¤ä¿¡æ¯å­—å…¸
        """
        try:
            # åˆ†æä¸åŒèŠ‚ç‚¹çš„å…³é”®ä¿¡æ¯
            if node_name == "query_analysis":
                query_type = node_data.get("query_type", "æœªçŸ¥")
                lightrag_mode = node_data.get("lightrag_mode", "æœªçŸ¥")
                print(f"ğŸ” æŸ¥è¯¢åˆ†æç»“æœ:")
                print(f"   æŸ¥è¯¢ç±»å‹: {query_type}")
                print(f"   LightRAGæ¨¡å¼: {lightrag_mode}")
                step_info["key_metrics"] = {"query_type": query_type, "lightrag_mode": lightrag_mode}
                
            elif node_name == "strategy_route":
                route_decision = node_data.get("lightrag_mode", "æœªçŸ¥")
                print(f"ğŸš¦ ç­–ç•¥è·¯ç”±å†³ç­–:")
                print(f"   é€‰æ‹©è·¯å¾„: {route_decision}")
                step_info["key_metrics"] = {"route_decision": route_decision}
                
            elif "search" in node_name:
                search_results = node_data.get("search_results", [])
                lightrag_results = node_data.get("lightrag_results", {})
                context_quality = node_data.get("context_quality", "æœªçŸ¥")
                retrieval_success = node_data.get("retrieval_success", False)
                
                print(f"ğŸ“š {node_name} æ£€ç´¢ç»“æœ:")
                print(f"   æ£€ç´¢æˆåŠŸ: {'æ˜¯' if retrieval_success else 'å¦'}")
                print(f"   æ£€ç´¢æ¡ç›®æ•°: {len(search_results) if search_results else 0}")
                print(f"   ä¸Šä¸‹æ–‡è´¨é‡: {context_quality}")
                
                if lightrag_results and lightrag_results.get("error"):
                    print(f"   âŒ LightRAGé”™è¯¯: {lightrag_results.get('error')}")
                    
                if search_results:
                    print(f"   ç»“æœé¢„è§ˆ: {str(search_results)[:200]}...")
                    
                step_info["key_metrics"] = {
                    "result_count": len(search_results) if search_results else 0,
                    "context_quality": context_quality,
                    "retrieval_success": retrieval_success
                }
                
            elif node_name == "quality_assessment":
                confidence_score = node_data.get("confidence_score", 0)
                quality_score = node_data.get("quality_score", 0)
                need_web_search = node_data.get("need_web_search", False)
                assessment_reason = node_data.get("assessment_reason", "æ— åŸå› ")
                
                print(f"â­ è´¨é‡è¯„ä¼°ç»“æœ:")
                print(f"   ç½®ä¿¡åº¦åˆ†æ•°: {confidence_score}")
                print(f"   è´¨é‡åˆ†æ•°: {quality_score}")
                print(f"   éœ€è¦ç½‘ç»œæœç´¢: {'æ˜¯' if need_web_search else 'å¦'}")
                print(f"   è¯„ä¼°åŸå› : {assessment_reason}")
                
                # è´¨é‡åˆ†æ
                if confidence_score < 0.7:
                    print("   âš ï¸ ç½®ä¿¡åº¦è¾ƒä½ï¼Œå¯èƒ½éœ€è¦æ”¹è¿›æ£€ç´¢ç­–ç•¥")
                if need_web_search:
                    print("   ğŸŒ å°†å¯ç”¨ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯")
                    
                step_info["key_metrics"] = {
                    "confidence_score": confidence_score,
                    "quality_score": quality_score,
                    "need_web_search": need_web_search,
                    "assessment_reason": assessment_reason
                }
                
            elif node_name == "web_search":
                web_results = node_data.get("web_results", [])
                enhanced_context = node_data.get("enhanced_context", "")
                web_search_summary = node_data.get("web_search_summary", "")
                
                print(f"ğŸŒ ç½‘ç»œæœç´¢ç»“æœ:")
                print(f"   ç½‘ç»œç»“æœæ•°: {len(web_results) if web_results else 0}")
                print(f"   å¢å¼ºä¸Šä¸‹æ–‡é•¿åº¦: {len(enhanced_context) if enhanced_context else 0}")
                print(f"   æœç´¢æ‘˜è¦: {web_search_summary}")
                
                step_info["key_metrics"] = {
                    "web_result_count": len(web_results) if web_results else 0,
                    "enhanced_context_length": len(enhanced_context) if enhanced_context else 0
                }
                
            elif node_name == "answer_generation":
                final_answer = node_data.get("final_answer", "")
                answer_confidence = node_data.get("answer_confidence", 0)
                sources = node_data.get("sources", [])
                context_used = node_data.get("context_used", "æœªçŸ¥")
                
                print(f"âœ¨ ç­”æ¡ˆç”Ÿæˆç»“æœ:")
                print(f"   ç­”æ¡ˆé•¿åº¦: {len(final_answer) if final_answer else 0}")
                print(f"   ç­”æ¡ˆç½®ä¿¡åº¦: {answer_confidence}")
                print(f"   ä¿¡æ¯æºæ•°é‡: {len(sources) if sources else 0}")
                print(f"   ä½¿ç”¨çš„ä¸Šä¸‹æ–‡: {context_used}")
                print(f"   ç­”æ¡ˆé¢„è§ˆ: {final_answer[:200] if final_answer else 'æ— '}...")
                
                step_info["key_metrics"] = {
                    "answer_length": len(final_answer) if final_answer else 0,
                    "answer_confidence": answer_confidence,
                    "source_count": len(sources) if sources else 0
                }
            else:
                # é€šç”¨èŠ‚ç‚¹å¤„ç†
                print(f"ğŸ”§ {node_name} èŠ‚ç‚¹:")
                print(f"   æ•°æ®é”®: {list(node_data.keys()) if isinstance(node_data, dict) else 'éå­—å…¸'}")
                step_info["key_metrics"] = {"data_keys": list(node_data.keys()) if isinstance(node_data, dict) else []}
                
        except Exception as e:
            print(f"   âŒ åˆ†æèŠ‚ç‚¹ {node_name} æ—¶å‡ºé”™: {e}")
            step_info["key_metrics"] = {"error": str(e)}

    def query(self, query_text: str, config: Optional[Dict] = None, debug: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰§è¡ŒæŸ¥è¯¢ã€‚
        
        Args:
            query_text (str): ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            config (Optional[Dict]): å¯é€‰çš„é…ç½®å‚æ•°
            debug (bool): æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æŸ¥è¯¢ç»“æœï¼Œå¦‚æœdebug=Trueåˆ™è¿”å›è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        """
        if debug:
            return self.debug_query(query_text, config)
        else:
            return safe_run_async(self.query_async(query_text, config))

    async def query_async(self, query_text: str, config: Optional[Dict] = None, debug: bool = False) -> Dict[str, Any]:
        """
        å¼‚æ­¥æ‰§è¡ŒæŸ¥è¯¢ã€‚
        
        Args:
            query_text (str): ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            config (Optional[Dict]): å¯é€‰çš„é…ç½®å‚æ•°
            debug (bool): æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æŸ¥è¯¢ç»“æœï¼Œå¦‚æœdebug=Trueåˆ™è¿”å›è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        """
        if debug:
            return await self.debug_query_async(query_text, config)
            
        # æ ‡å‡†æŸ¥è¯¢æ‰§è¡Œ
        start_time = time.time()
        query_id = f"query-{uuid.uuid4()}"
        thread_id = (config or {}).get("configurable", {}).get("thread_id", f"thread-{uuid.uuid4()}")

        initial_state = {
            "user_query": query_text,
            "session_id": thread_id,
            "query_id": query_id
        }

        run_config = {"configurable": {"thread_id": thread_id}}
        
        logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢ {query_id}: {query_text[:100]}...")
        final_state = await self.compiled_graph.ainvoke(initial_state, config=run_config)
        
        execution_time = time.time() - start_time
        logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ {query_id} ({execution_time:.2f}s)")
        
        # æ•´ç†è·¯ç”±è·¯å¾„
        route_taken = []
        
        # å°è¯•å¤šç§æ–¹å¼æå–è·¯ç”±ä¿¡æ¯
        logger.info(f"è°ƒè¯•ï¼šfinal_stateé”®: {list(final_state.keys())}")
        
        # æ–¹æ³•1ï¼šä»messagesä¸­æå–
        if 'messages' in final_state and final_state['messages']:
            for message in final_state['messages']:
                if hasattr(message, 'name') and message.name:
                    route_taken.append(message.name)
        
        # æ–¹æ³•2ï¼šä»æ‰§è¡Œå†å²ä¸­æå–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'execution_history' in final_state:
            route_taken.extend(final_state['execution_history'])
        
        # æ–¹æ³•3ï¼šä»å…¶ä»–å¯èƒ½çš„å­—æ®µæå–
        for key in ['node_sequence', 'path_taken', 'execution_path']:
            if key in final_state and final_state[key]:
                route_taken.extend(final_state[key])
        
        # å¦‚æœä»ç„¶ä¸ºç©ºï¼Œå°è¯•ä»å·¥ä½œæµå†³ç­–ä¸­æ¨æ–­
        if not route_taken:
            if final_state.get('lightrag_mode') == 'local':
                route_taken = ['query_analysis', 'strategy_route', 'local_search', 'quality_assessment']
            elif final_state.get('lightrag_mode') == 'global':
                route_taken = ['query_analysis', 'strategy_route', 'global_search', 'quality_assessment']
            elif final_state.get('lightrag_mode') == 'hybrid':
                route_taken = ['query_analysis', 'strategy_route', 'hybrid_search', 'quality_assessment']
            
            # æ·»åŠ æœ€ç»ˆæ­¥éª¤
            if final_state.get('need_web_search'):
                route_taken.append('web_search')
            route_taken.append('answer_generation')
        
        logger.info(f"è°ƒè¯•ï¼šæå–çš„è·¯ç”±: {route_taken}")

        # ç®€åŒ–æœ€ç»ˆè¾“å‡º
        return {
            "answer": final_state.get("final_answer"),
            "sources": final_state.get("sources"),
            "quality_score": final_state.get("confidence_score"),
            "answer_confidence": final_state.get("answer_confidence"),
            "execution_time": execution_time,
            "query_id": query_id,
            "route_taken": route_taken
        }

    async def debug_query_async(self, query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        å¼‚æ­¥æ‰§è¡ŒæŸ¥è¯¢å¹¶æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ã€‚
        ä½¿ç”¨stream_mode="debug"æ¥è·å–æ¯ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†æ‰§è¡ŒçŠ¶æ€ã€‚
        
        Args:
            query_text (str): ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            config (Optional[Dict]): å¯é€‰çš„é…ç½®å‚æ•°
        
        Returns:
            Dict[str, Any]: åŒ…å«è¯¦ç»†è°ƒè¯•ä¿¡æ¯çš„ç»“æœ
        """
        start_time = time.time()
        query_id = f"debug-query-{uuid.uuid4()}"
        thread_id = (config or {}).get("configurable", {}).get("thread_id", f"thread-{uuid.uuid4()}")

        initial_state = {
            "user_query": query_text,
            "session_id": thread_id,
            "query_id": query_id
        }

        run_config = {"configurable": {"thread_id": thread_id}}
        
        print("ğŸ” å¼€å§‹è¯¦ç»†è°ƒè¯•æ‰§è¡Œ...")
        print(f"ğŸ“ æŸ¥è¯¢: {query_text}")
        print(f"ğŸ†” æŸ¥è¯¢ID: {query_id}")
        print("=" * 80)

        # æ”¶é›†è°ƒè¯•ä¿¡æ¯
        debug_info = {
            "query_text": query_text,
            "query_id": query_id,
            "execution_steps": [],
            "final_result": None,
            "execution_time": 0
        }

        step_count = 0
        
        # ä½¿ç”¨debugæµæ¨¡å¼è·å–è¯¦ç»†ä¿¡æ¯
        async for chunk in self.compiled_graph.astream(initial_state, config=run_config, stream_mode="debug"):
            step_count += 1
            
            # æ‰“å°åŸå§‹chunkç”¨äºè°ƒè¯•
            print(f"\nğŸš€ æ­¥éª¤ {step_count}: æ”¶åˆ°è°ƒè¯•ä¿¡æ¯")
            print(f"Chunkç±»å‹: {type(chunk)}")
            print(f"Chunkå†…å®¹: {chunk}")
            print("-" * 60)
            
            # å®‰å…¨è§£æè°ƒè¯•chunk
            try:
                if isinstance(chunk, dict):
                    # å¤„ç†å­—å…¸ç±»å‹çš„chunk
                    for node_name, node_data in chunk.items():
                        if node_name not in ["__start__", "__end__", "step"]:
                            print(f"ğŸ¯ å¤„ç†èŠ‚ç‚¹: '{node_name}'")
                            
                            # æå–å…³é”®çŠ¶æ€ä¿¡æ¯
                            step_info = {
                                "step": step_count,
                                "node": node_name,
                                "input_state": {},
                                "output_state": {},
                                "key_metrics": {}
                            }

                            # å®‰å…¨è·å–èŠ‚ç‚¹æ•°æ®
                            if isinstance(node_data, dict):
                                self._analyze_node_data(node_name, node_data, step_info)
                                
                                # è®°å½•çŠ¶æ€å˜åŒ–
                                step_info["input_state"] = dict(node_data)
                                step_info["output_state"] = dict(node_data)
                                
                                debug_info["execution_steps"].append(step_info)
                            else:
                                print(f"   âš ï¸ èŠ‚ç‚¹æ•°æ®æ ¼å¼å¼‚å¸¸: {type(node_data)}")
                                
                elif hasattr(chunk, '__iter__') and not isinstance(chunk, (str, bytes)):
                    # å¤„ç†å…¶ä»–å¯è¿­ä»£ç±»å‹
                    print(f"   å¤„ç†å¯è¿­ä»£chunk: {chunk}")
                else:
                    # å¤„ç†å…¶ä»–ç±»å‹çš„chunk
                    print(f"   è·³è¿‡éå­—å…¸ç±»å‹chunk: {chunk}")
                    
            except Exception as e:
                print(f"   âŒ è§£æchunkæ—¶å‡ºé”™: {e}")
                print(f"   Chunkè¯¦æƒ…: {chunk}")
                continue
                    
        execution_time = time.time() - start_time
        debug_info["execution_time"] = execution_time
        
        # è·å–æœ€ç»ˆçŠ¶æ€
        final_state = await self.compiled_graph.ainvoke(initial_state, config=run_config)
        debug_info["final_result"] = {
            "answer": final_state.get("final_answer"),
            "sources": final_state.get("sources"),
            "quality_score": final_state.get("confidence_score"),
            "answer_confidence": final_state.get("answer_confidence")
        }

        print("\n" + "=" * 80)
        print("ğŸ“Š æ‰§è¡Œæ‘˜è¦:")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s")
        print(f"ğŸ”¢ æ‰§è¡Œæ­¥éª¤æ•°: {len(debug_info['execution_steps'])}")
        
        # åˆ†ææ‰§è¡Œè·¯å¾„
        node_sequence = [step["node"] for step in debug_info["execution_steps"]]
        print(f"ğŸ›¤ï¸  æ‰§è¡Œè·¯å¾„: {' â†’ '.join(node_sequence)}")
        
        # è´¨é‡åˆ†ææ‘˜è¦
        quality_issues = []
        for step in debug_info["execution_steps"]:
            if step["node"] == "quality_assessment":
                confidence = step["key_metrics"].get("confidence_score", 0)
                if confidence < 0.7:
                    quality_issues.append(f"è´¨é‡è¯„ä¼°ç½®ä¿¡åº¦è¾ƒä½ ({confidence})")
                if step["key_metrics"].get("need_web_search"):
                    quality_issues.append("æœ¬åœ°ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ç½‘ç»œæœç´¢è¡¥å……")
                    
        if quality_issues:
            print("âš ï¸ å‘ç°çš„é—®é¢˜:")
            for issue in quality_issues:
                print(f"   - {issue}")
        else:
            print("âœ… æ‰§è¡Œè¿‡ç¨‹æ— æ˜æ˜¾é—®é¢˜")
            
        return debug_info

    def debug_query(self, query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰§è¡ŒæŸ¥è¯¢å¹¶æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ã€‚
        
        Args:
            query_text (str): ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            config (Optional[Dict]): å¯é€‰çš„é…ç½®å‚æ•°
        
        Returns:
            Dict[str, Any]: åŒ…å«è¯¦ç»†è°ƒè¯•ä¿¡æ¯çš„ç»“æœ
        """
        return safe_run_async(self.debug_query_async(query_text, config))

    def query_stream(self, query_text: str, config: Optional[Dict] = None):
        """
        æµå¼æ‰§è¡ŒæŸ¥è¯¢ã€‚
        """
        return safe_run_async(self.query_stream_async(query_text, config))

    async def query_stream_async(self, query_text: str, config: Optional[Dict] = None):
        """
        å¼‚æ­¥æµå¼æ‰§è¡ŒæŸ¥è¯¢ã€‚
        """
        thread_id = (config or {}).get("configurable", {}).get("thread_id", f"thread-{uuid.uuid4()}")
        run_config = {"configurable": {"thread_id": thread_id}}
        initial_state = {"user_query": query_text, "session_id": thread_id}
        
        async for chunk in self.compiled_graph.astream(initial_state, run_config):
            yield chunk

    async def enhanced_debug_query_async(self, query_text: str, config: Optional[Dict] = None, 
                                         stream_modes: List[str] = None) -> Dict[str, Any]:
        """
        å¢å¼ºç‰ˆå¼‚æ­¥è°ƒè¯•æŸ¥è¯¢ï¼Œæ”¯æŒå¤šç§LangGraph stream_modeçš„ç»„åˆç›‘æ§ã€‚
        
        Args:
            query_text (str): ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            config (Optional[Dict]): å¯é€‰çš„é…ç½®å‚æ•°
            stream_modes (List[str]): ç›‘æ§æ¨¡å¼åˆ—è¡¨ï¼Œæ”¯æŒ ['debug', 'updates', 'values', 'messages', 'custom']
        
        Returns:
            Dict[str, Any]: åŒ…å«è¯¦ç»†è°ƒè¯•ä¿¡æ¯çš„ç»“æœ
        """
        if stream_modes is None:
            stream_modes = ["debug", "updates"]  # é»˜è®¤ç»„åˆæ¨¡å¼
            
        start_time = time.time()
        query_id = f"enhanced-debug-{uuid.uuid4()}"
        thread_id = (config or {}).get("configurable", {}).get("thread_id", f"thread-{uuid.uuid4()}")

        initial_state = {
            "user_query": query_text,
            "session_id": thread_id,
            "query_id": query_id
        }

        run_config = {"configurable": {"thread_id": thread_id}}
        
        print("ğŸ” å¼€å§‹å¢å¼ºç‰ˆè°ƒè¯•æ‰§è¡Œ...")
        print(f"ğŸ“ æŸ¥è¯¢: {query_text}")
        print(f"ğŸ†” æŸ¥è¯¢ID: {query_id}")
        print(f"ğŸ“Š ç›‘æ§æ¨¡å¼: {stream_modes}")
        print("=" * 80)

        # æ”¶é›†å¢å¼ºè°ƒè¯•ä¿¡æ¯
        enhanced_debug_info = {
            "query_text": query_text,
            "query_id": query_id,
            "stream_modes": stream_modes,
            "execution_steps": [],
            "state_history": [],
            "node_updates": [],
            "llm_messages": [],
            "custom_events": [],
            "final_result": None,
            "execution_time": 0,
            "data_verification": {}
        }

        step_count = 0
        
        # ä½¿ç”¨å¤šç§æµæ¨¡å¼è·å–ç»¼åˆä¿¡æ¯
        try:
            async for stream_mode, chunk in self.compiled_graph.astream(
                initial_state, 
                config=run_config, 
                stream_mode=stream_modes
            ):
                step_count += 1
                
                print(f"\nğŸš€ æ­¥éª¤ {step_count}: [{stream_mode}] æ¨¡å¼æ•°æ®")
                print(f"Chunkç±»å‹: {type(chunk)}")
                print(f"Chunkå†…å®¹: {chunk}")
                print("-" * 60)
                
                # æ ¹æ®æµæ¨¡å¼åˆ†ç±»å¤„ç†
                if stream_mode == "debug":
                    self._process_debug_chunk(chunk, enhanced_debug_info, step_count)
                elif stream_mode == "updates":
                    self._process_updates_chunk(chunk, enhanced_debug_info, step_count)
                elif stream_mode == "values":
                    self._process_values_chunk(chunk, enhanced_debug_info, step_count)
                elif stream_mode == "messages":
                    self._process_messages_chunk(chunk, enhanced_debug_info, step_count)
                elif stream_mode == "custom":
                    self._process_custom_chunk(chunk, enhanced_debug_info, step_count)
                    
        except Exception as e:
            print(f"âŒ å¢å¼ºè°ƒè¯•æ‰§è¡Œå‡ºé”™: {e}")
            enhanced_debug_info["execution_error"] = str(e)
            
        execution_time = time.time() - start_time
        enhanced_debug_info["execution_time"] = execution_time
        
        # æ‰§è¡Œæ•°æ®éªŒè¯æ£€æŸ¥
        enhanced_debug_info["data_verification"] = await self._verify_lightrag_data()
        
        # è·å–æœ€ç»ˆçŠ¶æ€
        try:
            final_state = await self.compiled_graph.ainvoke(initial_state, config=run_config)
            enhanced_debug_info["final_result"] = {
                "answer": final_state.get("final_answer"),
                "sources": final_state.get("sources"),
                "quality_score": final_state.get("confidence_score"),
                "answer_confidence": final_state.get("answer_confidence"),
                "full_state": final_state
            }
        except Exception as e:
            enhanced_debug_info["final_result"] = {"error": str(e)}

        # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        analysis_report = self._generate_analysis_report(enhanced_debug_info)
        enhanced_debug_info["analysis_report"] = analysis_report

        print("\n" + "=" * 80)
        print("ğŸ“Š å¢å¼ºè°ƒè¯•æ‘˜è¦:")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f}s")
        print(f"ğŸ”¢ ç›‘æ§äº‹ä»¶æ•°: {step_count}")
        print(f"ğŸ›¤ï¸  ç›‘æ§æ¨¡å¼: {stream_modes}")
        
        # æ‰“å°å…³é”®å‘ç°
        if analysis_report.get("critical_issues"):
            print("âš ï¸ å‘ç°å…³é”®é—®é¢˜:")
            for issue in analysis_report["critical_issues"]:
                print(f"   - {issue}")
        else:
            print("âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
            
        return enhanced_debug_info

    def _process_debug_chunk(self, chunk: Any, debug_info: Dict, step_count: int):
        """å¤„ç†debugæ¨¡å¼çš„chunkæ•°æ®"""
        try:
            if isinstance(chunk, dict):
                for node_name, node_data in chunk.items():
                    if node_name not in ["__start__", "__end__", "step"]:
                        step_info = {
                            "step": step_count,
                            "node": node_name,
                            "mode": "debug",
                            "data": node_data,
                            "timestamp": time.time()
                        }
                        
                        # åˆ†æèŠ‚ç‚¹æ•°æ®
                        if isinstance(node_data, dict):
                            self._analyze_node_data(node_name, node_data, step_info)
                            
                        debug_info["execution_steps"].append(step_info)
        except Exception as e:
            print(f"   âŒ å¤„ç†debug chunkå¤±è´¥: {e}")

    def _process_updates_chunk(self, chunk: Any, debug_info: Dict, step_count: int):
        """å¤„ç†updatesæ¨¡å¼çš„chunkæ•°æ®"""
        try:
            if isinstance(chunk, dict):
                for node_name, update_data in chunk.items():
                    update_info = {
                        "step": step_count,
                        "node": node_name,
                        "mode": "updates",
                        "update": update_data,
                        "timestamp": time.time()
                    }
                    debug_info["node_updates"].append(update_info)
                    print(f"   ğŸ“ èŠ‚ç‚¹æ›´æ–°: {node_name} -> {list(update_data.keys()) if isinstance(update_data, dict) else type(update_data)}")
        except Exception as e:
            print(f"   âŒ å¤„ç†updates chunkå¤±è´¥: {e}")

    def _process_values_chunk(self, chunk: Any, debug_info: Dict, step_count: int):
        """å¤„ç†valuesæ¨¡å¼çš„chunkæ•°æ®"""
        try:
            state_info = {
                "step": step_count,
                "mode": "values",
                "state": chunk,
                "timestamp": time.time()
            }
            debug_info["state_history"].append(state_info)
            print(f"   ğŸ›ï¸ å®Œæ•´çŠ¶æ€: {list(chunk.keys()) if isinstance(chunk, dict) else type(chunk)}")
        except Exception as e:
            print(f"   âŒ å¤„ç†values chunkå¤±è´¥: {e}")

    def _process_messages_chunk(self, chunk: Any, debug_info: Dict, step_count: int):
        """å¤„ç†messagesæ¨¡å¼çš„chunkæ•°æ®"""
        try:
            if isinstance(chunk, tuple) and len(chunk) == 2:
                message, metadata = chunk
                message_info = {
                    "step": step_count,
                    "mode": "messages",
                    "message": message,
                    "metadata": metadata,
                    "timestamp": time.time()
                }
                debug_info["llm_messages"].append(message_info)
                print(f"   ğŸ’¬ LLMæ¶ˆæ¯: {message.content[:50] if hasattr(message, 'content') and message.content else 'N/A'}...")
        except Exception as e:
            print(f"   âŒ å¤„ç†messages chunkå¤±è´¥: {e}")

    def _process_custom_chunk(self, chunk: Any, debug_info: Dict, step_count: int):
        """å¤„ç†customæ¨¡å¼çš„chunkæ•°æ®"""
        try:
            custom_info = {
                "step": step_count,
                "mode": "custom",
                "data": chunk,
                "timestamp": time.time()
            }
            debug_info["custom_events"].append(custom_info)
            print(f"   ğŸ¯ è‡ªå®šä¹‰äº‹ä»¶: {chunk}")
        except Exception as e:
            print(f"   âŒ å¤„ç†custom chunkå¤±è´¥: {e}")

    async def _verify_lightrag_data(self) -> Dict[str, Any]:
        """éªŒè¯LightRAGæ•°æ®çŠ¶æ€"""
        verification_result = {
            "lightrag_initialized": False,
            "data_loaded": False,
            "postgres_connected": False,
            "neo4j_connected": False,
            "vector_index_ready": False,
            "knowledge_graph_ready": False,
            "data_stats": {},
            "database_info": {
                "postgres_host": config.POSTGRES_HOST,
                "postgres_db": config.POSTGRES_DB,
                "neo4j_uri": config.NEO4J_URI
            }
        }
        
        try:
            # 1. æ£€æŸ¥LightRAGå®ä¾‹
            from ..utils.lightrag_client import get_lightrag_instance
            
            lightrag_client = get_lightrag_instance()
            if lightrag_client and lightrag_client.rag_instance:
                verification_result["lightrag_initialized"] = True
                
                # 2. æµ‹è¯•PostgreSQLè¿æ¥
                try:
                    import psycopg2
                    pg_conn = psycopg2.connect(
                        host=config.POSTGRES_HOST,
                        port=config.POSTGRES_PORT,
                        database=config.POSTGRES_DB,
                        user=config.POSTGRES_USER,
                        password=config.POSTGRES_PASSWORD
                    )
                    with pg_conn.cursor() as cur:
                        cur.execute("SELECT version();")
                        pg_version = cur.fetchone()[0]
                        verification_result["postgres_connected"] = True
                        verification_result["data_stats"]["postgres_version"] = pg_version
                    pg_conn.close()
                except Exception as e:
                    verification_result["data_stats"]["postgres_error"] = str(e)
                
                # 3. æµ‹è¯•Neo4jè¿æ¥
                try:
                    from neo4j import GraphDatabase
                    neo4j_driver = GraphDatabase.driver(
                        config.NEO4J_URI,
                        auth=(config.NEO4J_USERNAME, config.NEO4J_PASSWORD)
                    )
                    with neo4j_driver.session() as session:
                        result = session.run("CALL dbms.components() YIELD name, versions RETURN name, versions")
                        neo4j_info = result.single()
                        verification_result["neo4j_connected"] = True
                        verification_result["data_stats"]["neo4j_info"] = dict(neo4j_info)
                    neo4j_driver.close()
                except Exception as e:
                    verification_result["data_stats"]["neo4j_error"] = str(e)
                
                # 4. æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½
                if verification_result["postgres_connected"]:
                    try:
                        pg_conn = psycopg2.connect(
                            host=config.POSTGRES_HOST,
                            port=config.POSTGRES_PORT,
                            database=config.POSTGRES_DB,
                            user=config.POSTGRES_USER,
                            password=config.POSTGRES_PASSWORD
                        )
                        with pg_conn.cursor() as cur:
                            # æ£€æŸ¥æ–‡æ¡£è¡¨
                            cur.execute("SELECT COUNT(*) FROM full_docs;")
                            doc_count = cur.fetchone()[0]
                            verification_result["data_stats"]["document_count"] = doc_count
                            
                            # æ£€æŸ¥å‘é‡è¡¨
                            cur.execute("SELECT COUNT(*) FROM vdb_entities;")
                            entity_count = cur.fetchone()[0]
                            verification_result["data_stats"]["entity_count"] = entity_count
                            
                            if doc_count > 0:
                                verification_result["data_loaded"] = True
                            if entity_count > 0:
                                verification_result["vector_index_ready"] = True
                                
                        pg_conn.close()
                    except Exception as e:
                        verification_result["data_stats"]["pg_query_error"] = str(e)
                
                # 5. æ£€æŸ¥Neo4jçŸ¥è¯†å›¾è°±
                if verification_result["neo4j_connected"]:
                    try:
                        neo4j_driver = GraphDatabase.driver(
                            config.NEO4J_URI,
                            auth=(config.NEO4J_USERNAME, config.NEO4J_PASSWORD)
                        )
                        with neo4j_driver.session() as session:
                            # æ£€æŸ¥èŠ‚ç‚¹æ•°é‡
                            result = session.run("MATCH (n) RETURN COUNT(n) as node_count")
                            node_count = result.single()["node_count"]
                            verification_result["data_stats"]["neo4j_node_count"] = node_count
                            
                            # æ£€æŸ¥å…³ç³»æ•°é‡
                            result = session.run("MATCH ()-[r]->() RETURN COUNT(r) as rel_count")
                            rel_count = result.single()["rel_count"]
                            verification_result["data_stats"]["neo4j_relationship_count"] = rel_count
                            
                            if node_count > 0:
                                verification_result["knowledge_graph_ready"] = True
                                
                        neo4j_driver.close()
                    except Exception as e:
                        verification_result["data_stats"]["neo4j_query_error"] = str(e)
                        
            print(f"ğŸ” æ•°æ®æºéªŒè¯ç»“æœ: {verification_result}")
            
        except Exception as e:
            verification_result["error"] = str(e)
            print(f"âŒ æ•°æ®æºéªŒè¯å¤±è´¥: {e}")
            
        return verification_result

    def _generate_analysis_report(self, debug_info: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report = {
            "critical_issues": [],
            "warnings": [],
            "performance_metrics": {},
            "recommendations": [],
            "database_status": {}
        }
        
        try:
            # åˆ†ææ•°æ®éªŒè¯ç»“æœ
            data_verification = debug_info.get("data_verification", {})
            
            # æ•°æ®åº“è¿æ¥é—®é¢˜æ£€æŸ¥
            if not data_verification.get("lightrag_initialized"):
                report["critical_issues"].append("LightRAGæœªæ­£ç¡®åˆå§‹åŒ–")
            if not data_verification.get("postgres_connected"):
                report["critical_issues"].append(f"PostgreSQLè¿æ¥å¤±è´¥ ({data_verification.get('database_info', {}).get('postgres_host', 'unknown')})")
            if not data_verification.get("neo4j_connected"):
                report["critical_issues"].append(f"Neo4jè¿æ¥å¤±è´¥ ({data_verification.get('database_info', {}).get('neo4j_uri', 'unknown')})")
            
            # æ•°æ®å‡†å¤‡çŠ¶æ€æ£€æŸ¥
            if not data_verification.get("data_loaded"):
                report["warnings"].append("æ•°æ®åº“ä¸­æ— æ–‡æ¡£æ•°æ®")
            if not data_verification.get("vector_index_ready"):
                report["warnings"].append("å‘é‡ç´¢å¼•æœªå‡†å¤‡å°±ç»ª")
            if not data_verification.get("knowledge_graph_ready"):
                report["warnings"].append("çŸ¥è¯†å›¾è°±æœªå‡†å¤‡å°±ç»ª")
                
            # æ•°æ®åº“çŠ¶æ€æ‘˜è¦
            report["database_status"] = {
                "postgres_connected": data_verification.get("postgres_connected", False),
                "neo4j_connected": data_verification.get("neo4j_connected", False),
                "document_count": data_verification.get("data_stats", {}).get("document_count", 0),
                "entity_count": data_verification.get("data_stats", {}).get("entity_count", 0),
                "neo4j_node_count": data_verification.get("data_stats", {}).get("neo4j_node_count", 0),
                "neo4j_relationship_count": data_verification.get("data_stats", {}).get("neo4j_relationship_count", 0)
            }
                
            # åˆ†ææ‰§è¡Œæ­¥éª¤
            execution_steps = debug_info.get("execution_steps", [])
            search_steps = [step for step in execution_steps if "search" in step.get("node", "")]
            
            if search_steps:
                # æ£€æŸ¥æ£€ç´¢è´¨é‡
                low_quality_searches = []
                for step in search_steps:
                    key_metrics = step.get("key_metrics", {})
                    if key_metrics.get("context_quality") and "ä½" in str(key_metrics["context_quality"]):
                        low_quality_searches.append(step["node"])
                        
                if low_quality_searches:
                    report["warnings"].append(f"æ£€ç´¢è´¨é‡è¾ƒä½çš„èŠ‚ç‚¹: {low_quality_searches}")
                    
            # æ€§èƒ½æŒ‡æ ‡
            report["performance_metrics"] = {
                "execution_time": debug_info.get("execution_time", 0),
                "total_steps": len(execution_steps),
                "search_steps": len(search_steps)
            }
            
            # å»ºè®®ç”Ÿæˆ
            if report["critical_issues"]:
                if "PostgreSQLè¿æ¥å¤±è´¥" in str(report["critical_issues"]):
                    report["recommendations"].append("æ£€æŸ¥PostgreSQLæ•°æ®åº“è¿æ¥é…ç½®å’Œç½‘ç»œè¿é€šæ€§")
                if "Neo4jè¿æ¥å¤±è´¥" in str(report["critical_issues"]):
                    report["recommendations"].append("æ£€æŸ¥Neo4jæ•°æ®åº“è¿æ¥é…ç½®å’Œè®¤è¯ä¿¡æ¯")
                if "LightRAGæœªæ­£ç¡®åˆå§‹åŒ–" in str(report["critical_issues"]):
                    report["recommendations"].append("æ£€æŸ¥LightRAGé…ç½®å’Œä¾èµ–é¡¹")
            
            if report["warnings"]:
                if "æ•°æ®åº“ä¸­æ— æ–‡æ¡£æ•°æ®" in str(report["warnings"]):
                    report["recommendations"].append("éœ€è¦å…ˆå¯¼å…¥æ–‡æ¡£æ•°æ®åˆ°æ•°æ®åº“")
                if "å‘é‡ç´¢å¼•æœªå‡†å¤‡å°±ç»ª" in str(report["warnings"]):
                    report["recommendations"].append("éœ€è¦æ„å»ºå‘é‡ç´¢å¼•")
                if "çŸ¥è¯†å›¾è°±æœªå‡†å¤‡å°±ç»ª" in str(report["warnings"]):
                    report["recommendations"].append("éœ€è¦æ„å»ºçŸ¥è¯†å›¾è°±")
                if "æ£€ç´¢è´¨é‡è¾ƒä½" in str(report["warnings"]):
                    report["recommendations"].append("è€ƒè™‘è°ƒæ•´è´¨é‡è¯„ä¼°é˜ˆå€¼æˆ–ä¼˜åŒ–æ£€ç´¢ç­–ç•¥")
                
        except Exception as e:
            report["analysis_error"] = str(e)
            
        return report

    def enhanced_debug_query(self, query_text: str, config: Optional[Dict] = None,
                           stream_modes: List[str] = None) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆè°ƒè¯•æŸ¥è¯¢çš„åŒæ­¥å°è£…"""
        return safe_run_async(self.enhanced_debug_query_async(query_text, config, stream_modes))

# --- å…¨å±€å®ä¾‹å’Œä¾¿æ·å‡½æ•° ---

_workflow_instance: Optional[IntelligentQAWorkflow] = None

def get_workflow() -> IntelligentQAWorkflow:
    """è·å–å…¨å±€å·¥ä½œæµå®ä¾‹ (å•ä¾‹æ¨¡å¼)ã€‚"""
    global _workflow_instance
    if _workflow_instance is None:
        logger.info("åˆ›å»ºæ–°çš„å…¨å±€å·¥ä½œæµå®ä¾‹...")
        _workflow_instance = IntelligentQAWorkflow()
    return _workflow_instance

def query(query_text: str, config: Optional[Dict] = None, debug: bool = False) -> Dict[str, Any]:
    """ä¾¿æ·çš„åŒæ­¥æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return workflow.query(query_text, config, debug)

async def query_async(query_text: str, config: Optional[Dict] = None, debug: bool = False) -> Dict[str, Any]:
    """ä¾¿æ·çš„å¼‚æ­¥æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return await workflow.query_async(query_text, config, debug)

def debug_query(query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„åŒæ­¥è°ƒè¯•æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return workflow.debug_query(query_text, config)

async def debug_query_async(query_text: str, config: Optional[Dict] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„å¼‚æ­¥è°ƒè¯•æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return await workflow.debug_query_async(query_text, config)

def enhanced_debug_query(query_text: str, config: Optional[Dict] = None, 
                       stream_modes: List[str] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„å¢å¼ºç‰ˆåŒæ­¥è°ƒè¯•æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return workflow.enhanced_debug_query(query_text, config, stream_modes)

async def enhanced_debug_query_async(query_text: str, config: Optional[Dict] = None,
                                   stream_modes: List[str] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„å¢å¼ºç‰ˆå¼‚æ­¥è°ƒè¯•æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return await workflow.enhanced_debug_query_async(query_text, config, stream_modes)

def query_stream(query_text: str, config: Optional[Dict] = None):
    """ä¾¿æ·çš„æµå¼æŸ¥è¯¢å‡½æ•°ã€‚"""
    workflow = get_workflow()
    return workflow.query_stream(query_text, config)
    
def get_workflow_info() -> Dict[str, Any]:
    """è·å–å·¥ä½œæµçš„å…ƒæ•°æ®ä¿¡æ¯ã€‚"""
    workflow = get_workflow()
    return {
        "id": workflow.workflow_id,
        "version": workflow.version,
        "nodes": SUPPORTED_NODES,
        "graph_type": "StateGraph",
        "framework": "LangGraph"
    }

if __name__ == '__main__':
    # ç”¨äºç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    print("è¿è¡Œå·¥ä½œæµæµ‹è¯•...")
    print("âœ… ä½¿ç”¨å·²é¢„å¤„ç†çš„LightRAGæ•°æ®è¿›è¡Œæµ‹è¯•")

    # åŸºäºdocsç›®å½•å†…å®¹çš„æµ‹è¯•æŸ¥è¯¢ï¼ˆæ•°æ®å·²é¢„å¤„ç†ï¼‰
    test_queries = [
        {
            "query": "OpenAIåœ¨2024å¹´ç­¹é›†äº†å¤šå°‘èµ„é‡‘ï¼Ÿ",
            "expected_strategy": "local_search",
            "description": "äº‹å®æ€§æŸ¥è¯¢æµ‹è¯• - OpenAIèèµ„"
        },
        {
            "query": "Amazonå¯¹Anthropicæ€»å…±æŠ•èµ„äº†å¤šå°‘é’±ï¼Ÿ",
            "expected_strategy": "local_search", 
            "description": "äº‹å®æ€§æŸ¥è¯¢æµ‹è¯• - AmazonæŠ•èµ„"
        },
        {
            "query": "Amazonå’ŒAnthropicçš„åˆä½œå…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
            "expected_strategy": "global_search", 
            "description": "å…³ç³»æ€§æŸ¥è¯¢æµ‹è¯• - åˆä½œå…³ç³»"
        },
        {
            "query": "Databricksçš„æœ€æ–°ä¼°å€¼æ˜¯å¤šå°‘ï¼Ÿ",
            "expected_strategy": "local_search",
            "description": "äº‹å®æ€§æŸ¥è¯¢æµ‹è¯• - Databricksä¼°å€¼"
        },
        {
            "query": "åˆ†æè¿™äº›AIå…¬å¸çš„èèµ„æƒ…å†µå’ŒæŠ•èµ„å…³ç³»",
            "expected_strategy": "hybrid_search",
            "description": "å¤æ‚æŸ¥è¯¢æµ‹è¯• - èèµ„åˆ†æ"
        },
        {
            "query": "è¿™äº›å…¬å¸ä¹‹é—´å­˜åœ¨ä»€ä¹ˆæŠ•èµ„å’Œåˆä½œå…³ç³»ï¼Ÿ",
            "expected_strategy": "global_search",
            "description": "å…³ç³»æ€§æŸ¥è¯¢æµ‹è¯• - æŠ•èµ„å…³ç³»"
        }
    ]
    
    print("\n=== å¤šç­–ç•¥æ£€ç´¢æµ‹è¯• ===")
    for i, test_case in enumerate(test_queries, 1):
        print(f"\n--- æµ‹è¯• {i}: {test_case['description']} ---")
        print(f"æŸ¥è¯¢: {test_case['query']}")
        print(f"æœŸæœ›ç­–ç•¥: {test_case['expected_strategy']}")
        
        try:
            result = query(test_case['query'])
            
            # åˆ†æè·¯ç”±ç»“æœ
            route_taken = result.get('route_taken', [])
            actual_strategy = "æœªçŸ¥"
            
            if 'local_search' in route_taken:
                actual_strategy = "local_search"
            elif 'global_search' in route_taken:
                actual_strategy = "global_search"
            elif 'hybrid_search' in route_taken:
                actual_strategy = "hybrid_search"
            
            print(f"å®é™…ç­–ç•¥: {actual_strategy}")
            print(f"æ‰§è¡Œæ—¶é—´: {result.get('execution_time', 0):.2f}s")
            print(f"ç½®ä¿¡åº¦: {result.get('quality_score', 'N/A')}")
            print(f"å®Œæ•´è·¯ç”±: {' -> '.join(route_taken)}")
            
            # æ˜¾ç¤ºç­”æ¡ˆçš„å‰200ä¸ªå­—ç¬¦
            answer = result.get('answer', 'æ— ç­”æ¡ˆ')
            print(f"ç­”æ¡ˆæ‘˜è¦: {answer[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæœŸæœ›
            if actual_strategy == test_case['expected_strategy']:
                print("âœ… è·¯ç”±ç­–ç•¥ç¬¦åˆé¢„æœŸ")
            else:
                print("âŒ è·¯ç”±ç­–ç•¥ä¸ç¬¦åˆé¢„æœŸ")
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æœ¬åœ°æ£€ç´¢ï¼ˆæ–°å¢ï¼‰
            sources = result.get('sources', [])
            web_search_used = any('web_search' in str(source).lower() for source in sources) if sources else False
            need_web_search = result.get('route_taken', [])
            web_in_route = 'web_search' in need_web_search
            
            if web_in_route:
                print("âš ï¸ ä½¿ç”¨äº†ç½‘ç»œæœç´¢è¡¥å……")
                # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæœ¬åœ°æ£€ç´¢è´¨é‡ä¸è¶³
                confidence = result.get('quality_score', 0)
                print(f"   æœ¬åœ°æ£€ç´¢ç½®ä¿¡åº¦: {confidence}")
                if confidence < 0.7:
                    print("   åŸå› : æœ¬åœ°å†…å®¹è´¨é‡ä¸è¶³ï¼Œéœ€è¦ç½‘ç»œè¡¥å……")
                else:
                    print("   åŸå› : å…¶ä»–å› ç´ è§¦å‘ç½‘ç»œæœç´¢")
            else:
                print("âœ… å®Œå…¨ä½¿ç”¨æœ¬åœ°æ£€ç´¢ï¼Œæ— éœ€ç½‘ç»œæœç´¢")
            
            # ç‰¹åˆ«æ£€æŸ¥é¢„æœŸçš„å…³é”®ä¿¡æ¯æ˜¯å¦å­˜åœ¨
            answer_lower = answer.lower()
            query_lower = test_case['query'].lower()
            
            expected_keywords = {
                "openai": ["6.6", "billion", "157", "thrive"],
                "amazon": ["8", "billion", "anthropic", "4 billion"],  
                "databricks": ["62", "billion", "10 billion", "series j"],
                "åˆä½œ": ["amazon", "anthropic", "aws", "partnership"],
                "æŠ•èµ„": ["amazon", "anthropic", "thrive", "funding"]
            }
            
            found_keywords = False
            for category, keywords in expected_keywords.items():
                if category in query_lower:
                    found_count = sum(1 for kw in keywords if kw in answer_lower)
                    if found_count >= 2:  # è‡³å°‘æ‰¾åˆ°2ä¸ªå…³é”®è¯
                        print(f"âœ… æ‰¾åˆ°æœŸæœ›çš„å…³é”®ä¿¡æ¯ ({category}): {found_count}/{len(keywords)} ä¸ªå…³é”®è¯")
                        found_keywords = True
                        break
            
            if not found_keywords:
                print("âš ï¸ æœªæ‰¾åˆ°æ˜æ˜¾çš„æœŸæœ›å…³é”®ä¿¡æ¯")
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        print("-" * 80)

    # åˆå¹¶æ‰€æœ‰æµ‹è¯•åˆ°ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ä¸­
    async def run_all_tests():
        print("\n=== åŸæœ‰æµ‹è¯•ä¿æŒä¸å˜ ===")
        original_test_query = "LangGraphæ˜¯ä»€ä¹ˆï¼Ÿå®ƒå’ŒLangChainæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
        
        # åŒæ­¥æŸ¥è¯¢æµ‹è¯•
        print("\n--- åŒæ­¥æŸ¥è¯¢æµ‹è¯• ---")
        sync_result = query(original_test_query)
        print(f"ç­”æ¡ˆ: {sync_result['answer']}")
        print(f"æ¥æº: {sync_result['sources']}")
        print(f"è·¯ç”±: {sync_result['route_taken']}")

        # å¼‚æ­¥æŸ¥è¯¢æµ‹è¯•
        print("\n--- å¼‚æ­¥æŸ¥è¯¢æµ‹è¯• ---")
        async_result = await query_async(original_test_query)
        print(f"ç­”æ¡ˆ: {async_result['answer']}")

        # æµå¼æŸ¥è¯¢æµ‹è¯•
        print("\n--- æµå¼æŸ¥è¯¢æµ‹è¯• ---")
        workflow = get_workflow()
        async for step in workflow.query_stream_async(original_test_query):
            print(step)
            print("-" * 20)
    
    # åªæ‰§è¡Œä¸€æ¬¡asyncio.run()
    safe_run_async(run_all_tests())
    
    print("\n" + "=" * 100)
    print("ğŸ” è°ƒè¯•æ¨¡å¼æµ‹è¯•")
    print("=" * 100)
    print("ä½¿ç”¨debugæ¨¡å¼æŸ¥çœ‹è¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹ï¼Œè¯Šæ–­æ£€ç´¢è´¨é‡é—®é¢˜...")
    
    # é€‰æ‹©ä¸€ä¸ªå…¸å‹çš„ä½ç½®ä¿¡åº¦æŸ¥è¯¢è¿›è¡Œè¯¦ç»†åˆ†æ
    debug_test_query = "OpenAIåœ¨2024å¹´ç­¹é›†äº†å¤šå°‘èµ„é‡‘ï¼Ÿ"
    print(f"\nğŸ“‹ è°ƒè¯•æŸ¥è¯¢: {debug_test_query}")
    print("è¿™ä¸ªæŸ¥è¯¢ä¹‹å‰æ˜¾ç¤ºç½®ä¿¡åº¦è¾ƒä½ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹...")
    
    try:
        print("\nğŸš€ å¼€å§‹Debugæ¨¡å¼æ‰§è¡Œ...")
        debug_result = debug_query(debug_test_query)
        
        print("\nğŸ“ˆ è°ƒè¯•åˆ†æå®Œæˆ!")
        print("å¦‚æœæ‚¨çœ‹åˆ°ä»»ä½•è´¨é‡é—®é¢˜ï¼Œå¯ä»¥ï¼š")
        print("1. æ£€æŸ¥LightRAGæ•°æ®æ˜¯å¦å……åˆ†")
        print("2. è°ƒæ•´quality_assessmentçš„è¯„åˆ†é˜ˆå€¼")
        print("3. æ”¹è¿›æ£€ç´¢ç­–ç•¥æˆ–æç¤ºè¯")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•æ‰§è¡Œå¤±è´¥: {e}")
        print("è¿™å¯èƒ½æ˜¯å› ä¸ºLightRAGæœªæ­£ç¡®åˆå§‹åŒ–æˆ–æ•°æ®æœ‰é—®é¢˜")
    
    print("\nğŸ’¡ æç¤ºï¼šåœ¨æ—¥å¸¸ä½¿ç”¨ä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ç”¨è°ƒè¯•:")
    print("- query('ä½ çš„é—®é¢˜', debug=True)")  
    print("- debug_query('ä½ çš„é—®é¢˜')")
    print("- workflow.debug_query('ä½ çš„é—®é¢˜')")
    print("\nè°ƒè¯•æ¨¡å¼ä¼šæ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†æ‰§è¡Œä¿¡æ¯ï¼Œå¸®åŠ©æ‚¨è¯Šæ–­å’Œæ”¹è¿›ç³»ç»Ÿæ€§èƒ½ã€‚")