"""
å¢å¼ºç‰ˆå·¥ä½œæµç¼–æ’ - é›†æˆç»¼åˆé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""

import asyncio
import time
import uuid
import sys
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

# è§£å†³ç›¸å¯¹å¯¼å…¥é—®é¢˜
if __name__ == "__main__":
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    current_dir = Path(__file__).parent
    project_root = current_dir.parent.parent
    sys.path.insert(0, str(project_root))
    
    # ä½¿ç”¨ç»å¯¹å¯¼å…¥
    from src.core.state import AgentState
    from src.core.config import config
    from src.agents.query_analysis import query_analysis_node
    from src.agents.strategy_route import strategy_route_node
    from src.agents.local_search import local_search_node
    from src.agents.global_search import global_search_node
    from src.agents.hybrid_search import hybrid_search_node
    from src.agents.quality_assessment import quality_assessment_node
    from src.agents.web_search import web_search_node
    from src.agents.answer_generation import answer_generation_node
    from src.utils.advanced_logging import (
        setup_logger, get_performance_logger, audit_log, 
        performance_context, record_metric
    )
    from src.utils.error_handling import (
        handle_errors, retry_on_failure, ErrorContext,
        SystemError, ConfigurationError, ErrorSeverity, ErrorCategory
    )
else:
    # æ­£å¸¸çš„ç›¸å¯¹å¯¼å…¥
    from .state import AgentState
    from .config import config
    from ..agents.query_analysis import query_analysis_node
    from ..agents.strategy_route import strategy_route_node
    from ..agents.local_search import local_search_node
    from ..agents.global_search import global_search_node
    from ..agents.hybrid_search import hybrid_search_node
    from ..agents.quality_assessment import quality_assessment_node
    from ..agents.web_search import web_search_node
    from ..agents.answer_generation import answer_generation_node
    from ..utils.advanced_logging import (
        setup_logger, get_performance_logger, audit_log, 
        performance_context, record_metric
    )
    from ..utils.error_handling import (
        handle_errors, retry_on_failure, ErrorContext,
        SystemError, ConfigurationError, ErrorSeverity, ErrorCategory
    )

logger = setup_logger(__name__)
perf_logger = get_performance_logger(__name__)


class EnhancedIntelligentQAWorkflow:
    """
    å¢å¼ºç‰ˆæ™ºèƒ½é—®ç­”å·¥ä½œæµç®¡ç†å™¨
    
    é›†æˆäº†å…¨é¢çš„é”™è¯¯å¤„ç†ã€æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—è®°å½•åŠŸèƒ½
    """
    
    def __init__(self):
        self.graph = None
        self.compiled_graph = None
        self.is_initialized = False
        self.workflow_id = str(uuid.uuid4())
        self.performance_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "average_response_time": 0.0,
            "node_performance": {}
        }
        
        # å®¡è®¡æ—¥å¿—
        audit_log("workflow_creation", details={"workflow_id": self.workflow_id})
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        self._initialize_workflow()
    
    @handle_errors(reraise=True)
    def _initialize_workflow(self):
        """åˆå§‹åŒ–å·¥ä½œæµå›¾"""
        with performance_context("workflow_initialization", __name__):
            logger.info(f"åˆå§‹åŒ–æ™ºèƒ½é—®ç­”å·¥ä½œæµ {self.workflow_id}...")
            
            try:
                # åˆ›å»ºçŠ¶æ€å›¾
                self.graph = StateGraph(AgentState)
                
                # æ·»åŠ èŠ‚ç‚¹
                self._add_nodes()
                
                # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
                self._add_edges()
                
                # ç¼–è¯‘å·¥ä½œæµ
                self._compile_workflow()
                
                self.is_initialized = True
                logger.info(f"âœ… æ™ºèƒ½é—®ç­”å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ {self.workflow_id}")
                
                # è®°å½•æŒ‡æ ‡
                record_metric("workflow_initialized", 1, workflow_id=self.workflow_id)
                
            except Exception as e:
                logger.error(f"âŒ å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
                raise ConfigurationError(
                    f"å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {str(e)}",
                    error_code="WORKFLOW_INIT_FAILED",
                    severity=ErrorSeverity.CRITICAL,
                    recovery_suggestions=[
                        "æ£€æŸ¥æ‰€æœ‰ä¾èµ–æ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…",
                        "éªŒè¯é…ç½®æ–‡ä»¶è®¾ç½®",
                        "æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—"
                    ]
                )
    
    def _add_nodes(self):
        """æ·»åŠ å·¥ä½œæµèŠ‚ç‚¹"""
        logger.info("æ·»åŠ å·¥ä½œæµèŠ‚ç‚¹...")
        
        # ä½¿ç”¨è£…é¥°å™¨åŒ…è£…èŠ‚ç‚¹ä»¥å¢åŠ é”™è¯¯å¤„ç†
        wrapped_nodes = {
            "query_analysis": self._wrap_node(query_analysis_node, "query_analysis"),
            "strategy_route": self._wrap_node(strategy_route_node, "strategy_route"),
            "local_search": self._wrap_node(local_search_node, "local_search"),
            "global_search": self._wrap_node(global_search_node, "global_search"),
            "hybrid_search": self._wrap_node(hybrid_search_node, "hybrid_search"),
            "quality_assessment": self._wrap_node(quality_assessment_node, "quality_assessment"),
            "web_search": self._wrap_node(web_search_node, "web_search"),
            "answer_generation": self._wrap_node(answer_generation_node, "answer_generation")
        }
        
        # æ·»åŠ èŠ‚ç‚¹åˆ°å›¾
        for node_name, node_func in wrapped_nodes.items():
            self.graph.add_node(node_name, node_func)
            logger.debug(f"æ·»åŠ èŠ‚ç‚¹: {node_name}")
        
        logger.info("æ‰€æœ‰èŠ‚ç‚¹å·²æ·»åŠ å®Œæˆ")
    
    def _wrap_node(self, node_func, node_name: str):
        """åŒ…è£…èŠ‚ç‚¹å‡½æ•°ä»¥å¢åŠ é”™è¯¯å¤„ç†å’Œæ€§èƒ½ç›‘æ§"""
        
        def wrapped_node(state: AgentState) -> Dict[str, Any]:
            node_start_time = time.time()
            
            try:
                with performance_context(f"node_{node_name}", f"workflow.{node_name}"):
                    logger.info(f"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
                    
                    # è®°å½•èŠ‚ç‚¹å¼€å§‹
                    audit_log(
                        "node_execution_start",
                        details={
                            "node_name": node_name,
                            "workflow_id": self.workflow_id,
                            "state_keys": list(state.keys())
                        }
                    )
                    
                    # æ‰§è¡ŒèŠ‚ç‚¹
                    result = node_func(state)
                    
                    # è®°å½•æ‰§è¡Œæ—¶é—´
                    execution_time = time.time() - node_start_time
                    self._update_node_performance(node_name, execution_time, True)
                    
                    logger.info(f"âœ… èŠ‚ç‚¹æ‰§è¡ŒæˆåŠŸ: {node_name} ({execution_time:.2f}s)")
                    
                    # è®°å½•èŠ‚ç‚¹å®Œæˆ
                    audit_log(
                        "node_execution_success",
                        details={
                            "node_name": node_name,
                            "workflow_id": self.workflow_id,
                            "execution_time": execution_time,
                            "result_keys": list(result.keys()) if result else []
                        }
                    )
                    
                    return result
                    
            except Exception as e:
                execution_time = time.time() - node_start_time
                self._update_node_performance(node_name, execution_time, False)
                
                logger.error(f"âŒ èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {node_name} ({execution_time:.2f}s) - {str(e)}")
                
                # è®°å½•é”™è¯¯
                audit_log(
                    "node_execution_error",
                    details={
                        "node_name": node_name,
                        "workflow_id": self.workflow_id,
                        "execution_time": execution_time,
                        "error": str(e)
                    }
                )
                
                # æ ¹æ®èŠ‚ç‚¹ç±»å‹å†³å®šé”™è¯¯å¤„ç†ç­–ç•¥
                if node_name in ["query_analysis", "lightrag_retrieval"]:
                    # å…³é”®èŠ‚ç‚¹é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise SystemError(
                        f"{node_name} èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}",
                        error_code=f"NODE_{node_name.upper()}_FAILED",
                        category=ErrorCategory.SYSTEM,
                        severity=ErrorSeverity.HIGH
                    )
                else:
                    # éå…³é”®èŠ‚ç‚¹é”™è¯¯ï¼Œè¿”å›é”™è¯¯çŠ¶æ€
                    return {
                        "error": str(e),
                        "node_name": node_name,
                        "execution_time": execution_time,
                        "success": False
                    }
        
        return wrapped_node
    
    def _update_node_performance(self, node_name: str, execution_time: float, success: bool):
        """æ›´æ–°èŠ‚ç‚¹æ€§èƒ½ç»Ÿè®¡"""
        if node_name not in self.performance_stats["node_performance"]:
            self.performance_stats["node_performance"][node_name] = {
                "total_executions": 0,
                "successful_executions": 0,
                "failed_executions": 0,
                "total_time": 0.0,
                "average_time": 0.0
            }
        
        stats = self.performance_stats["node_performance"][node_name]
        stats["total_executions"] += 1
        stats["total_time"] += execution_time
        stats["average_time"] = stats["total_time"] / stats["total_executions"]
        
        if success:
            stats["successful_executions"] += 1
        else:
            stats["failed_executions"] += 1
        
        # è®°å½•æŒ‡æ ‡
        record_metric(
            f"node_{node_name}_execution_time",
            execution_time,
            node_name=node_name,
            success=success
        )
    
    def _add_edges(self):
        """æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±"""
        logger.info("é…ç½®æ™ºèƒ½æ£€ç´¢å·¥ä½œæµè·¯ç”±...")
        
        # è®¾ç½®å…¥å£ç‚¹
        self.graph.set_entry_point("query_analysis")
        
        # æŸ¥è¯¢åˆ†æ â†’ ç­–ç•¥è·¯ç”±
        self.graph.add_edge("query_analysis", "strategy_route")
        
        # ç­–ç•¥è·¯ç”± â†’ æ¡ä»¶è·¯ç”±åˆ°ä¸‰ä¸ªæ£€ç´¢èŠ‚ç‚¹
        self.graph.add_conditional_edges(
            "strategy_route",
            self._route_to_search_node,
            {
                "local_search": "local_search",
                "global_search": "global_search", 
                "hybrid_search": "hybrid_search"
            }
        )
        
        # ä¸‰ä¸ªæ£€ç´¢èŠ‚ç‚¹ â†’ è´¨é‡è¯„ä¼°
        self.graph.add_edge("local_search", "quality_assessment")
        self.graph.add_edge("global_search", "quality_assessment")
        self.graph.add_edge("hybrid_search", "quality_assessment")
        
        # è´¨é‡è¯„ä¼° â†’ æ¡ä»¶è·¯ç”± (ç½‘ç»œæœç´¢ æˆ– ç­”æ¡ˆç”Ÿæˆ)
        self.graph.add_conditional_edges(
            "quality_assessment",
            self._should_use_web_search,
            {
                "web_search": "web_search",
                "answer_generation": "answer_generation"
            }
        )
        
        # ç½‘ç»œæœç´¢ â†’ ç­”æ¡ˆç”Ÿæˆ
        self.graph.add_edge("web_search", "answer_generation")
        
        # ç­”æ¡ˆç”Ÿæˆ â†’ ç»“æŸ
        self.graph.add_edge("answer_generation", END)
        
        logger.info("æ™ºèƒ½æ£€ç´¢å·¥ä½œæµè·¯ç”±é…ç½®å®Œæˆ")
    
    def _route_to_search_node(self, state: AgentState) -> str:
        """
        å†³å®šä½¿ç”¨å“ªä¸ªæ£€ç´¢èŠ‚ç‚¹
        
        æ ¹æ®ç­–ç•¥è·¯ç”±èŠ‚ç‚¹çš„å†³ç­–ç»“æœï¼Œå°†æŸ¥è¯¢è·¯ç”±åˆ°ç›¸åº”çš„æ£€ç´¢èŠ‚ç‚¹ï¼š
        - localæ¨¡å¼ â†’ local_search
        - globalæ¨¡å¼ â†’ global_search  
        - hybridæ¨¡å¼ â†’ hybrid_search
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            ç›®æ ‡æ£€ç´¢èŠ‚ç‚¹åç§°
        """
        try:
            lightrag_mode = state.get("lightrag_mode", "hybrid")
            query_type = state.get("query_type", "ANALYTICAL")
            
            logger.debug(f"ç­–ç•¥è·¯ç”±å†³ç­–: æŸ¥è¯¢ç±»å‹={query_type}, æ£€ç´¢æ¨¡å¼={lightrag_mode}")
            
            # è·¯ç”±æ˜ å°„
            route_mapping = {
                "local": "local_search",
                "global": "global_search",
                "hybrid": "hybrid_search",
                "naive": "local_search",    # é™çº§åˆ°local
                "mix": "hybrid_search"      # é™çº§åˆ°hybrid
            }
            
            target_node = route_mapping.get(lightrag_mode, "hybrid_search")
            
            logger.info(f"ğŸ”€ ç­–ç•¥è·¯ç”±: {query_type} â†’ {lightrag_mode} â†’ {target_node}")
            
            # è®°å½•è·¯ç”±å†³ç­–æŒ‡æ ‡
            record_metric("strategy_route_decision", 1, 
                         query_type=query_type, 
                         lightrag_mode=lightrag_mode,
                         target_node=target_node)
            
            return target_node
            
        except Exception as e:
            logger.error(f"ç­–ç•¥è·¯ç”±å†³ç­–å¤±è´¥: {e}")
            # é»˜è®¤åˆ°æ··åˆæ£€ç´¢
            logger.warning("ä½¿ç”¨é»˜è®¤çš„æ··åˆæ£€ç´¢èŠ‚ç‚¹")
            return "hybrid_search"
    
    def _should_use_web_search(self, state: AgentState) -> str:
        """
        å†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åç§°
        """
        try:
            need_web_search = state.get("need_web_search", False)
            confidence_score = state.get("confidence_score", 0.0)
            
            logger.debug(f"è´¨é‡è¯„ä¼°å†³ç­–: need_web_search={need_web_search}, confidence={confidence_score:.2f}")
            
            if need_web_search:
                logger.info("ğŸ” è´¨é‡è¯„ä¼°åˆ¤å®šéœ€è¦ç½‘ç»œæœç´¢è¡¥å……")
                record_metric("web_search_triggered", 1, confidence_score=confidence_score)
                return "web_search"
            else:
                logger.info("âœ… è´¨é‡è¯„ä¼°åˆ¤å®šå¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆ")
                record_metric("direct_answer_generation", 1, confidence_score=confidence_score)
                return "answer_generation"
                
        except Exception as e:
            logger.error(f"è·¯ç”±å†³ç­–å¤±è´¥: {e}")
            # é»˜è®¤åˆ°ç­”æ¡ˆç”Ÿæˆ
            return "answer_generation"
    
    @handle_errors(reraise=True)
    def _compile_workflow(self):
        """ç¼–è¯‘å·¥ä½œæµ"""
        logger.info("ç¼–è¯‘å·¥ä½œæµ...")
        
        try:
            # åˆ›å»ºå†…å­˜æ£€æŸ¥ç‚¹
            memory = MemorySaver()
            
            # ç¼–è¯‘å›¾
            self.compiled_graph = self.graph.compile(
                checkpointer=memory,
                debug=config.DEBUG_MODE
            )
            
            logger.info("âœ… å·¥ä½œæµç¼–è¯‘æˆåŠŸ")
            
            # ç¼–è¯‘æˆåŠŸåç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡
            self._generate_visualization()
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¼–è¯‘å¤±è´¥: {e}")
            raise ConfigurationError(
                f"å·¥ä½œæµç¼–è¯‘å¤±è´¥: {str(e)}",
                error_code="WORKFLOW_COMPILE_FAILED",
                severity=ErrorSeverity.CRITICAL
            )
    
    def _generate_visualization(self):
        """ç”Ÿæˆå·¥ä½œæµå¯è§†åŒ–å›¾ç‰‡"""
        try:
            if self.compiled_graph:
                logger.info("ç”Ÿæˆå·¥ä½œæµå¯è§†åŒ–å›¾ç‰‡...")
                
                # ç”ŸæˆPNGå›¾åƒ
                png_data = self.compiled_graph.get_graph().draw_mermaid_png()
                output_file = "graph.png"
                
                with open(output_file, "wb") as f:
                    f.write(png_data)
                
                logger.info(f"âœ… å·¥ä½œæµå¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜: {output_file}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡å¤±è´¥ (ä¸å½±å“å·¥ä½œæµåŠŸèƒ½): {e}")
    
    @handle_errors(reraise=True)
    @retry_on_failure(max_retries=2, backoff_factor=1.0)
    async def arun(self, 
                   user_query: str, 
                   config_override: Optional[Dict[str, Any]] = None,
                   thread_id: Optional[str] = None) -> Dict[str, Any]:
        """
        å¼‚æ­¥è¿è¡Œå·¥ä½œæµ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            config_override: é…ç½®è¦†ç›–
            thread_id: çº¿ç¨‹ID (ç”¨äºä¼šè¯ç®¡ç†)
            
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        if not self.is_initialized:
            raise SystemError(
                "å·¥ä½œæµæœªåˆå§‹åŒ–",
                error_code="WORKFLOW_NOT_INITIALIZED",
                severity=ErrorSeverity.HIGH
            )
        
        query_id = str(uuid.uuid4())
        start_time = time.time()
        
        with performance_context(f"workflow_execution_{query_id}", __name__):
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢ {query_id}: {user_query[:100]}...")
            
            # è®°å½•æŸ¥è¯¢å¼€å§‹
            audit_log(
                "query_start",
                details={
                    "query_id": query_id,
                    "user_query": user_query[:500],  # é™åˆ¶é•¿åº¦
                    "thread_id": thread_id,
                    "workflow_id": self.workflow_id
                }
            )
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = {
                "user_query": user_query,
                "query_id": query_id,
                "thread_id": thread_id or "default",
                "workflow_id": self.workflow_id,
                "start_time": start_time,
                "query_type": "",
                "lightrag_mode": "",
                "key_entities": [],
                "processed_query": "",
                "lightrag_results": {},
                "retrieval_score": 0.0,
                "retrieval_success": False,
                "confidence_score": 0.0,
                "need_web_search": False,
                "web_results": [],
                "final_answer": "",
                "sources": [],
                "context_used": 0,
                "answer_confidence": 0.0
            }
            
            # é…ç½®
            run_config = {
                "configurable": {
                    "thread_id": thread_id or "default"
                }
            }
            
            if config_override:
                run_config.update(config_override)
            
            try:
                # æ‰§è¡Œå·¥ä½œæµ
                result = await self.compiled_graph.ainvoke(
                    initial_state,
                    config=run_config
                )
                
                # è®¡ç®—æ‰§è¡Œæ—¶é—´
                execution_time = time.time() - start_time
                
                # æ›´æ–°ç»Ÿè®¡
                self._update_query_stats(True, execution_time)
                
                # è®°å½•æˆåŠŸ
                logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ {query_id} ({execution_time:.2f}s)")
                
                audit_log(
                    "query_success",
                    details={
                        "query_id": query_id,
                        "execution_time": execution_time,
                        "final_answer_length": len(result.get("final_answer", "")),
                        "sources_count": len(result.get("sources", [])),
                        "confidence_score": result.get("answer_confidence", 0.0)
                    }
                )
                
                # æ·»åŠ æ‰§è¡Œä¿¡æ¯
                result.update({
                    "query_id": query_id,
                    "execution_time": execution_time,
                    "workflow_id": self.workflow_id,
                    "success": True
                })
                
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                self._update_query_stats(False, execution_time)
                
                logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥ {query_id} ({execution_time:.2f}s): {e}")
                
                audit_log(
                    "query_failure",
                    details={
                        "query_id": query_id,
                        "execution_time": execution_time,
                        "error": str(e)
                    }
                )
                
                raise
    
    def _update_query_stats(self, success: bool, execution_time: float):
        """æ›´æ–°æŸ¥è¯¢ç»Ÿè®¡"""
        self.performance_stats["total_queries"] += 1
        
        if success:
            self.performance_stats["successful_queries"] += 1
        else:
            self.performance_stats["failed_queries"] += 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total_time = (self.performance_stats["average_response_time"] * 
                     (self.performance_stats["total_queries"] - 1) + execution_time)
        self.performance_stats["average_response_time"] = total_time / self.performance_stats["total_queries"]
        
        # è®°å½•æŒ‡æ ‡
        record_metric("query_execution_time", execution_time, success=success)
        record_metric("total_queries", self.performance_stats["total_queries"])
        record_metric("success_rate", 
                     self.performance_stats["successful_queries"] / self.performance_stats["total_queries"])
    
    def run(self, 
            user_query: str, 
            config_override: Optional[Dict[str, Any]] = None,
            thread_id: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥è¿è¡Œå·¥ä½œæµ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            config_override: é…ç½®è¦†ç›–
            thread_id: çº¿ç¨‹ID
            
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        return asyncio.run(self.arun(user_query, config_override, thread_id))
    
    def stream(self, 
               user_query: str, 
               config_override: Optional[Dict[str, Any]] = None,
               thread_id: Optional[str] = None):
        """
        æµå¼æ‰§è¡Œå·¥ä½œæµ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            config_override: é…ç½®è¦†ç›–
            thread_id: çº¿ç¨‹ID
            
        Yields:
            å·¥ä½œæµæ‰§è¡Œæ­¥éª¤
        """
        if not self.is_initialized:
            raise SystemError(
                "å·¥ä½œæµæœªåˆå§‹åŒ–",
                error_code="WORKFLOW_NOT_INITIALIZED",
                severity=ErrorSeverity.HIGH
            )
        
        query_id = str(uuid.uuid4())
        start_time = time.time()
        
        logger.info(f"å¼€å§‹æµå¼å¤„ç†æŸ¥è¯¢ {query_id}: {user_query[:100]}...")
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "user_query": user_query,
            "query_id": query_id,
            "thread_id": thread_id or "default",
            "workflow_id": self.workflow_id,
            "start_time": start_time,
            "query_type": "",
            "lightrag_mode": "",
            "key_entities": [],
            "processed_query": "",
            "lightrag_results": {},
            "retrieval_score": 0.0,
            "retrieval_success": False,
            "confidence_score": 0.0,
            "need_web_search": False,
            "web_results": [],
            "final_answer": "",
            "sources": [],
            "context_used": 0,
            "answer_confidence": 0.0
        }
        
        # é…ç½®
        run_config = {
            "configurable": {
                "thread_id": thread_id or "default"
            }
        }
        
        if config_override:
            run_config.update(config_override)
        
        try:
            # æµå¼æ‰§è¡Œ
            for step in self.compiled_graph.stream(initial_state, config=run_config):
                # æ·»åŠ æ—¶é—´æˆ³å’ŒæŸ¥è¯¢ID
                if step:
                    step_data = list(step.values())[0] if step else {}
                    step_data.update({
                        "query_id": query_id,
                        "timestamp": time.time(),
                        "workflow_id": self.workflow_id
                    })
                    
                yield step
                
            # è®°å½•æˆåŠŸ
            execution_time = time.time() - start_time
            self._update_query_stats(True, execution_time)
            
            logger.info(f"âœ… æµå¼å·¥ä½œæµæ‰§è¡Œå®Œæˆ {query_id} ({execution_time:.2f}s)")
                
        except Exception as e:
            execution_time = time.time() - start_time
            self._update_query_stats(False, execution_time)
            
            logger.error(f"âŒ æµå¼å·¥ä½œæµæ‰§è¡Œå¤±è´¥ {query_id} ({execution_time:.2f}s): {e}")
            raise
    
    def get_workflow_info(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµä¿¡æ¯"""
        return {
            "workflow_id": self.workflow_id,
            "name": "å¢å¼ºç‰ˆæ™ºèƒ½é—®ç­”å·¥ä½œæµ",
            "version": "2.0.0",
            "initialized": self.is_initialized,
            "nodes": [
                {
                    "name": "query_analysis",
                    "description": "æŸ¥è¯¢åˆ†æèŠ‚ç‚¹",
                    "function": "åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®å®šæŸ¥è¯¢ç±»å‹å¹¶é€‰æ‹©æœ€ä½³LightRAGæ¨¡å¼"
                },
                {
                    "name": "lightrag_retrieval", 
                    "description": "LightRAGæ£€ç´¢èŠ‚ç‚¹ (HKUDS/LightRAG)",
                    "function": "ä½¿ç”¨HKUDS/LightRAGæ‰§è¡Œæ™ºèƒ½æ£€ç´¢ï¼Œæ”¯æŒnaive/local/global/hybrid/mixæ¨¡å¼"
                },
                {
                    "name": "quality_assessment",
                    "description": "è´¨é‡è¯„ä¼°èŠ‚ç‚¹",
                    "function": "è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢è¡¥å……"
                },
                {
                    "name": "web_search",
                    "description": "ç½‘ç»œæœç´¢èŠ‚ç‚¹",
                    "function": "å½“æœ¬åœ°ä¿¡æ¯ä¸è¶³æ—¶ï¼Œä»ç½‘ç»œè·å–è¡¥å……ä¿¡æ¯"
                },
                {
                    "name": "answer_generation",
                    "description": "ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹",
                    "function": "æ•´åˆæœ¬åœ°å’Œç½‘ç»œä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"
                }
            ],
            "workflow_pattern": "æŸ¥è¯¢åˆ†æ â†’ HKUDS/LightRAGæ£€ç´¢ â†’ è´¨é‡è¯„ä¼° â†’ [ç½‘ç»œæœç´¢] â†’ ç­”æ¡ˆç”Ÿæˆ",
            "features": [
                "æ™ºèƒ½æŸ¥è¯¢åˆ†æå’Œè·¯ç”±",
                "å¤šæ¨¡å¼HKUDS/LightRAGæ£€ç´¢",
                "è´¨é‡è¯„ä¼°å’Œå†³ç­–",
                "ç½‘ç»œæœç´¢è¡¥å……",
                "ç­”æ¡ˆç”Ÿæˆå’Œæ•´åˆ",
                "æµå¼å¤„ç†æ”¯æŒ",
                "ä¼šè¯è®°å¿†ç®¡ç†",
                "ç»¼åˆé”™è¯¯å¤„ç†",
                "æ€§èƒ½ç›‘æ§",
                "å®¡è®¡æ—¥å¿—è®°å½•"
            ],
            "performance_stats": self.performance_stats
        }
    
    def get_workflow_graph(self) -> Optional[str]:
        """è·å–å·¥ä½œæµå›¾çš„å¯è§†åŒ–è¡¨ç¤º"""
        try:
            if self.compiled_graph:
                return self.compiled_graph.get_graph().draw_mermaid()
            return None
        except Exception as e:
            logger.warning(f"æ— æ³•ç”Ÿæˆå·¥ä½œæµå›¾: {e}")
            return None
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return dict(self.performance_stats)
    
    def reset_performance_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "average_response_time": 0.0,
            "node_performance": {}
        }
        
        logger.info("æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®")
        audit_log("performance_stats_reset", details={"workflow_id": self.workflow_id})


# ä½¿ç”¨å¢å¼ºç‰ˆå·¥ä½œæµæ›¿æ¢åŸæœ‰çš„å·¥ä½œæµ
IntelligentQAWorkflow = EnhancedIntelligentQAWorkflow

# å…¨å±€å·¥ä½œæµå®ä¾‹
_workflow_instance = None


def get_workflow() -> IntelligentQAWorkflow:
    """è·å–å…¨å±€å·¥ä½œæµå®ä¾‹"""
    global _workflow_instance
    if _workflow_instance is None:
        _workflow_instance = IntelligentQAWorkflow()
    return _workflow_instance


def reset_workflow():
    """é‡ç½®å…¨å±€å·¥ä½œæµå®ä¾‹"""
    global _workflow_instance
    if _workflow_instance:
        audit_log("workflow_reset", details={"workflow_id": _workflow_instance.workflow_id})
    _workflow_instance = None


# ä¾¿æ·å‡½æ•° - ç°åœ¨åŒ…å«é”™è¯¯å¤„ç†
async def query_async(user_query: str, 
                     config_override: Optional[Dict[str, Any]] = None,
                     thread_id: Optional[str] = None) -> Dict[str, Any]:
    """å¼‚æ­¥æŸ¥è¯¢ä¾¿æ·å‡½æ•°"""
    workflow = get_workflow()
    return await workflow.arun(user_query, config_override, thread_id)


def query(user_query: str, 
          config_override: Optional[Dict[str, Any]] = None,
          thread_id: Optional[str] = None) -> Dict[str, Any]:
    """åŒæ­¥æŸ¥è¯¢ä¾¿æ·å‡½æ•°"""
    workflow = get_workflow()
    return workflow.run(user_query, config_override, thread_id)


def query_stream(user_query: str, 
                 config_override: Optional[Dict[str, Any]] = None,
                 thread_id: Optional[str] = None):
    """æµå¼æŸ¥è¯¢ä¾¿æ·å‡½æ•°"""
    workflow = get_workflow()
    yield from workflow.stream(user_query, config_override, thread_id)


def get_workflow_info() -> Dict[str, Any]:
    """è·å–å·¥ä½œæµä¿¡æ¯"""
    workflow = get_workflow()
    return workflow.get_workflow_info()


def get_workflow_graph() -> Optional[str]:
    """è·å–å·¥ä½œæµå›¾"""
    workflow = get_workflow()
    return workflow.get_workflow_graph()


def get_performance_stats() -> Dict[str, Any]:
    """è·å–æ€§èƒ½ç»Ÿè®¡"""
    workflow = get_workflow()
    return workflow.get_performance_stats()


def reset_performance_stats():
    """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
    workflow = get_workflow()
    workflow.reset_performance_stats()


if __name__ == "__main__":
    """
    è¿è¡Œæ­¤æ–‡ä»¶ç”ŸæˆLangGraphç¼–è¯‘å¯è§†åŒ–å›¾
    """
    print("ğŸ”§ ç”ŸæˆLangGraphå·¥ä½œæµå¯è§†åŒ–å›¾")
    print("=" * 50)
    
    try:
        # è·å–å·¥ä½œæµå®ä¾‹ (è¿™ä¼šè‡ªåŠ¨ç¼–è¯‘å¹¶ç”Ÿæˆgraph.png)
        print("æ­£åœ¨åˆ›å»ºå·¥ä½œæµå®ä¾‹...")
        workflow = get_workflow()
        
        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        info = workflow.get_workflow_info()
        print(f"âœ… å·¥ä½œæµç¼–è¯‘æˆåŠŸ")
        print(f"ğŸ“Š å·¥ä½œæµID: {info['workflow_id']}")
        print(f"ğŸ“Š ç‰ˆæœ¬: {info['version']}")
        print(f"ğŸ“Š èŠ‚ç‚¹æ•°é‡: {len(info['nodes'])}")
        
        # æ£€æŸ¥ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶
        import os
        graph_file = "graph.png"
        if os.path.exists(graph_file):
            print(f"âœ… å¯è§†åŒ–å›¾ç‰‡å·²ç”Ÿæˆ: {graph_file}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(graph_file)} bytes")
            print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {os.path.abspath(graph_file)}")
        else:
            print("âš ï¸ å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå¯èƒ½ç”Ÿæˆå¤±è´¥")
        
        print("\nğŸ‰ LangGraphå¯è§†åŒ–å›¾ç”Ÿæˆå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()