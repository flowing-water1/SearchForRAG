"""
å¢å¼ºç‰ˆå·¥ä½œæµç¼–æ’ - é›†æˆç»¼åˆé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""

import asyncio
import time
import uuid
from typing import Dict, Any, List, Optional, Union
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from .state import AgentState
from .config import config
from ..agents.query_analysis import query_analysis_node
from ..agents.lightrag_retrieval import lightrag_retrieval_node
from ..agents.quality_assessment import quality_assessment_node
from ..agents.web_search import web_search_node
from ..agents.answer_generation import answer_generation_node
from ..utils.advanced_logging import (
    setup_logger, get_performance_logger, audit_log, 
    performance_context, record_metric
)
from ..utils.error_handling import (
    handle_errors, retry_on_failure, ErrorContext,
    SystemError, ConfigurationError, ErrorSeverity, ErrorCategory
)

logger = setup_logger(__name__)
perf_logger = get_performance_logger(__name__)


class EnhancedIntelligentQAWorkflow:
    """
    å¢å¼ºç‰ˆæ™ºèƒ½é—®ç­”å·¥ä½œæµç®¡ç†å™¨
    
    é›†æˆäº†å…¨é¢çš„é”™è¯¯å¤„ç†ã€æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—è®°å½•åŠŸèƒ½
    """
    
    def __init__(self):
        self.graph = None
        self.compiled_graph = None
        self.is_initialized = False
        self.workflow_id = str(uuid.uuid4())
        self.performance_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "average_response_time": 0.0,
            "node_performance": {}
        }
        
        # å®¡è®¡æ—¥å¿—
        audit_log("workflow_creation", details={"workflow_id": self.workflow_id})
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        self._initialize_workflow()
    
    @handle_errors(reraise=True)
    def _initialize_workflow(self):
        """åˆå§‹åŒ–å·¥ä½œæµå›¾"""
        with performance_context("workflow_initialization", __name__):
            logger.info(f"åˆå§‹åŒ–æ™ºèƒ½é—®ç­”å·¥ä½œæµ {self.workflow_id}...")
            
            try:
                # åˆ›å»ºçŠ¶æ€å›¾
                self.graph = StateGraph(AgentState)
                
                # æ·»åŠ èŠ‚ç‚¹
                self._add_nodes()
                
                # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
                self._add_edges()
                
                # ç¼–è¯‘å·¥ä½œæµ
                self._compile_workflow()
                
                self.is_initialized = True
                logger.info(f"âœ… æ™ºèƒ½é—®ç­”å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ {self.workflow_id}")
                
                # è®°å½•æŒ‡æ ‡
                record_metric("workflow_initialized", 1, workflow_id=self.workflow_id)
                
            except Exception as e:
                logger.error(f"âŒ å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
                raise ConfigurationError(
                    f"å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {str(e)}",
                    error_code="WORKFLOW_INIT_FAILED",
                    severity=ErrorSeverity.CRITICAL,
                    recovery_suggestions=[
                        "æ£€æŸ¥æ‰€æœ‰ä¾èµ–æ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…",
                        "éªŒè¯é…ç½®æ–‡ä»¶è®¾ç½®",
                        "æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—"
                    ]
                )
    
    def _add_nodes(self):
        """æ·»åŠ å·¥ä½œæµèŠ‚ç‚¹"""
        logger.info("æ·»åŠ å·¥ä½œæµèŠ‚ç‚¹...")
        
        # ä½¿ç”¨è£…é¥°å™¨åŒ…è£…èŠ‚ç‚¹ä»¥å¢åŠ é”™è¯¯å¤„ç†
        wrapped_nodes = {
            "query_analysis": self._wrap_node(query_analysis_node, "query_analysis"),
            "lightrag_retrieval": self._wrap_node(lightrag_retrieval_node, "lightrag_retrieval"),
            "quality_assessment": self._wrap_node(quality_assessment_node, "quality_assessment"),
            "web_search": self._wrap_node(web_search_node, "web_search"),
            "answer_generation": self._wrap_node(answer_generation_node, "answer_generation")
        }
        
        # æ·»åŠ èŠ‚ç‚¹åˆ°å›¾
        for node_name, node_func in wrapped_nodes.items():
            self.graph.add_node(node_name, node_func)
            logger.debug(f"æ·»åŠ èŠ‚ç‚¹: {node_name}")
        
        logger.info("æ‰€æœ‰èŠ‚ç‚¹å·²æ·»åŠ å®Œæˆ")
    
    def _wrap_node(self, node_func, node_name: str):
        """åŒ…è£…èŠ‚ç‚¹å‡½æ•°ä»¥å¢åŠ é”™è¯¯å¤„ç†å’Œæ€§èƒ½ç›‘æ§"""
        
        def wrapped_node(state: AgentState) -> Dict[str, Any]:
            node_start_time = time.time()
            
            try:
                with performance_context(f"node_{node_name}", f"workflow.{node_name}"):
                    logger.info(f"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
                    
                    # è®°å½•èŠ‚ç‚¹å¼€å§‹
                    audit_log(
                        "node_execution_start",
                        details={
                            "node_name": node_name,
                            "workflow_id": self.workflow_id,
                            "state_keys": list(state.keys())
                        }
                    )
                    
                    # æ‰§è¡ŒèŠ‚ç‚¹
                    result = node_func(state)
                    
                    # è®°å½•æ‰§è¡Œæ—¶é—´
                    execution_time = time.time() - node_start_time
                    self._update_node_performance(node_name, execution_time, True)
                    
                    logger.info(f"âœ… èŠ‚ç‚¹æ‰§è¡ŒæˆåŠŸ: {node_name} ({execution_time:.2f}s)")
                    
                    # è®°å½•èŠ‚ç‚¹å®Œæˆ
                    audit_log(
                        "node_execution_success",
                        details={
                            "node_name": node_name,
                            "workflow_id": self.workflow_id,
                            "execution_time": execution_time,
                            "result_keys": list(result.keys()) if result else []
                        }
                    )
                    
                    return result
                    
            except Exception as e:
                execution_time = time.time() - node_start_time
                self._update_node_performance(node_name, execution_time, False)
                
                logger.error(f"âŒ èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {node_name} ({execution_time:.2f}s) - {str(e)}")
                
                # è®°å½•é”™è¯¯
                audit_log(
                    "node_execution_error",
                    details={
                        "node_name": node_name,
                        "workflow_id": self.workflow_id,
                        "execution_time": execution_time,
                        "error": str(e)
                    }
                )
                
                # æ ¹æ®èŠ‚ç‚¹ç±»å‹å†³å®šé”™è¯¯å¤„ç†ç­–ç•¥
                if node_name in ["query_analysis", "lightrag_retrieval"]:
                    # å…³é”®èŠ‚ç‚¹é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise SystemError(
                        f"{node_name} èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}",
                        error_code=f"NODE_{node_name.upper()}_FAILED",
                        category=ErrorCategory.SYSTEM,
                        severity=ErrorSeverity.HIGH
                    )
                else:
                    # éå…³é”®èŠ‚ç‚¹é”™è¯¯ï¼Œè¿”å›é”™è¯¯çŠ¶æ€
                    return {
                        "error": str(e),
                        "node_name": node_name,
                        "execution_time": execution_time,
                        "success": False
                    }
        
        return wrapped_node
    
    def _update_node_performance(self, node_name: str, execution_time: float, success: bool):
        """æ›´æ–°èŠ‚ç‚¹æ€§èƒ½ç»Ÿè®¡"""
        if node_name not in self.performance_stats["node_performance"]:
            self.performance_stats["node_performance"][node_name] = {
                "total_executions": 0,
                "successful_executions": 0,
                "failed_executions": 0,
                "total_time": 0.0,
                "average_time": 0.0
            }
        
        stats = self.performance_stats["node_performance"][node_name]
        stats["total_executions"] += 1
        stats["total_time"] += execution_time
        stats["average_time"] = stats["total_time"] / stats["total_executions"]
        
        if success:
            stats["successful_executions"] += 1
        else:
            stats["failed_executions"] += 1
        
        # è®°å½•æŒ‡æ ‡
        record_metric(
            f"node_{node_name}_execution_time",
            execution_time,
            node_name=node_name,
            success=success
        )
    
    def _add_edges(self):
        """æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±"""
        logger.info("é…ç½®å·¥ä½œæµè·¯ç”±...")
        
        # è®¾ç½®å…¥å£ç‚¹
        self.graph.set_entry_point("query_analysis")
        
        # æŸ¥è¯¢åˆ†æ â†’ LightRAG æ£€ç´¢
        self.graph.add_edge("query_analysis", "lightrag_retrieval")
        
        # LightRAG æ£€ç´¢ â†’ è´¨é‡è¯„ä¼°
        self.graph.add_edge("lightrag_retrieval", "quality_assessment")
        
        # è´¨é‡è¯„ä¼° â†’ æ¡ä»¶è·¯ç”± (ç½‘ç»œæœç´¢ æˆ– ç­”æ¡ˆç”Ÿæˆ)
        self.graph.add_conditional_edges(
            "quality_assessment",
            self._should_use_web_search,
            {
                "web_search": "web_search",
                "answer_generation": "answer_generation"
            }
        )
        
        # ç½‘ç»œæœç´¢ â†’ ç­”æ¡ˆç”Ÿæˆ
        self.graph.add_edge("web_search", "answer_generation")
        
        # ç­”æ¡ˆç”Ÿæˆ â†’ ç»“æŸ
        self.graph.add_edge("answer_generation", END)
        
        logger.info("å·¥ä½œæµè·¯ç”±é…ç½®å®Œæˆ")
    
    def _should_use_web_search(self, state: AgentState) -> str:
        """
        å†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åç§°
        """
        try:
            need_web_search = state.get("need_web_search", False)
            confidence_score = state.get("confidence_score", 0.0)
            
            logger.debug(f"è´¨é‡è¯„ä¼°å†³ç­–: need_web_search={need_web_search}, confidence={confidence_score:.2f}")
            
            if need_web_search:
                logger.info("ğŸ” è´¨é‡è¯„ä¼°åˆ¤å®šéœ€è¦ç½‘ç»œæœç´¢è¡¥å……")
                record_metric("web_search_triggered", 1, confidence_score=confidence_score)
                return "web_search"
            else:
                logger.info("âœ… è´¨é‡è¯„ä¼°åˆ¤å®šå¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆ")
                record_metric("direct_answer_generation", 1, confidence_score=confidence_score)
                return "answer_generation"
                
        except Exception as e:
            logger.error(f"è·¯ç”±å†³ç­–å¤±è´¥: {e}")
            # é»˜è®¤åˆ°ç­”æ¡ˆç”Ÿæˆ
            return "answer_generation"
    
    @handle_errors(reraise=True)
    def _compile_workflow(self):
        """ç¼–è¯‘å·¥ä½œæµ"""
        logger.info("ç¼–è¯‘å·¥ä½œæµ...")
        
        try:
            # åˆ›å»ºå†…å­˜æ£€æŸ¥ç‚¹
            memory = MemorySaver()
            
            # ç¼–è¯‘å›¾
            self.compiled_graph = self.graph.compile(
                checkpointer=memory,
                debug=config.DEBUG_MODE
            )
            
            logger.info("âœ… å·¥ä½œæµç¼–è¯‘æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¼–è¯‘å¤±è´¥: {e}")
            raise ConfigurationError(
                f"å·¥ä½œæµç¼–è¯‘å¤±è´¥: {str(e)}",
                error_code="WORKFLOW_COMPILE_FAILED",
                severity=ErrorSeverity.CRITICAL
            )
    
    @handle_errors(reraise=True)
    @retry_on_failure(max_retries=2, backoff_factor=1.0)
    async def arun(self, 
                   user_query: str, 
                   config_override: Optional[Dict[str, Any]] = None,
                   thread_id: Optional[str] = None) -> Dict[str, Any]:
        """
        å¼‚æ­¥è¿è¡Œå·¥ä½œæµ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            config_override: é…ç½®è¦†ç›–
            thread_id: çº¿ç¨‹ID (ç”¨äºä¼šè¯ç®¡ç†)
            
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        if not self.is_initialized:
            raise SystemError(
                "å·¥ä½œæµæœªåˆå§‹åŒ–",
                error_code="WORKFLOW_NOT_INITIALIZED",
                severity=ErrorSeverity.HIGH
            )
        
        query_id = str(uuid.uuid4())
        start_time = time.time()
        
        with performance_context(f"workflow_execution_{query_id}", __name__):
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢ {query_id}: {user_query[:100]}...")
            
            # è®°å½•æŸ¥è¯¢å¼€å§‹
            audit_log(
                "query_start",
                details={
                    "query_id": query_id,
                    "user_query": user_query[:500],  # é™åˆ¶é•¿åº¦
                    "thread_id": thread_id,
                    "workflow_id": self.workflow_id
                }
            )
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = {
                "user_query": user_query,
                "query_id": query_id,
                "thread_id": thread_id or "default",
                "workflow_id": self.workflow_id,
                "start_time": start_time,
                "query_type": "",
                "lightrag_mode": "",
                "key_entities": [],
                "processed_query": "",
                "lightrag_results": {},
                "retrieval_score": 0.0,
                "retrieval_success": False,
                "confidence_score": 0.0,
                "need_web_search": False,
                "web_results": [],
                "final_answer": "",
                "sources": [],
                "context_used": 0,
                "answer_confidence": 0.0
            }
            
            # é…ç½®
            run_config = {
                "configurable": {
                    "thread_id": thread_id or "default"
                }
            }
            
            if config_override:
                run_config.update(config_override)
            
            try:
                # æ‰§è¡Œå·¥ä½œæµ
                result = await self.compiled_graph.ainvoke(
                    initial_state,
                    config=run_config
                )
                
                # è®¡ç®—æ‰§è¡Œæ—¶é—´
                execution_time = time.time() - start_time
                
                # æ›´æ–°ç»Ÿè®¡
                self._update_query_stats(True, execution_time)
                
                # è®°å½•æˆåŠŸ
                logger.info(f"âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ {query_id} ({execution_time:.2f}s)")
                
                audit_log(
                    "query_success",
                    details={
                        "query_id": query_id,
                        "execution_time": execution_time,
                        "final_answer_length": len(result.get("final_answer", "")),
                        "sources_count": len(result.get("sources", [])),
                        "confidence_score": result.get("answer_confidence", 0.0)
                    }
                )
                
                # æ·»åŠ æ‰§è¡Œä¿¡æ¯
                result.update({
                    "query_id": query_id,
                    "execution_time": execution_time,
                    "workflow_id": self.workflow_id,
                    "success": True
                })
                
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                self._update_query_stats(False, execution_time)
                
                logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥ {query_id} ({execution_time:.2f}s): {e}")
                
                audit_log(
                    "query_failure",
                    details={
                        "query_id": query_id,
                        "execution_time": execution_time,
                        "error": str(e)
                    }
                )
                
                raise
    
    def _update_query_stats(self, success: bool, execution_time: float):
        """æ›´æ–°æŸ¥è¯¢ç»Ÿè®¡"""
        self.performance_stats["total_queries"] += 1
        
        if success:
            self.performance_stats["successful_queries"] += 1
        else:
            self.performance_stats["failed_queries"] += 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total_time = (self.performance_stats["average_response_time"] * 
                     (self.performance_stats["total_queries"] - 1) + execution_time)
        self.performance_stats["average_response_time"] = total_time / self.performance_stats["total_queries"]
        
        # è®°å½•æŒ‡æ ‡
        record_metric("query_execution_time", execution_time, success=success)
        record_metric("total_queries", self.performance_stats["total_queries"])
        record_metric("success_rate", 
                     self.performance_stats["successful_queries"] / self.performance_stats["total_queries"])
    
    def run(self, 
            user_query: str, 
            config_override: Optional[Dict[str, Any]] = None,
            thread_id: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥è¿è¡Œå·¥ä½œæµ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            config_override: é…ç½®è¦†ç›–
            thread_id: çº¿ç¨‹ID
            
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        return asyncio.run(self.arun(user_query, config_override, thread_id))
    
    def stream(self, 
               user_query: str, 
               config_override: Optional[Dict[str, Any]] = None,
               thread_id: Optional[str] = None):
        """
        æµå¼æ‰§è¡Œå·¥ä½œæµ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            config_override: é…ç½®è¦†ç›–
            thread_id: çº¿ç¨‹ID
            
        Yields:
            å·¥ä½œæµæ‰§è¡Œæ­¥éª¤
        """
        if not self.is_initialized:
            raise SystemError(
                "å·¥ä½œæµæœªåˆå§‹åŒ–",
                error_code="WORKFLOW_NOT_INITIALIZED",
                severity=ErrorSeverity.HIGH
            )
        
        query_id = str(uuid.uuid4())
        start_time = time.time()
        
        logger.info(f"å¼€å§‹æµå¼å¤„ç†æŸ¥è¯¢ {query_id}: {user_query[:100]}...")
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "user_query": user_query,
            "query_id": query_id,
            "thread_id": thread_id or "default",
            "workflow_id": self.workflow_id,
            "start_time": start_time,
            "query_type": "",
            "lightrag_mode": "",
            "key_entities": [],
            "processed_query": "",
            "lightrag_results": {},
            "retrieval_score": 0.0,
            "retrieval_success": False,
            "confidence_score": 0.0,
            "need_web_search": False,
            "web_results": [],
            "final_answer": "",
            "sources": [],
            "context_used": 0,
            "answer_confidence": 0.0
        }
        
        # é…ç½®
        run_config = {
            "configurable": {
                "thread_id": thread_id or "default"
            }
        }
        
        if config_override:
            run_config.update(config_override)
        
        try:
            # æµå¼æ‰§è¡Œ
            for step in self.compiled_graph.stream(initial_state, config=run_config):
                # æ·»åŠ æ—¶é—´æˆ³å’ŒæŸ¥è¯¢ID
                if step:
                    step_data = list(step.values())[0] if step else {}
                    step_data.update({
                        "query_id": query_id,
                        "timestamp": time.time(),
                        "workflow_id": self.workflow_id
                    })
                    
                yield step
                
            # è®°å½•æˆåŠŸ
            execution_time = time.time() - start_time
            self._update_query_stats(True, execution_time)
            
            logger.info(f"âœ… æµå¼å·¥ä½œæµæ‰§è¡Œå®Œæˆ {query_id} ({execution_time:.2f}s)")
                
        except Exception as e:
            execution_time = time.time() - start_time
            self._update_query_stats(False, execution_time)
            
            logger.error(f"âŒ æµå¼å·¥ä½œæµæ‰§è¡Œå¤±è´¥ {query_id} ({execution_time:.2f}s): {e}")
            raise
    
    def get_workflow_info(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµä¿¡æ¯"""
        return {
            "workflow_id": self.workflow_id,
            "name": "å¢å¼ºç‰ˆæ™ºèƒ½é—®ç­”å·¥ä½œæµ",
            "version": "2.0.0",
            "initialized": self.is_initialized,
            "nodes": [
                {
                    "name": "query_analysis",
                    "description": "æŸ¥è¯¢åˆ†æèŠ‚ç‚¹",
                    "function": "åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®å®šæŸ¥è¯¢ç±»å‹å¹¶é€‰æ‹©æœ€ä½³LightRAGæ¨¡å¼"
                },
                {
                    "name": "lightrag_retrieval", 
                    "description": "LightRAGæ£€ç´¢èŠ‚ç‚¹ (HKUDS/LightRAG)",
                    "function": "ä½¿ç”¨HKUDS/LightRAGæ‰§è¡Œæ™ºèƒ½æ£€ç´¢ï¼Œæ”¯æŒnaive/local/global/hybrid/mixæ¨¡å¼"
                },
                {
                    "name": "quality_assessment",
                    "description": "è´¨é‡è¯„ä¼°èŠ‚ç‚¹",
                    "function": "è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢è¡¥å……"
                },
                {
                    "name": "web_search",
                    "description": "ç½‘ç»œæœç´¢èŠ‚ç‚¹",
                    "function": "å½“æœ¬åœ°ä¿¡æ¯ä¸è¶³æ—¶ï¼Œä»ç½‘ç»œè·å–è¡¥å……ä¿¡æ¯"
                },
                {
                    "name": "answer_generation",
                    "description": "ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹",
                    "function": "æ•´åˆæœ¬åœ°å’Œç½‘ç»œä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"
                }
            ],
            "workflow_pattern": "æŸ¥è¯¢åˆ†æ â†’ HKUDS/LightRAGæ£€ç´¢ â†’ è´¨é‡è¯„ä¼° â†’ [ç½‘ç»œæœç´¢] â†’ ç­”æ¡ˆç”Ÿæˆ",
            "features": [
                "æ™ºèƒ½æŸ¥è¯¢åˆ†æå’Œè·¯ç”±",
                "å¤šæ¨¡å¼HKUDS/LightRAGæ£€ç´¢",
                "è´¨é‡è¯„ä¼°å’Œå†³ç­–",
                "ç½‘ç»œæœç´¢è¡¥å……",
                "ç­”æ¡ˆç”Ÿæˆå’Œæ•´åˆ",
                "æµå¼å¤„ç†æ”¯æŒ",
                "ä¼šè¯è®°å¿†ç®¡ç†",
                "ç»¼åˆé”™è¯¯å¤„ç†",
                "æ€§èƒ½ç›‘æ§",
                "å®¡è®¡æ—¥å¿—è®°å½•"
            ],
            "performance_stats": self.performance_stats
        }
    
    def get_workflow_graph(self) -> Optional[str]:
        """è·å–å·¥ä½œæµå›¾çš„å¯è§†åŒ–è¡¨ç¤º"""
        try:
            if self.compiled_graph:
                return self.compiled_graph.get_graph().draw_mermaid()
            return None
        except Exception as e:
            logger.warning(f"æ— æ³•ç”Ÿæˆå·¥ä½œæµå›¾: {e}")
            return None
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return dict(self.performance_stats)
    
    def reset_performance_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "average_response_time": 0.0,
            "node_performance": {}
        }
        
        logger.info("æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®")
        audit_log("performance_stats_reset", details={"workflow_id": self.workflow_id})


# ä½¿ç”¨å¢å¼ºç‰ˆå·¥ä½œæµæ›¿æ¢åŸæœ‰çš„å·¥ä½œæµ
IntelligentQAWorkflow = EnhancedIntelligentQAWorkflow

# å…¨å±€å·¥ä½œæµå®ä¾‹
_workflow_instance = None


def get_workflow() -> IntelligentQAWorkflow:
    """è·å–å…¨å±€å·¥ä½œæµå®ä¾‹"""
    global _workflow_instance
    if _workflow_instance is None:
        _workflow_instance = IntelligentQAWorkflow()
    return _workflow_instance


def reset_workflow():
    """é‡ç½®å…¨å±€å·¥ä½œæµå®ä¾‹"""
    global _workflow_instance
    if _workflow_instance:
        audit_log("workflow_reset", details={"workflow_id": _workflow_instance.workflow_id})
    _workflow_instance = None


# ä¾¿æ·å‡½æ•° - ç°åœ¨åŒ…å«é”™è¯¯å¤„ç†
async def query_async(user_query: str, 
                     config_override: Optional[Dict[str, Any]] = None,
                     thread_id: Optional[str] = None) -> Dict[str, Any]:
    """å¼‚æ­¥æŸ¥è¯¢ä¾¿æ·å‡½æ•°"""
    workflow = get_workflow()
    return await workflow.arun(user_query, config_override, thread_id)


def query(user_query: str, 
          config_override: Optional[Dict[str, Any]] = None,
          thread_id: Optional[str] = None) -> Dict[str, Any]:
    """åŒæ­¥æŸ¥è¯¢ä¾¿æ·å‡½æ•°"""
    workflow = get_workflow()
    return workflow.run(user_query, config_override, thread_id)


def query_stream(user_query: str, 
                 config_override: Optional[Dict[str, Any]] = None,
                 thread_id: Optional[str] = None):
    """æµå¼æŸ¥è¯¢ä¾¿æ·å‡½æ•°"""
    workflow = get_workflow()
    yield from workflow.stream(user_query, config_override, thread_id)


def get_workflow_info() -> Dict[str, Any]:
    """è·å–å·¥ä½œæµä¿¡æ¯"""
    workflow = get_workflow()
    return workflow.get_workflow_info()


def get_workflow_graph() -> Optional[str]:
    """è·å–å·¥ä½œæµå›¾"""
    workflow = get_workflow()
    return workflow.get_workflow_graph()


def get_performance_stats() -> Dict[str, Any]:
    """è·å–æ€§èƒ½ç»Ÿè®¡"""
    workflow = get_workflow()
    return workflow.get_performance_stats()


def reset_performance_stats():
    """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
    workflow = get_workflow()
    workflow.reset_performance_stats()