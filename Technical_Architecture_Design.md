# æ™ºèƒ½é—®ç­”ç³»ç»ŸæŠ€æœ¯æ¶æ„è®¾è®¡æ–‡æ¡£

## 1. é¡¹ç›®æ¦‚è¿°ä¸æŠ€æœ¯èƒŒæ™¯

### 1.1 é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ªåŸºäº Agentic RAG çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä»æœ¬åœ°æ–‡æ¡£åº“æ£€ç´¢ä¿¡æ¯ï¼Œå½“æœ¬åœ°çŸ¥è¯†ä¸è¶³æ—¶æ™ºèƒ½è°ƒç”¨ç½‘ç»œæœç´¢ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å…¨é¢çš„ç­”æ¡ˆã€‚

### 1.2 MVPç‰ˆæœ¬åŠŸèƒ½è¾¹ç•Œ

**MVP 1.0 æ ¸å¿ƒåŠŸèƒ½**ï¼š

- åŸºäºLightRAGçš„æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ï¼ˆæ”¯æŒå‘é‡æ£€ç´¢å’Œå›¾æ£€ç´¢
- æ™ºèƒ½è´¨é‡è¯„ä¼°å’Œç½‘ç»œæœç´¢è¡¥å……
- Streamlit Webç•Œé¢
- åŸºæœ¬çš„å®æ—¶æµå¼æ˜¾ç¤º

**MVP 1.0 ä¸åŒ…å«**ï¼š

- å¤æ‚çš„å¤šè½®å¯¹è¯
- ç”¨æˆ·ç®¡ç†å’Œæƒé™æ§åˆ¶
- é«˜çº§å¯è§†åŒ–å’Œåˆ†æåŠŸèƒ½
- å‰ç«¯Reacté¡µé¢
- æœåŠ¡å™¨è¿ç»´å®‰å…¨
- Graphitiå®æ—¶å›¾è°±æ›´æ–°

### 1.3 æŠ€æœ¯èƒŒæ™¯è§£é‡Š

#### 1.3.1 ä»€ä¹ˆæ˜¯RAGï¼Ÿ

**RAGï¼ˆRetrieval Augmented Generationï¼‰**æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œç”Ÿæˆå¼AIçš„æŠ€æœ¯æ¶æ„ï¼š

- **ä¼ ç»Ÿæ–¹å¼çš„é—®é¢˜**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å¼ºå¤§ï¼Œä½†å­˜åœ¨çŸ¥è¯†æˆªæ­¢æ—¶é—´é™åˆ¶ï¼Œæ— æ³•è·å–æœ€æ–°ä¿¡æ¯ï¼Œä¸”å¯èƒ½äº§ç”Ÿå¹»è§‰ï¼ˆç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼‰
- **RAGçš„è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨ç”Ÿæˆç­”æ¡ˆå‰ï¼Œå…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶ååŸºäºæ£€ç´¢åˆ°çš„çœŸå®ä¿¡æ¯æ¥ç”Ÿæˆç­”æ¡ˆ
- **ä¼˜åŠ¿**ï¼šç¡®ä¿ç­”æ¡ˆåŸºäºçœŸå®æ•°æ®ï¼Œå¯ä»¥æ•´åˆæœ€æ–°ä¿¡æ¯ï¼Œå‡å°‘å¹»è§‰ç°è±¡

#### 1.3.2 ä»€ä¹ˆæ˜¯Agentic RAGï¼Ÿ

**Agentic RAG**æ˜¯RAGæŠ€æœ¯çš„è¿›åŒ–ç‰ˆæœ¬ï¼Œå¼•å…¥äº†æ™ºèƒ½ä»£ç†ï¼ˆAgentï¼‰çš„æ¦‚å¿µï¼š

- **ä¼ ç»ŸRAG**ï¼šæ£€ç´¢â†’ç”Ÿæˆï¼Œæµç¨‹å›ºå®š
- **Agentic RAG**ï¼šæ™ºèƒ½ä»£ç†å¯ä»¥æ ¹æ®æŸ¥è¯¢ç±»å‹å’Œç»“æœè´¨é‡ï¼ŒåŠ¨æ€å†³å®šæ£€ç´¢ç­–ç•¥ã€æ˜¯å¦éœ€è¦å¤šè½®æ£€ç´¢ã€æ˜¯å¦è°ƒç”¨å¤–éƒ¨å·¥å…·ç­‰
- **ä¸ºä»€ä¹ˆéœ€è¦**ï¼šä¸åŒç±»å‹çš„é—®é¢˜éœ€è¦ä¸åŒçš„æ£€ç´¢ç­–ç•¥ï¼ŒAgentic RAGå¯ä»¥æ™ºèƒ½é€‰æ‹©æœ€ä½³è·¯å¾„

#### 1.3.3 æ ¸å¿ƒæŠ€æœ¯ç»„ä»¶

**LightRAG**ï¼š

- **ä½œç”¨**ï¼šè½»é‡çº§çš„RAGæ¡†æ¶ï¼Œå¤„ç†æ–‡æ¡£å‘é‡åŒ–ã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ£€ç´¢
- **é‡è¦è¯´æ˜**ï¼šä½¿ç”¨æ­£ç¡®çš„HKUDS/LightRAGæ¡†æ¶ (https://github.com/HKUDS/LightRAG)
- **ä¸ºä»€ä¹ˆé€‰æ‹©**ï¼šHKUDS/LightRAGå†…ç½®å¤šç§æ£€ç´¢æ¨¡å¼ï¼ˆnaiveã€localã€globalã€hybridã€mixï¼‰ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œå‘é‡æ£€ç´¢å’Œå›¾æ£€ç´¢ï¼Œæ»¡è¶³ä¸åŒæŸ¥è¯¢éœ€æ±‚
- **æ£€ç´¢æ¨¡å¼**ï¼š
  - naiveæ¨¡å¼ï¼šåŸºç¡€æ£€ç´¢æ¨¡å¼
  - localæ¨¡å¼ï¼šçº¯å‘é‡æ£€ç´¢ï¼Œé€‚åˆäº‹å®æ€§æŸ¥è¯¢
  - globalæ¨¡å¼ï¼šå›¾æ£€ç´¢ï¼Œé€‚åˆå…³ç³»æ€§æŸ¥è¯¢
  - hybridæ¨¡å¼ï¼šæ··åˆæ£€ç´¢ï¼Œé€‚åˆå¤æ‚åˆ†ææ€§æŸ¥è¯¢
  - mixæ¨¡å¼ï¼šç»¼åˆæ¨¡å¼

**LangGraph**ï¼š

- **ä½œç”¨**ï¼šæ™ºèƒ½å·¥ä½œæµç¼–æ’å¼•æ“
- **ä¸ºä»€ä¹ˆéœ€è¦**ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹åŠ¨æ€é€‰æ‹©æœ€ä½³æ£€ç´¢ç­–ç•¥ï¼Œå®ç°æ™ºèƒ½è·¯ç”±å’Œè´¨é‡è¯„ä¼°

**æ•°æ®å­˜å‚¨**ï¼š

- **LightRAGå†…ç½®å­˜å‚¨**ï¼šè´Ÿè´£å‘é‡å­˜å‚¨ã€å›¾æ•°æ®ç®¡ç†å’Œæ–‡æ¡£å¤„ç†
- **Neo4j**ï¼šå›¾æ•°æ®åº“ï¼Œå­˜å‚¨LightRAGç”Ÿæˆçš„çŸ¥è¯†å›¾è°±
- **PostgreSQL**ï¼šå…³ç³»æ•°æ®åº“ï¼Œæ”¯æŒå‘é‡æ‰©å±•ï¼Œå­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®

### 1.4 MVPæ ¸å¿ƒåŠŸèƒ½

- **æ™ºèƒ½é—®ç­”**ï¼šè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œç†è§£ç”¨æˆ·æ„å›¾å¹¶é€‰æ‹©åˆé€‚çš„æ£€ç´¢ç­–ç•¥
- **å¤šæ¨¡å¼æ£€ç´¢**ï¼šåŸºäºLightRAGçš„local/global/hybridæ£€ç´¢æ¨¡å¼
- **è´¨é‡è¯„ä¼°**ï¼šæ™ºèƒ½è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦è¡¥å……æœç´¢
- **ç½‘ç»œæœç´¢è¡¥å……**ï¼šå½“æœ¬åœ°çŸ¥è¯†ç½®ä¿¡åº¦ä¸è¶³æ—¶ï¼Œè‡ªåŠ¨è°ƒç”¨ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯
- **å®æ—¶æµå¼æ˜¾ç¤º**ï¼šç”¨æˆ·å¯ä»¥çœ‹åˆ°ç³»ç»Ÿçš„æ€è€ƒè¿‡ç¨‹å’Œä¿¡æ¯æ¥æº

### 1.5 MVPæŠ€æœ¯æ ˆ

#### æ ¸å¿ƒæ¡†æ¶

- **HKUDS/LightRAG**: è½»é‡çº§RAGå¼•æ“ï¼Œè´Ÿè´£æ–‡æ¡£å¤„ç†ã€å‘é‡æ£€ç´¢ã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ£€ç´¢
- **LangGraph**: æ™ºèƒ½ä»£ç†å·¥ä½œæµç¼–æ’
- **Streamlit**: å‰ç«¯ç•Œé¢å’Œç”¨æˆ·äº¤äº’

#### æ•°æ®å­˜å‚¨

- **Neo4j**: å›¾æ•°æ®åº“ï¼Œå­˜å‚¨HKUDS/LightRAGç”Ÿæˆçš„çŸ¥è¯†å›¾è°±
- **PostgreSQL**: å…³ç³»æ•°æ®åº“ï¼Œæ”¯æŒpgvectoræ‰©å±•ï¼Œå­˜å‚¨å‘é‡æ•°æ®
- **æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ**: åŸå§‹æ–‡æ¡£å­˜å‚¨

#### å¤–éƒ¨æœåŠ¡

- **OpenAI API**: LLMæœåŠ¡å’ŒåµŒå…¥æ¨¡å‹
- **Tavily API**: ç½‘ç»œæœç´¢æœåŠ¡

## 2. ç³»ç»Ÿæ¶æ„è®¾è®¡

### 2.1 MVPæ™ºèƒ½æ£€ç´¢å·¥ä½œæµç¨‹å›¾

```mermaid
graph TB
    Start([å¼€å§‹]) --> QueryAnalysis[æŸ¥è¯¢åˆ†æèŠ‚ç‚¹<br/>åˆ†æç”¨æˆ·æ„å›¾å’ŒæŸ¥è¯¢ç±»å‹]
  
    QueryAnalysis --> StrategyRoute{æ£€ç´¢ç­–ç•¥è·¯ç”±<br/>é€‰æ‹©LightRAGæ£€ç´¢æ¨¡å¼}
  
    StrategyRoute -->|äº‹å®æ€§æŸ¥è¯¢| LocalSearch[LightRAG Localæ¨¡å¼<br/>å‘é‡æ£€ç´¢]
    StrategyRoute -->|å…³ç³»æ€§æŸ¥è¯¢| GlobalSearch[LightRAG Globalæ¨¡å¼<br/>å›¾æ£€ç´¢]
    StrategyRoute -->|å¤æ‚æŸ¥è¯¢| HybridSearch[LightRAG Hybridæ¨¡å¼<br/>æ··åˆæ£€ç´¢]
  
    LocalSearch --> QualityCheck{ç»“æœè´¨é‡è¯„ä¼°<br/>è¯„ä¼°ç½®ä¿¡åº¦}
    GlobalSearch --> QualityCheck
    HybridSearch --> QualityCheck
  
    QualityCheck -->|ç½®ä¿¡åº¦ >= é˜ˆå€¼| AnswerGen[ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹<br/>æ•´åˆæœ¬åœ°çŸ¥è¯†ç”Ÿæˆç­”æ¡ˆ]
    QualityCheck -->|ç½®ä¿¡åº¦ < é˜ˆå€¼| WebSearch[ç½‘ç»œæœç´¢èŠ‚ç‚¹<br/>è¡¥å……å¤–éƒ¨ä¿¡æ¯]
  
    WebSearch --> AnswerGen
    AnswerGen --> StreamResult[æµå¼è¾“å‡ºç»“æœ]
    StreamResult --> End([ç»“æŸ])
  
    subgraph "å­˜å‚¨å±‚"
        LightRAGStorage[(HKUDS/LightRAGå­˜å‚¨<br/>å‘é‡+å›¾æ•°æ®)]
        Documents[(æ–‡æ¡£å­˜å‚¨)]
    end
  
    subgraph "å¤–éƒ¨æœåŠ¡"
        OpenAI[OpenAI API<br/>LLM+åµŒå…¥æœåŠ¡]
        TavilyAPI[Tavily API<br/>ç½‘ç»œæœç´¢]
    end
  
    LocalSearch -.-> LightRAGStorage
    GlobalSearch -.-> LightRAGStorage
    HybridSearch -.-> LightRAGStorage
    WebSearch -.-> TavilyAPI
    AnswerGen -.-> OpenAI
  
    classDef nodeStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef decisionStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef dataStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef serviceStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
  
    class QueryAnalysis,LocalSearch,GlobalSearch,HybridSearch,AnswerGen,StreamResult nodeStyle
    class StrategyRoute,QualityCheck decisionStyle
    class LightRAGStorage,Documents dataStyle
    class OpenAI,TavilyAPI serviceStyle
```

### 2.2 MVPç³»ç»Ÿæ¶æ„åˆ†å±‚

```mermaid
graph TB
    subgraph "ç”¨æˆ·ç•Œé¢å±‚"
        Streamlit[Streamlit Webç•Œé¢]
    end
  
    subgraph "æ™ºèƒ½ä»£ç†å±‚"
        LangGraph[LangGraphå·¥ä½œæµå¼•æ“]
        Agents[æ™ºèƒ½ä»£ç†èŠ‚ç‚¹é›†åˆ]
    end
  
    subgraph "RAGå¼•æ“å±‚" 
        LightRAG[HKUDS/LightRAGæ£€ç´¢å¼•æ“<br/>æ”¯æŒnaive/local/global/hybrid/mixæ¨¡å¼<br/>å†…ç½®çŸ¥è¯†å›¾è°±æ„å»º]
    end
  
    subgraph "å­˜å‚¨å±‚"
        Neo4j[Neo4jå›¾æ•°æ®åº“<br/>çŸ¥è¯†å›¾è°±å­˜å‚¨]
        PostgreSQL[PostgreSQLæ•°æ®åº“<br/>å‘é‡å­˜å‚¨]
        Documents[æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ<br/>æ–‡æ¡£å­˜å‚¨]
    end
  
    subgraph "å¤–éƒ¨æœåŠ¡å±‚"
        OpenAI[OpenAI API<br/>LLM+åµŒå…¥æœåŠ¡]
        TavilyAPI[Tavilyæœç´¢API]
    end
  
    Streamlit <--> LangGraph
    LangGraph <--> Agents
    Agents <--> LightRAG
    LightRAG <--> Neo4j
    LightRAG <--> PostgreSQL
    LightRAG <--> Documents
    Agents <--> OpenAI
    Agents <--> TavilyAPI
```

### 2.3 MVPæ¶æ„è®¾è®¡ç†å¿µ

#### 2.3.1 ç®€åŒ–æ¶æ„çš„ä¼˜åŠ¿

**MVPè®¾è®¡åŸåˆ™**ï¼š

- **å•ä¸€èŒè´£**ï¼šHKUDS/LightRAGä¸“æ³¨äºæ‰€æœ‰æ£€ç´¢ä»»åŠ¡ï¼Œé¿å…æŠ€æœ¯æ ˆå¤æ‚åŒ–
- **å¿«é€Ÿè¿­ä»£**ï¼šä½¿ç”¨æˆç†Ÿç¨³å®šçš„ç»„ä»¶ï¼Œå‡å°‘é›†æˆé£é™©
- **èµ„æºä¼˜åŒ–**ï¼šå‡å°‘å¤–éƒ¨ä¾èµ–ï¼Œé™ä½éƒ¨ç½²å’Œç»´æŠ¤æˆæœ¬

**æ™ºèƒ½è·¯ç”±çš„ä»·å€¼**ï¼š

- **ç­–ç•¥é€‰æ‹©**ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„HKUDS/LightRAGæ£€ç´¢æ¨¡å¼
- **è´¨é‡ä¿è¯**ï¼šæ™ºèƒ½è¯„ä¼°ç»“æœè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦å¤–éƒ¨è¡¥å……
- **ç”¨æˆ·ä½“éªŒ**ï¼šæä¾›å®æ—¶åé¦ˆå’Œé€æ˜çš„å¤„ç†è¿‡ç¨‹

#### 2.3.2 MVPæ•°æ®æµè½¬è¿‡ç¨‹

**ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢åˆ†æ**

```
ç”¨æˆ·è¾“å…¥ â†’ æŸ¥è¯¢åˆ†æèŠ‚ç‚¹ â†’ ç¡®å®šæŸ¥è¯¢ç±»å‹å’Œæ£€ç´¢ç­–ç•¥
```

- **ç›®çš„**ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢æ„å›¾ï¼Œé€‰æ‹©æœ€é€‚åˆçš„HKUDS/LightRAGæ£€ç´¢æ¨¡å¼
- **ç­–ç•¥é€‰æ‹©**ï¼š
  - äº‹å®æ€§æŸ¥è¯¢ â†’ localæ¨¡å¼ï¼ˆå‘é‡æ£€ç´¢ï¼‰
  - å…³ç³»æ€§æŸ¥è¯¢ â†’ globalæ¨¡å¼ï¼ˆå›¾æ£€ç´¢ï¼‰
  - å¤æ‚æŸ¥è¯¢ â†’ hybridæ¨¡å¼ï¼ˆæ··åˆæ£€ç´¢ï¼‰

**ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½æ£€ç´¢**

```
æŸ¥è¯¢ç±»å‹ â†’ é€‰æ‹©HKUDS/LightRAGæ¨¡å¼ â†’ æ‰§è¡Œæ£€ç´¢ â†’ è·å–ç»“æœ
```

- **localæ¨¡å¼**ï¼šåŸºäºå‘é‡ç›¸ä¼¼æ€§æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
- **globalæ¨¡å¼**ï¼šåŸºäºå›¾å…³ç³»æ‰¾åˆ°å®ä½“è¿æ¥å’Œå…³ç³»ä¿¡æ¯
- **hybridæ¨¡å¼**ï¼šç»“åˆå‘é‡å’Œå›¾æ£€ç´¢ï¼Œæä¾›æœ€å…¨é¢çš„ä¿¡æ¯

**ç¬¬ä¸‰æ­¥ï¼šè´¨é‡è¯„ä¼°**

```
æ£€ç´¢ç»“æœ â†’ ç½®ä¿¡åº¦è¯„ä¼° â†’ å†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
```

- **è¯„ä¼°ç»´åº¦**ï¼šä¿¡æ¯å®Œæ•´æ€§ã€ç›¸å…³æ€§ã€å¯ä¿¡åº¦
- **æ™ºèƒ½å†³ç­–**ï¼šæ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼å†³å®šæ˜¯å¦è¡¥å……å¤–éƒ¨ä¿¡æ¯

**ç¬¬å››æ­¥ï¼šç­”æ¡ˆç”Ÿæˆ**

```
æœ¬åœ°ç»“æœ + ç½‘ç»œè¡¥å…… â†’ LLMèåˆç”Ÿæˆ â†’ æ ‡æ³¨æ¥æºè¾“å‡º
```

- **ä¿¡æ¯æ•´åˆ**ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ°å¯ä¿¡ä¿¡æ¯ï¼Œç½‘ç»œä¿¡æ¯ä½œä¸ºè¡¥å……
- **é€æ˜è¾“å‡º**ï¼šæ¸…æ¥šæ ‡æ˜æ¯ä¸ªä¿¡æ¯çš„æ¥æºå’Œç½®ä¿¡åº¦

## 3. LangGraphèŠ‚ç‚¹è¯¦ç»†å®ç°

### 3.1 æ¯ä¸ªèŠ‚ç‚¹çš„åŠŸèƒ½è§£é‡Šä¸ä»£ç å®ç°

#### 3.1.1 æŸ¥è¯¢åˆ†æèŠ‚ç‚¹ (QueryAnalysisNode)

**èŠ‚ç‚¹ä½œç”¨**ï¼šç†è§£ç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œé€‰æ‹©æœ€ä½³çš„LightRAGæ£€ç´¢ç­–ç•¥
**ä¸ºä»€ä¹ˆéœ€è¦**ï¼šä¸åŒç±»å‹çš„é—®é¢˜éœ€è¦ä¸åŒçš„LightRAGæ£€ç´¢æ¨¡å¼

- äº‹å®æ€§é—®é¢˜ï¼š"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ" â†’ é€‚åˆlocalæ¨¡å¼ï¼ˆå‘é‡æ£€ç´¢ï¼‰
- å…³ç³»æ€§é—®é¢˜ï¼š"è°å‘æ˜äº†æœºå™¨å­¦ä¹ ï¼Ÿ" â†’ é€‚åˆglobalæ¨¡å¼ï¼ˆå›¾æ£€ç´¢ï¼‰
- å¤æ‚åˆ†æï¼š"æœºå™¨å­¦ä¹ å¯¹æœªæ¥çš„å½±å“ï¼Ÿ" â†’ é€‚åˆhybridæ¨¡å¼ï¼ˆæ··åˆæ£€ç´¢ï¼‰

```python
from typing import TypedDict
from langgraph import StateGraph
from langchain_openai import ChatOpenAI
import json

class AgentState(TypedDict):
    user_query: str
    query_type: str
    processed_query: str
    lightrag_mode: str
    lightrag_results: dict
    web_results: list
    confidence_score: float
    need_web_search: bool
    final_answer: str
    sources: list

def query_analysis_node(state: AgentState):
    """åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­æŸ¥è¯¢ç±»å‹å¹¶é€‰æ‹©æœ€ä½³LightRAGæ£€ç´¢æ¨¡å¼"""
    llm = ChatOpenAI(model="gpt-4", temperature=0)
  
    analysis_prompt = f"""
    åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œç¡®å®šæœ€é€‚åˆçš„LightRAGæ£€ç´¢æ¨¡å¼ï¼š
  
    æŸ¥è¯¢ï¼š{state["user_query"]}
  
    è¯·åˆ¤æ–­æŸ¥è¯¢ç±»å‹å¹¶é€‰æ‹©æ£€ç´¢æ¨¡å¼ï¼š
    1. FACTUAL (äº‹å®æ€§) â†’ localæ¨¡å¼: å¯»æ‰¾å…·ä½“äº‹å®ã€å®šä¹‰ã€æ¦‚å¿µ
    2. RELATIONAL (å…³ç³»æ€§) â†’ globalæ¨¡å¼: æ¢ç´¢å®ä½“é—´å…³ç³»ã€å½±å“ã€è”ç³»
    3. ANALYTICAL (åˆ†ææ€§) â†’ hybridæ¨¡å¼: éœ€è¦ç»¼åˆåˆ†æã€æ¨ç†ã€å¤šç»´ä¿¡æ¯
  
    è¿”å›JSONæ ¼å¼ï¼š
    {{
        "query_type": "FACTUAL/RELATIONAL/ANALYTICAL",
        "lightrag_mode": "local/global/hybrid",
        "key_entities": ["å®ä½“1", "å®ä½“2"],
        "processed_query": "ä¼˜åŒ–åçš„æŸ¥è¯¢",
        "reasoning": "é€‰æ‹©è¯¥æ¨¡å¼çš„åŸå› "
    }}
    """
  
    result = llm.invoke(analysis_prompt)
    analysis = json.loads(result.content)
  
    return {
        "query_type": analysis["query_type"],
        "lightrag_mode": analysis["lightrag_mode"],
        "processed_query": analysis["processed_query"],
        "key_entities": analysis["key_entities"],
        "mode_reasoning": analysis["reasoning"]
    }
```

#### 3.1.2 LightRAGæ£€ç´¢èŠ‚ç‚¹

**èŠ‚ç‚¹ä½œç”¨**ï¼šåŸºäºæŸ¥è¯¢åˆ†æç»“æœï¼Œä½¿ç”¨å¯¹åº”çš„LightRAGæ£€ç´¢æ¨¡å¼è·å–ç›¸å…³ä¿¡æ¯
**ä¸ºä»€ä¹ˆé‡è¦**ï¼šLightRAGæ˜¯å”¯ä¸€çš„çŸ¥è¯†æ£€ç´¢å¼•æ“ï¼Œå…¶å¤šæ¨¡å¼æ£€ç´¢èƒ½åŠ›ç¡®ä¿è·å¾—æœ€ç›¸å…³çš„ä¿¡æ¯

```python
from lightrag import LightRAG, QueryParam

def lightrag_retrieval_node(state: AgentState):
    """ä½¿ç”¨LightRAGè¿›è¡Œæ™ºèƒ½æ£€ç´¢"""
  
    # åˆå§‹åŒ–LightRAGå®¢æˆ·ç«¯ï¼ˆé…ç½®Neo4jå’ŒPostgreSQLï¼‰
    lightrag_client = LightRAG(
        working_dir="./rag_storage",
        llm_model_func=gpt_4o_mini_complete,
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(texts, model="text-embedding-3-large")
        ),
        graph_storage="Neo4JStorage",
        vector_storage="PGVectorStorage",
        chunk_token_size=1200,
        chunk_overlap_token_size=100
    )
  
    # æ ¹æ®æŸ¥è¯¢åˆ†æç»“æœé€‰æ‹©æ£€ç´¢æ¨¡å¼
    retrieval_mode = state.get("lightrag_mode", "hybrid")
    processed_query = state.get("processed_query", state["user_query"])
  
    # æ‰§è¡ŒLightRAGæ£€ç´¢
    try:
        results = lightrag_client.query(
            processed_query,
            param=QueryParam(mode=retrieval_mode)
        )
    
        # è®¡ç®—æ£€ç´¢è´¨é‡åˆ†æ•°
        quality_score = calculate_retrieval_quality(results, retrieval_mode)
    
        return {
            "lightrag_results": {
                "content": results,
                "mode": retrieval_mode,
                "query": processed_query,
                "source": "lightrag"
            },
            "retrieval_score": quality_score,
            "retrieval_success": True
        }
  
    except Exception as e:
        return {
            "lightrag_results": {"error": str(e)},
            "retrieval_score": 0.0,
            "retrieval_success": False
        }

def calculate_retrieval_quality(results: str, mode: str) -> float:
    """è®¡ç®—LightRAGæ£€ç´¢ç»“æœçš„è´¨é‡åˆ†æ•°"""
    if not results or len(results.strip()) < 50:
        return 0.1
  
    # åŸºäºå†…å®¹é•¿åº¦å’Œæ¨¡å¼çš„åŸºç¡€åˆ†æ•°
    content_length = len(results)
    base_score = min(content_length / 1000, 0.8)  # åŸºäºå†…å®¹é•¿åº¦
  
    # æ ¹æ®æ£€ç´¢æ¨¡å¼è°ƒæ•´åˆ†æ•°
    mode_bonus = {
        "local": 0.1,    # å‘é‡æ£€ç´¢ç›¸å¯¹ç®€å•
        "global": 0.15,  # å›¾æ£€ç´¢æ›´å¤æ‚
        "hybrid": 0.2    # æ··åˆæ£€ç´¢æœ€å…¨é¢
    }.get(mode, 0.1)
  
    total_score = min(base_score + mode_bonus, 1.0)
    return total_score
```

#### 3.1.3 ç»“æœè´¨é‡è¯„ä¼°èŠ‚ç‚¹

**èŠ‚ç‚¹ä½œç”¨**ï¼šè¯„ä¼°LightRAGæ£€ç´¢ç»“æœçš„è´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢è¡¥å……
**ä¸ºä»€ä¹ˆé‡è¦**ï¼šé¿å…ä¸å¿…è¦çš„ç½‘ç»œæœç´¢ï¼Œæé«˜å“åº”é€Ÿåº¦ï¼›ç¡®ä¿ä¿¡æ¯å……åˆ†æ€§

```python
def quality_assessment_node(state: AgentState):
    """è¯„ä¼°LightRAGæ£€ç´¢ç»“æœè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢"""
  
    lightrag_results = state.get("lightrag_results", {})
    retrieval_success = state.get("retrieval_success", False)
  
    if not retrieval_success:
        # æ£€ç´¢å¤±è´¥ï¼Œå¿…é¡»è¿›è¡Œç½‘ç»œæœç´¢
        return {
            "confidence_score": 0.0,
            "need_web_search": True,
            "assessment_reason": "LightRAGæ£€ç´¢å¤±è´¥ï¼Œéœ€è¦ç½‘ç»œæœç´¢è¡¥å……"
        }
  
    # ç½®ä¿¡åº¦è¯„ä¼°æ ‡å‡†
    confidence_factors = {
        "retrieval_score": state.get("retrieval_score", 0) * 0.4,
        "content_completeness": evaluate_content_completeness(lightrag_results) * 0.3,
        "entity_coverage": evaluate_entity_coverage(state) * 0.2,
        "mode_effectiveness": evaluate_mode_effectiveness(state) * 0.1
    }
  
    total_confidence = sum(confidence_factors.values())
  
    # æ ¹æ®æŸ¥è¯¢ç±»å‹è®¾ç½®åŠ¨æ€é˜ˆå€¼
    query_type = state.get("query_type", "ANALYTICAL")
    threshold_map = {
        "FACTUAL": 0.7,     # äº‹å®æŸ¥è¯¢è¦æ±‚è¾ƒé«˜ç½®ä¿¡åº¦
        "RELATIONAL": 0.6,  # å…³ç³»æŸ¥è¯¢ä¸­ç­‰ç½®ä¿¡åº¦
        "ANALYTICAL": 0.5   # åˆ†ææŸ¥è¯¢è¾ƒä½ç½®ä¿¡åº¦
    }
    threshold = threshold_map.get(query_type, 0.6)
  
    need_web_search = total_confidence < threshold
  
    return {
        "confidence_score": total_confidence,
        "confidence_breakdown": confidence_factors,
        "need_web_search": need_web_search,
        "confidence_threshold": threshold,
        "assessment_reason": f"ç½®ä¿¡åº¦ {total_confidence:.2f} {'<' if need_web_search else '>='} é˜ˆå€¼ {threshold}"
    }

def evaluate_entity_coverage(state):
    """è¯„ä¼°å…³é”®å®ä½“è¦†ç›–åº¦"""
    expected_entities = state.get("key_entities", [])
    if not expected_entities:
        return 1.0
  
    lightrag_content = state.get("lightrag_results", {}).get("content", "")
    if not lightrag_content:
        return 0.0
  
    # æ£€æŸ¥å…³é”®å®ä½“æ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­è¢«æåŠ
    found_count = 0
    for entity in expected_entities:
        if entity.lower() in lightrag_content.lower():
            found_count += 1
  
    coverage = found_count / len(expected_entities)
    return coverage

def evaluate_content_completeness(lightrag_results: dict) -> float:
    """è¯„ä¼°å†…å®¹å®Œæ•´æ€§"""
    content = lightrag_results.get("content", "")
    if not content:
        return 0.0
  
    content_length = len(content.strip())
  
    # åŸºäºå†…å®¹é•¿åº¦è¯„ä¼°å®Œæ•´æ€§
    if content_length >= 1000:
        return 1.0
    elif content_length >= 500:
        return 0.8
    elif content_length >= 200:
        return 0.6
    elif content_length >= 100:
        return 0.4
    else:
        return 0.2

def evaluate_mode_effectiveness(state) -> float:
    """è¯„ä¼°æ£€ç´¢æ¨¡å¼çš„æœ‰æ•ˆæ€§"""
    query_type = state.get("query_type", "")
    lightrag_mode = state.get("lightrag_mode", "")
  
    # æ£€æŸ¥æ¨¡å¼ä¸æŸ¥è¯¢ç±»å‹çš„åŒ¹é…åº¦
    ideal_matches = {
        "FACTUAL": "local",
        "RELATIONAL": "global", 
        "ANALYTICAL": "hybrid"
    }
  
    if ideal_matches.get(query_type) == lightrag_mode:
        return 1.0  # å®Œç¾åŒ¹é…
    elif lightrag_mode == "hybrid":
        return 0.8  # hybridæ¨¡å¼é€šå¸¸è¡¨ç°è‰¯å¥½
    else:
        return 0.6  # æ¬¡ä¼˜åŒ¹é…
```

#### 3.1.4 ç½‘ç»œæœç´¢èŠ‚ç‚¹

**èŠ‚ç‚¹ä½œç”¨**ï¼šå½“æœ¬åœ°ä¿¡æ¯ä¸è¶³æ—¶ï¼Œä»ç½‘ç»œè·å–è¡¥å……ä¿¡æ¯
**ä»€ä¹ˆæ—¶å€™è§¦å‘**ï¼šè´¨é‡è¯„ä¼°èŠ‚ç‚¹åˆ¤å®šéœ€è¦è¡¥å……ä¿¡æ¯æ—¶

```python
from tavily import TavilySearchAPIWrapper

def web_search_node(state: AgentState):
    """ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯"""
  
    # åªæœ‰å½“éœ€è¦ç½‘ç»œæœç´¢æ—¶æ‰æ‰§è¡Œ
    if not state.get("need_web_search", False):
        return {"web_results": []}
  
    # åˆå§‹åŒ–Tavilyæœç´¢
    tavily_search = TavilySearchAPIWrapper(
        tavily_api_key=os.getenv("TAVILY_API_KEY")
    )
  
    # åŸºäºæŸ¥è¯¢ç±»å‹ä¼˜åŒ–æœç´¢ç­–ç•¥
    if state["query_type"] == "FACTUAL":
        search_mode = "factual"
        max_results = 3
    elif state["query_type"] == "ANALYTICAL":
        search_mode = "comprehensive" 
        max_results = 5
    else:
        search_mode = "balanced"
        max_results = 4
  
    search_results = tavily_search.search(
        query=state["processed_query"],
        search_depth=search_mode,
        max_results=max_results,
        include_answer=True,
        include_raw_content=False
    )
  
    # å¤„ç†æœç´¢ç»“æœ
    processed_results = []
    for result in search_results:
        processed_results.append({
            "title": result.get("title", ""),
            "content": result.get("content", ""),
            "url": result.get("url", ""),
            "score": result.get("score", 0),
            "source_type": "web_search"
        })
  
    return {
        "web_results": processed_results,
        "web_search_summary": f"ä»ç½‘ç»œè·å– {len(processed_results)} ä¸ªè¡¥å……ä¿¡æ¯"
    }
```

#### 3.1.5 ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹

**èŠ‚ç‚¹ä½œç”¨**ï¼šæ•´åˆLightRAGæ£€ç´¢ç»“æœå’Œç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
**ä¸ºä»€ä¹ˆæ˜¯æ ¸å¿ƒ**ï¼šè¿™æ˜¯ç”¨æˆ·æœ€ç»ˆçœ‹åˆ°çš„ç»“æœï¼Œéœ€è¦å‡†ç¡®ã€å…¨é¢ã€æœ‰è¯´æœåŠ›

```python
def answer_generation_node(state: AgentState):
    """æ•´åˆLightRAGå’Œç½‘ç»œæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
  
    llm = ChatOpenAI(model="gpt-4", temperature=0.1)
  
    # æ•´åˆæ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯
    context_parts = []
    sources = []
  
    # LightRAGæ£€ç´¢ç»“æœ
    lightrag_results = state.get("lightrag_results", {})
    if lightrag_results.get("content"):
        context_parts.append(f"çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯ï¼ˆ{lightrag_results.get('mode', 'unknown')}æ¨¡å¼ï¼‰ï¼š\n{lightrag_results['content']}")
        sources.append({
            "type": "lightrag_knowledge",
            "mode": lightrag_results.get("mode", "unknown"),
            "confidence": state.get("retrieval_score", 0),
            "query": lightrag_results.get("query", "")
        })
  
    # ç½‘ç»œæœç´¢è¡¥å……ç»“æœ
    if state.get("web_results"):
        web_context = format_web_results(state["web_results"])
        context_parts.append(f"ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯ï¼š\n{web_context}")
        for result in state["web_results"]:
            sources.append({
                "type": "web_search",
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "score": result.get("score", 0)
            })
  
    # æ„å»ºæœ€ç»ˆæç¤ºè¯
    full_context = "\n\n".join(context_parts)
    lightrag_mode = lightrag_results.get("mode", "unknown")
  
    answer_prompt = f"""
    åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
  
    ç”¨æˆ·é—®é¢˜ï¼š{state["user_query"]}
    æŸ¥è¯¢ç±»å‹ï¼š{state.get("query_type", "UNKNOWN")}
    æ£€ç´¢æ¨¡å¼ï¼š{lightrag_mode}
  
    å¯ç”¨ä¿¡æ¯ï¼š
    {full_context}
  
    è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
    1. ä¼˜å…ˆä½¿ç”¨LightRAGçŸ¥è¯†åº“çš„ä¿¡æ¯ä½œä¸ºä¸»è¦ç­”æ¡ˆæ¥æº
    2. ç”¨ç½‘ç»œæœç´¢ç»“æœè¡¥å……æœ€æ–°ä¿¡æ¯æˆ–å¡«è¡¥çŸ¥è¯†ç©ºç™½
    3. æ¸…æ¥šæ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆçŸ¥è¯†åº“ vs ç½‘ç»œæœç´¢ï¼‰
    4. å¦‚æœä¿¡æ¯ä¸è¶³æˆ–å­˜åœ¨çŸ›ç›¾ï¼Œæ˜ç¡®è¯´æ˜
    5. æä¾›ç»“æ„åŒ–ã€æ˜“æ‡‚çš„ç­”æ¡ˆ
    6. æ ¹æ®æ£€ç´¢æ¨¡å¼è°ƒæ•´ç­”æ¡ˆé£æ ¼ï¼š
       - localæ¨¡å¼ï¼šæä¾›å‡†ç¡®çš„äº‹å®æ€§ç­”æ¡ˆ
       - globalæ¨¡å¼ï¼šå¼ºè°ƒå®ä½“å…³ç³»å’Œè”ç³»
       - hybridæ¨¡å¼ï¼šæä¾›å…¨é¢çš„ç»¼åˆåˆ†æ
  
    æ ¼å¼è¦æ±‚ï¼š
    - ä¸»è¦ç­”æ¡ˆ
    - è¯¦ç»†è§£é‡Šï¼ˆå¦‚æœéœ€è¦ï¼‰
    - ç›¸å…³è¡¥å……ä¿¡æ¯
    - ä¿¡æ¯æ¥æºè¯´æ˜
    """
  
    answer_result = llm.invoke(answer_prompt)
  
    return {
        "final_answer": answer_result.content,
        "sources": sources,
        "context_used": len(context_parts),
        "lightrag_mode_used": lightrag_mode,
        "answer_confidence": calculate_answer_confidence(state)
    }

def format_web_results(web_results):
    """æ ¼å¼åŒ–ç½‘ç»œæœç´¢ç»“æœ"""
    formatted = []
    for i, result in enumerate(web_results[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
        title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
        content = result.get("content", "")[:200]
        url = result.get("url", "")
      
        formatted.append(f"{i}. {title}")
        if content:
            formatted.append(f"   {content}...")
        formatted.append(f"   æ¥æºï¼š{url}")
  
    return "\n".join(formatted)

def calculate_answer_confidence(state):
    """è®¡ç®—æœ€ç»ˆç­”æ¡ˆçš„ç½®ä¿¡åº¦"""
    base_confidence = state.get("confidence_score", 0.5)
  
    # æ ¹æ®ä¿¡æ¯æ¥æºæ•°é‡å’Œè´¨é‡è°ƒæ•´
    source_bonus = 0
  
    # LightRAGç»“æœçš„è´¨é‡å¥–åŠ±
    if state.get("lightrag_results", {}).get("content"):
        lightrag_mode = state.get("lightrag_mode", "")
        mode_bonus = {
            "local": 0.15,
            "global": 0.2,
            "hybrid": 0.25  # hybridæ¨¡å¼è·å¾—æœ€é«˜å¥–åŠ±
        }.get(lightrag_mode, 0.1)
        source_bonus += mode_bonus
  
    # ç½‘ç»œæœç´¢è¡¥å……çš„å¥–åŠ±
    web_results = state.get("web_results", [])
    if web_results:
        web_bonus = min(len(web_results) * 0.05, 0.15)  # æœ€å¤š0.15çš„å¥–åŠ±
        source_bonus += web_bonus
  
    # æ£€ç´¢æˆåŠŸçš„åŸºç¡€å¥–åŠ±
    if state.get("retrieval_success", False):
        source_bonus += 0.1
  
    final_confidence = min(base_confidence + source_bonus, 1.0)
    return final_confidence
```

### 3.2 LangGraphå·¥ä½œæµæ„å»º

#### 3.2.1 ç®€åŒ–çš„æ™ºèƒ½æ£€ç´¢ç­–ç•¥

**æ ¸å¿ƒè®¾è®¡æ€è·¯**ï¼š

1. **æŸ¥è¯¢åˆ†æ** â†’ åˆ¤æ–­æœ€ä½³LightRAGæ£€ç´¢æ¨¡å¼ï¼ˆlocal/global/hybridï¼‰
2. **æ™ºèƒ½æ£€ç´¢** â†’ ä½¿ç”¨é€‰å®šçš„LightRAGæ¨¡å¼æ‰§è¡Œæ£€ç´¢
3. **è´¨é‡è¯„ä¼°** â†’ è¯„ä¼°æ£€ç´¢ç»“æœæ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜
4. **æ¡ä»¶åˆ†æ”¯** â†’ è´¨é‡è¶³å¤Ÿâ†’ç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼›è´¨é‡ä¸è¶³â†’ç½‘ç»œæœç´¢è¡¥å……

**LangGraphå·¥ä½œæµç¨‹å›¾**ï¼š

```mermaid
graph TD
    START([å¼€å§‹]) --> QueryAnalysis["æŸ¥è¯¢åˆ†æèŠ‚ç‚¹<br/>åˆ†ææ„å›¾å¹¶é€‰æ‹©LightRAGæ¨¡å¼"]
  
    QueryAnalysis --> LightRAGRetrieval["LightRAGæ£€ç´¢èŠ‚ç‚¹<br/>æ‰§è¡Œlocal/global/hybridæ£€ç´¢"]
  
    LightRAGRetrieval --> QualityCheck["è´¨é‡è¯„ä¼°èŠ‚ç‚¹<br/>è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡"]
  
    QualityCheck --> Decision{"è´¨é‡è¯„ä¼°å†³ç­–<br/>æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢ï¼Ÿ"}
    Decision -->|è´¨é‡è¶³å¤Ÿ| DirectAnswer["ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹<br/>åŸºäºLightRAGç»“æœ"]
    Decision -->|è´¨é‡ä¸è¶³| WebSearch["ç½‘ç»œæœç´¢èŠ‚ç‚¹<br/>Tavilyè¡¥å……ä¿¡æ¯"]
  
    WebSearch --> EnhancedAnswer["ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹<br/>æ•´åˆLightRAG+ç½‘ç»œæœç´¢"]
    DirectAnswer --> END([ç»“æŸ])
    EnhancedAnswer --> END
```

**ç®€åŒ–æ¶æ„çš„ä¼˜åŠ¿**ï¼š

- **æµç¨‹æ¸…æ™°**ï¼šç›´çº¿å‹å·¥ä½œæµï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
- **ä¸“æ³¨æ ¸å¿ƒ**ï¼šä»¥LightRAGä¸ºå”¯ä¸€çŸ¥è¯†æ£€ç´¢å¼•æ“ï¼Œé¿å…æŠ€æœ¯æ ˆå¤æ‚åŒ–
- **æ™ºèƒ½å†³ç­–**ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„LightRAGæ£€ç´¢æ¨¡å¼

**çŠ¶æ€æµè½¬å›¾**ï¼š

```mermaid
stateDiagram-v2
    [*] --> QueryAnalysis: ç”¨æˆ·è¾“å…¥
  
    QueryAnalysis --> LightRAGRetrieval: åˆ†æå®Œæˆï¼Œé€‰å®šæ£€ç´¢æ¨¡å¼
  
    LightRAGRetrieval --> QualityAssessment: LightRAGæ£€ç´¢ç»“æœ
  
    QualityAssessment --> DirectGeneration: è´¨é‡>=é˜ˆå€¼
    QualityAssessment --> WebSearching: è´¨é‡<é˜ˆå€¼
  
    WebSearching --> EnhancedGeneration: è¡¥å……ä¿¡æ¯
    DirectGeneration --> [*]: LightRAGç­”æ¡ˆ
    EnhancedGeneration --> [*]: å¢å¼ºç­”æ¡ˆ
  
    note right of QueryAnalysis
        LightRAGæ¨¡å¼é€‰æ‹©ï¼š
        - FACTUAL â†’ localæ¨¡å¼
        - RELATIONAL â†’ globalæ¨¡å¼  
        - ANALYTICAL â†’ hybridæ¨¡å¼
    end note
```

**æ•°æ®æµè½¬è¯¦å›¾**ï¼š

```mermaid
flowchart TD
    subgraph "è¾“å…¥å¤„ç†"
        UserQuery["ç”¨æˆ·æŸ¥è¯¢"] --> QueryAnalysis["æŸ¥è¯¢åˆ†æ<br/>â€¢ æ„å›¾è¯†åˆ«<br/>â€¢ æŸ¥è¯¢ç±»å‹åˆ¤æ–­<br/>â€¢ LightRAGæ¨¡å¼é€‰æ‹©<br/>â€¢ å…³é”®å®ä½“æå–"]
    end
  
    subgraph "æ™ºèƒ½æ£€ç´¢å±‚"
        QueryAnalysis --> LightRAGEngine["LightRAGæ£€ç´¢å¼•æ“<br/>â€¢ æ ¹æ®é€‰å®šæ¨¡å¼æ‰§è¡Œæ£€ç´¢<br/>â€¢ local/global/hybrid"]
      
        LightRAGEngine --> LocalMode["localæ¨¡å¼<br/>å‘é‡æ£€ç´¢<br/>â€¢ è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…"]
        LightRAGEngine --> GlobalMode["globalæ¨¡å¼<br/>å›¾æ£€ç´¢<br/>â€¢ å®ä½“å…³ç³»éå†"]
        LightRAGEngine --> HybridMode["hybridæ¨¡å¼<br/>æ··åˆæ£€ç´¢<br/>â€¢ å‘é‡+å›¾è°±èåˆ"]
    end
  
    subgraph "è´¨é‡æ§åˆ¶å±‚"
        LocalMode --> QualityGate["è´¨é‡è¯„ä¼°<br/>â€¢ å†…å®¹å®Œæ•´æ€§<br/>â€¢ å®ä½“è¦†ç›–åº¦<br/>â€¢ æ¨¡å¼æœ‰æ•ˆæ€§<br/>â€¢ ç½®ä¿¡åº¦è®¡ç®—"]
        GlobalMode --> QualityGate
        HybridMode --> QualityGate
  
        QualityGate --> DecisionPoint{"è´¨é‡è¾¾æ ‡?"}
    end
  
    subgraph "ç­”æ¡ˆç”Ÿæˆå±‚"
        DecisionPoint -->|æ˜¯| FastTrack["å¿«é€Ÿé€šé“<br/>åŸºäºLightRAGç»“æœ"]
        DecisionPoint -->|å¦| Enhancement["å¢å¼ºé€šé“<br/>ç½‘ç»œæœç´¢è¡¥å¼º"]
  
        Enhancement --> WebSearch["Tavilyæœç´¢"]
        WebSearch --> EnhancedGen["ç»¼åˆç­”æ¡ˆç”Ÿæˆ<br/>LightRAG + ç½‘ç»œæœç´¢"]
  
        FastTrack --> FinalAnswer["æœ€ç»ˆç­”æ¡ˆ"]
        EnhancedGen --> FinalAnswer
    end
```

#### 3.2.2 ç®€åŒ–çš„å·¥ä½œæµä»£ç å®ç°

**æ ¸å¿ƒçŠ¶æ€å®šä¹‰**ï¼š

```python
from typing import TypedDict, Literal
from langgraph import StateGraph, END

class AgentState(TypedDict):
    # è¾“å…¥
    user_query: str
    processed_query: str
    session_id: str
  
    # åˆ†æç»“æœ
    query_type: Literal["factual", "relational", "analytical"] 
    key_entities: list[str]
  
    # è·¯ç”±æ§åˆ¶
    next_node: str
    retrieval_mode: str
  
    # æ£€ç´¢ç»“æœ
    retrieval_results: dict
    quality_score: float
    confidence_level: str
  
    # å†³ç­–ç»“æœ
    needs_web_search: bool
    web_results: list = None
  
    # æœ€ç»ˆè¾“å‡º
    final_answer: str
    sources: dict
```

**ç®€åŒ–çš„èŠ‚ç‚¹å®ç°**ï¼š

```python
def query_analysis_node(state: AgentState) -> AgentState:
    """åˆ†ææŸ¥è¯¢ç±»å‹å’Œæ„å›¾"""
    llm = ChatOpenAI(model="gpt-4", temperature=0)
  
    analysis_prompt = f"""
    åˆ†æç”¨æˆ·æŸ¥è¯¢çš„ç±»å‹å’Œæ„å›¾ï¼š
  
    æŸ¥è¯¢: {state["user_query"]}
  
    æŸ¥è¯¢ç±»å‹åˆ†ç±»ï¼š
    1. factual: å¯»æ‰¾å®šä¹‰ã€æ¦‚å¿µã€å…·ä½“äº‹å®
    2. relational: æ¢ç´¢å…³ç³»ã€å½±å“ã€è¿æ¥
    3. analytical: å¤æ‚åˆ†æã€ç»¼åˆåˆ¤æ–­
  
    è¿”å›JSON: {{"type": "factual/relational/analytical", "entities": ["å®ä½“1", "å®ä½“2"], "processed_query": "ä¼˜åŒ–åçš„æŸ¥è¯¢"}}
    """
  
    result = llm.invoke(analysis_prompt)
    analysis = json.loads(result.content)
  
    state.update({
        "query_type": analysis["type"], 
        "key_entities": analysis["entities"],
        "processed_query": analysis["processed_query"]
    })
    return state

def strategy_routing_node(state: AgentState) -> AgentState:
    """ç­–ç•¥è·¯ç”±èŠ‚ç‚¹ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹å†³å®šä¸‹ä¸€ä¸ªæ‰§è¡Œçš„èŠ‚ç‚¹"""
    query_type = state["query_type"]
  
    # è®¾ç½®ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„è·¯ç”±
    if query_type == "factual":
        next_node = "vector_retrieval"
    elif query_type == "relational":
        next_node = "graph_retrieval"
    else:  # analytical
        next_node = "hybrid_retrieval"
  
    state["next_node"] = next_node
    state["retrieval_mode"] = query_type
    return state

def vector_retrieval_node(state: AgentState) -> AgentState:
    """å‘é‡æ£€ç´¢èŠ‚ç‚¹ï¼šLightRAG localæ¨¡å¼"""
    query = state.get("processed_query", state["user_query"])
  
    # ä½¿ç”¨LightRAGçš„localæ¨¡å¼è¿›è¡Œå‘é‡æ£€ç´¢
    results = lightrag_client.query(query, param=QueryParam(mode="local"))
  
    state["retrieval_results"] = {
        "content": results,
        "mode": "local",
        "retrieval_type": "vector",
        "source": "lightrag_local"
    }
    return state

def graph_retrieval_node(state: AgentState) -> AgentState:
    """å›¾è°±æ£€ç´¢èŠ‚ç‚¹ï¼šLightRAG globalæ¨¡å¼"""
    query = state.get("processed_query", state["user_query"])
  
    # ä½¿ç”¨LightRAGçš„globalæ¨¡å¼è¿›è¡Œå›¾è°±æ£€ç´¢
    results = lightrag_client.query(query, param=QueryParam(mode="global"))
  
    state["retrieval_results"] = {
        "content": results,
        "mode": "global", 
        "retrieval_type": "graph",
        "source": "lightrag_global"
    }
    return state

def hybrid_retrieval_node(state: AgentState) -> AgentState:
    """æ··åˆæ£€ç´¢èŠ‚ç‚¹ï¼šLightRAG hybridæ¨¡å¼"""
    query = state.get("processed_query", state["user_query"])
  
    # ä½¿ç”¨LightRAGçš„hybridæ¨¡å¼è¿›è¡Œæ··åˆæ£€ç´¢
    results = lightrag_client.query(query, param=QueryParam(mode="hybrid"))
  
    state["retrieval_results"] = {
        "content": results,
        "mode": "hybrid",
        "retrieval_type": "hybrid", 
        "source": "lightrag_hybrid"
    }
    return state

def quality_assessment_node(state: AgentState) -> AgentState:
    """è¯„ä¼°æ£€ç´¢è´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢"""
    llm = ChatOpenAI(model="gpt-4", temperature=0)
  
    assessment_prompt = f"""
    è¯„ä¼°æ£€ç´¢ç»“æœæ˜¯å¦è¶³ä»¥å›ç­”æŸ¥è¯¢ï¼š
  
    ç”¨æˆ·æŸ¥è¯¢: {state["user_query"]}
    æ£€ç´¢ç»“æœ: {state["retrieval_results"]}
  
    è¯„ä¼°æ ‡å‡†ï¼š
    1. ä¿¡æ¯å®Œæ•´æ€§ (0-10)
    2. ç›´æ¥ç›¸å…³æ€§ (0-10) 
    3. æƒå¨æ€§/å¯ä¿¡åº¦ (0-10)
  
    æ€»åˆ†>=24ä¸”ä¿¡æ¯å®Œæ•´ â†’ æ— éœ€ç½‘ç»œæœç´¢
    æ€»åˆ†<24æˆ–ä¿¡æ¯è¿‡æ—¶ â†’ éœ€è¦ç½‘ç»œæœç´¢
  
    è¿”å›JSON: {{"total_score": 0-30, "needs_web_search": true/false, "confidence": "high/medium/low"}}
    """
  
    result = llm.invoke(assessment_prompt)
    assessment = json.loads(result.content)
  
    state.update({
        "quality_score": assessment["total_score"],
        "needs_web_search": assessment["needs_web_search"],
        "confidence_level": assessment["confidence"]
    })
    return state

def web_search_node(state: AgentState) -> AgentState:
    """ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯"""
    search_results = tavily_client.search(
        query=state["user_query"],
        max_results=5,
        search_depth="advanced"
    )
    state["web_results"] = search_results
    return state

def build_agentic_rag_workflow():
    """æ„å»ºåŸºäºLightRAGçš„Agentic RAGå·¥ä½œæµ"""
    from langgraph import StateGraph, END
  
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AgentState)
  
    # æ·»åŠ æ ¸å¿ƒèŠ‚ç‚¹
    workflow.add_node("query_analysis", query_analysis_node)
    workflow.add_node("lightrag_retrieval", lightrag_retrieval_node)
    workflow.add_node("quality_assessment", quality_assessment_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("answer_generation", answer_generation_node)
  
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("query_analysis")
  
    # çº¿æ€§è¿æ¥ä¸»è·¯å¾„
    workflow.add_edge("query_analysis", "lightrag_retrieval")
    workflow.add_edge("lightrag_retrieval", "quality_assessment")
  
    # è´¨é‡è¯„ä¼°çš„æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
    def should_web_search(state: AgentState) -> str:
        return "web_search" if state.get("need_web_search", False) else "answer_generation"
  
    workflow.add_conditional_edges(
        "quality_assessment",
        should_web_search,
        {
            "web_search": "web_search",
            "answer_generation": "answer_generation"
        }
    )
  
    # ç½‘ç»œæœç´¢åä¹Ÿåˆ°ç­”æ¡ˆç”Ÿæˆ
    workflow.add_edge("web_search", "answer_generation")
  
    # ç­”æ¡ˆç”Ÿæˆåˆ°ç»“æŸ
    workflow.add_edge("answer_generation", END)
  
    # ç¼–è¯‘å·¥ä½œæµ
    app = workflow.compile()
    return app

def answer_generation_node(state: AgentState) -> AgentState:
    """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    llm = ChatOpenAI(model="gpt-4", temperature=0.3)
  
    # æ ¹æ®æ˜¯å¦æœ‰ç½‘ç»œæœç´¢é€‰æ‹©ä¸åŒçš„ç”Ÿæˆç­–ç•¥
    if state.get("web_results"):
        generation_context = f"""
        æœ¬åœ°æ£€ç´¢: {state["retrieval_results"]}
        ç½‘ç»œè¡¥å……: {state["web_results"]}
        ç½®ä¿¡åº¦: {state["confidence_level"]}
        """
    else:
        generation_context = f"""
        æœ¬åœ°æ£€ç´¢: {state["retrieval_results"]}
        ç½®ä¿¡åº¦: {state["confidence_level"]}
        """
  
    answer_prompt = f"""
    åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·æŸ¥è¯¢ï¼š
  
    ç”¨æˆ·æŸ¥è¯¢: {state["user_query"]}
  
    {generation_context}
  
    è¦æ±‚ï¼š
    1. ç­”æ¡ˆå‡†ç¡®å®Œæ•´
    2. é€»è¾‘æ¸…æ™°æœ‰æ¡ç†  
    3. æ ‡æ³¨ä¿¡æ¯æ¥æº
    4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜
    """
  
    result = llm.invoke(answer_prompt)
    state["final_answer"] = result.content
  
    # ç»Ÿè®¡ä¿¡æ¯æ¥æº
    state["sources"] = {
        "strategy": state["retrieval_strategy"],
        "confidence": state["confidence_level"],
        "web_enhanced": bool(state.get("web_results"))
    }
  
    return state
```

**ç®€åŒ–çš„å·¥ä½œæµæ„å»º**ï¼š

```python
def build_simplified_workflow():
    """æ„å»ºç®€åŒ–çš„æ™ºèƒ½é—®ç­”å·¥ä½œæµ"""
  
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AgentState)
  
    # æ·»åŠ æ ¸å¿ƒèŠ‚ç‚¹
    workflow.add_node("query_analysis", query_analysis_node)
    workflow.add_node("smart_retrieval", smart_retrieval_node)
    workflow.add_node("quality_assessment", quality_assessment_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("answer_generation", answer_generation_node)
  
    # è®¾ç½®å…¥å£
    workflow.set_entry_point("query_analysis")
  
    # çº¿æ€§è¿æ¥ä¸»è·¯å¾„
    workflow.add_edge("query_analysis", "smart_retrieval")
    workflow.add_edge("smart_retrieval", "quality_assessment") 
  
    # æ¡ä»¶åˆ†æ”¯ï¼šè´¨é‡è¯„ä¼°åçš„è·¯å¾„é€‰æ‹©
    def route_after_quality(state: AgentState) -> str:
        if state["needs_web_search"]:
            return "web_search"
        else:
            return "answer_generation"
  
    workflow.add_conditional_edges(
        "quality_assessment",
        route_after_quality,
        {
            "web_search": "web_search",
            "answer_generation": "answer_generation"
        }
    )
  
    # ç½‘ç»œæœç´¢åæ±‡åˆåˆ°ç­”æ¡ˆç”Ÿæˆ
    workflow.add_edge("web_search", "answer_generation")
  
    # ç»“æŸ
    workflow.add_edge("answer_generation", END)
  
    return workflow.compile()
```

## 4. æœ¬åœ°éƒ¨ç½²æŒ‡å—

### 4.1 éƒ¨ç½²æ¶æ„è¯´æ˜

#### 4.1.1 æ¨èæ··åˆéƒ¨ç½²æ¶æ„

```mermaid
graph TB
    subgraph "æœ¬åœ°å¼€å‘ç¯å¢ƒ"
        Streamlit[Streamlitåº”ç”¨<br/>ç«¯å£:8501]
        Neo4j[Neo4jå›¾æ•°æ®åº“<br/>ç«¯å£:7474/7687]
        LightRAG[LightRAGå­˜å‚¨<br/>æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ]
        Python[Pythonåº”ç”¨]
    end
  
    subgraph "è¿œç¨‹æœåŠ¡å™¨ï¼ˆå®å¡”é¢æ¿ï¼‰"
        PostgreSQL[PostgreSQLæ•°æ®åº“<br/>ç«¯å£:5432]
        PgVector[pgvectoræ‰©å±•]
    end
  
    subgraph "å¤–éƒ¨APIæœåŠ¡"
        OpenAI[OpenAI API]
        Tavily[Tavilyæœç´¢API]
    end
  
    Python --> PostgreSQL
    Python --> Neo4j
    Python --> LightRAG
    Python --> OpenAI
    Python --> Tavily
    Streamlit --> Python
  
    classDef localStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef remoteStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef apiStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
  
    class Streamlit,Neo4j,LightRAG,Python localStyle
    class PostgreSQL,PgVector remoteStyle
    class OpenAI,Tavily apiStyle
```

**éƒ¨ç½²ç­–ç•¥è¯´æ˜**ï¼š

- **PostgreSQL**: éƒ¨ç½²åœ¨æœåŠ¡å™¨ä¸Šï¼ˆå®å¡”é¢æ¿ç®¡ç†ï¼‰ï¼Œé€šè¿‡ç½‘ç»œè¿æ¥
- **Neo4j**: æœ¬åœ°éƒ¨ç½²ï¼Œæä¾›Webç®¡ç†ç•Œé¢ (http://localhost:7474)
- **LightRAG**: æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨
- **Streamlit**: æœ¬åœ°è¿è¡Œï¼Œä¾¿äºå¼€å‘å’Œè°ƒè¯•

#### 4.1.2 ç³»ç»Ÿè¦æ±‚

**æœ¬åœ°ç¯å¢ƒè¦æ±‚**ï¼š

- **CPU**ï¼š4æ ¸ä»¥ä¸Šï¼Œæ¨è8æ ¸
- **å†…å­˜**ï¼š16GBä»¥ä¸Šï¼Œæ¨è32GB
- **å­˜å‚¨**ï¼šSSD 50GBä»¥ä¸Šå¯ç”¨ç©ºé—´ï¼ˆä¸»è¦ç”¨äºNeo4jå’Œæ–‡æ¡£å­˜å‚¨ï¼‰
- **ç½‘ç»œ**ï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥ï¼ˆè¿æ¥è¿œç¨‹æ•°æ®åº“å’ŒAPIï¼‰

**è¿œç¨‹æœåŠ¡å™¨è¦æ±‚**ï¼š

- **PostgreSQL**ï¼šæ”¯æŒpgvectoræ‰©å±•
- **ç½‘ç»œ**ï¼šå¼€æ”¾5432ç«¯å£ï¼ˆæˆ–è‡ªå®šä¹‰ç«¯å£ï¼‰
- **å®å¡”é¢æ¿**ï¼šæ”¯æŒæ•°æ®åº“ç®¡ç†

**è½¯ä»¶è¦æ±‚**ï¼š

- **æ“ä½œç³»ç»Ÿ**ï¼šWindows 10/11, macOS 10.15+, Ubuntu 18.04+
- **Python**ï¼š3.9-3.11ï¼ˆæ¨è3.10ï¼‰
- **Git**ï¼šæœ€æ–°ç‰ˆæœ¬

### 4.2 æ•°æ®åº“éƒ¨ç½²é…ç½®

#### 4.2.1 PostgreSQLè¿œç¨‹æœåŠ¡å™¨é…ç½®ï¼ˆå®å¡”é¢æ¿ï¼‰

**åœ¨æœåŠ¡å™¨ä¸Šé€šè¿‡å®å¡”é¢æ¿å®‰è£…PostgreSQL**ï¼š

1. **å®‰è£…PostgreSQL**ï¼š

   ```bash
   # å®å¡”é¢æ¿ > è½¯ä»¶å•†åº— > æœç´¢"PostgreSQL" > å®‰è£…
   # æˆ–é€šè¿‡SSHå®‰è£…pgvectoræ‰©å±•
   sudo apt-get update
   sudo apt-get install postgresql-contrib
   # ä¸‹è½½å¹¶ç¼–è¯‘pgvector
   git clone https://github.com/pgvector/pgvector.git
   cd pgvector
   make
   sudo make install
   ```
2. **å®å¡”é¢æ¿æ•°æ®åº“é…ç½®**ï¼š

   ```
   å®å¡”é¢æ¿ > æ•°æ®åº“ > PostgreSQL > æ·»åŠ æ•°æ®åº“
   - æ•°æ®åº“åï¼šqa_system
   - ç”¨æˆ·åï¼šqa_user  
   - å¯†ç ï¼š[è®¾ç½®å¼ºå¯†ç ]
   - è®¿é—®æƒé™ï¼šå…è®¸è¿œç¨‹è¿æ¥
   ```
3. **å¼€æ”¾ç«¯å£å’Œå®‰å…¨ç»„**ï¼š

   ```bash
   # å®å¡”é¢æ¿ > å®‰å…¨ > æ”¾è¡Œç«¯å£
   # æ·»åŠ ç«¯å£ï¼š5432ï¼ˆæˆ–è‡ªå®šä¹‰ç«¯å£ï¼‰
   # ç±»å‹ï¼šTCP

   # å¦‚æœæ˜¯äº‘æœåŠ¡å™¨ï¼Œè¿˜éœ€è¦åœ¨äº‘å‚å•†æ§åˆ¶å°å¼€æ”¾å®‰å…¨ç»„
   ```
4. **å®‰è£…pgvectoræ‰©å±•**ï¼š

   ```sql
   -- é€šè¿‡å®å¡”é¢æ¿çš„phpPgAdminæˆ–å‘½ä»¤è¡Œè¿æ¥æ•°æ®åº“
   \c qa_system
   CREATE EXTENSION IF NOT EXISTS vector;

   -- éªŒè¯å®‰è£…
   SELECT * FROM pg_extension WHERE extname = 'vector';
   ```

#### 4.2.2 Neo4jæœ¬åœ°å®‰è£…ï¼ˆæ¨èæ–¹å¼ï¼‰

**Neo4j Desktopå®‰è£…ï¼ˆWindows/macOSæ¨èï¼‰**ï¼š

```bash
# 1. ä¸‹è½½Neo4j Desktop
# è®¿é—®ï¼šhttps://neo4j.com/download/
# ä¸‹è½½Neo4j Desktopå¹¶å®‰è£…

# 2. åˆ›å»ºæ•°æ®åº“é¡¹ç›®
# - æ‰“å¼€Neo4j Desktop
# - åˆ›å»ºæ–°é¡¹ç›®ï¼š"æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
# - æ·»åŠ æœ¬åœ°æ•°æ®åº“ï¼š"qa-knowledge-graph"
# - ç‰ˆæœ¬é€‰æ‹©ï¼š5.xï¼ˆæ¨èæœ€æ–°ç¨³å®šç‰ˆï¼‰
# - è®¾ç½®å¯†ç ï¼špasswordï¼ˆæˆ–è‡ªå®šä¹‰å¼ºå¯†ç ï¼‰

# 3. å¯åŠ¨æ•°æ®åº“
# - ç‚¹å‡»"Start"æŒ‰é’®å¯åŠ¨æ•°æ®åº“
# - å¯åŠ¨åä¼šæ˜¾ç¤ºè¿è¡ŒçŠ¶æ€å’Œç«¯å£ä¿¡æ¯

# 4. è®¿é—®ç®¡ç†ç•Œé¢
# æµè§ˆå™¨è®¿é—®ï¼šhttp://localhost:7474
# æ•°æ®åº“è¿æ¥ï¼šbolt://localhost:7687
# ç”¨æˆ·åï¼šneo4j
# å¯†ç ï¼šä½ è®¾ç½®çš„å¯†ç 
```

**Neo4j Community Editionå®‰è£…ï¼ˆLinuxæœåŠ¡å™¨ï¼‰**ï¼š

```bash
# Ubuntu/Debianå®‰è£…
curl -fsSL https://debian.neo4j.com/neotechnology.gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/neo4j.gpg
echo "deb [signed-by=/usr/share/keyrings/neo4j.gpg] https://debian.neo4j.com stable latest" | sudo tee -a /etc/apt/sources.list.d/neo4j.list
sudo apt-get update
sudo apt-get install neo4j

# é…ç½®Neo4j
sudo systemctl start neo4j
sudo systemctl enable neo4j

# è®¾ç½®åˆå§‹å¯†ç 
sudo neo4j-admin dbms set-initial-password password

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
sudo systemctl status neo4j
```

**Neo4jç«¯å£å’Œè®¿é—®è¯´æ˜**ï¼š

- **Webç•Œé¢ï¼ˆBrowserï¼‰**ï¼šhttp://localhost:7474
- **Boltåè®®ç«¯å£**ï¼š7687ï¼ˆåº”ç”¨ç¨‹åºè¿æ¥ä½¿ç”¨ï¼‰
- **HTTP APIç«¯å£**ï¼š7474ï¼ˆWebç•Œé¢å’ŒREST APIï¼‰
- **HTTPSç«¯å£**ï¼š7473ï¼ˆå¦‚æœå¯ç”¨HTTPSï¼‰

**é‡è¦é…ç½®æ–‡ä»¶ä½ç½®**ï¼š

```bash
# Neo4jé…ç½®æ–‡ä»¶
# Windows: %NEO4J_HOME%\conf\neo4j.conf
# macOS: /usr/local/etc/neo4j/neo4j.conf
# Linux: /etc/neo4j/neo4j.conf

# ä¸»è¦é…ç½®é¡¹
server.default_listen_address=0.0.0.0  # å…è®¸å¤–éƒ¨è®¿é—®
server.bolt.listen_address=:7687        # Boltç«¯å£
server.http.listen_address=:7474        # HTTPç«¯å£
dbms.memory.heap.initial_size=1G        # åˆå§‹å †å†…å­˜
dbms.memory.heap.max_size=2G            # æœ€å¤§å †å†…å­˜
```

### 4.3 Pythonç¯å¢ƒé…ç½®

#### 4.3.1 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir intelligent-qa-system
cd intelligent-qa-system

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv qa_env

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
qa_env\Scripts\activate
# macOS/Linux  
source qa_env/bin/activate
```

#### 4.3.2 å®‰è£…ä¾èµ–åŒ…

åˆ›å»º `requirements.txt`ï¼ˆMVPç‰ˆæœ¬ï¼‰ï¼š

```txt
# æ ¸å¿ƒæ¡†æ¶
lightrag==0.0.5
langgraph==0.2.16
streamlit==1.28.1

# LLMå’ŒAPI
langchain==0.1.0
langchain-openai==0.1.0
openai==1.6.1
tavily-python==0.3.3

# æ•°æ®åº“è¿æ¥
neo4j==5.15.0
psycopg2-binary==2.9.9

# æ•°æ®å¤„ç†
pandas==2.1.4
numpy==1.24.3
pypdf==3.17.4
python-docx==1.1.0

# å®ç”¨å·¥å…·
python-dotenv==1.0.0
pydantic==2.5.2
```

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

### 4.4 æ··åˆéƒ¨ç½²ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
# OpenAI APIé…ç½®
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4

# Tavilyæœç´¢APIé…ç½®
TAVILY_API_KEY=your_tavily_api_key_here

# PostgreSQLè¿œç¨‹æ•°æ®åº“é…ç½®ï¼ˆæœåŠ¡å™¨ï¼‰
POSTGRES_HOST=117.72.54.192              # æœåŠ¡å™¨IP
POSTGRES_PORT=5432                        # æ•°æ®åº“ç«¯å£
POSTGRES_DB=searchforrag                  # æ•°æ®åº“å
POSTGRES_USER=searchforrag                # æ•°æ®åº“ç”¨æˆ·å
POSTGRES_PASSWORD=searchforrag            # æ•°æ®åº“å¯†ç 
POSTGRES_SSL_MODE=prefer                  # SSLè¿æ¥æ¨¡å¼

# Neo4jæœ¬åœ°å›¾æ•°æ®åº“é…ç½®
NEO4J_URI=bolt://localhost:7687           # æœ¬åœ°Neo4jè¿æ¥
NEO4J_USERNAME=neo4j                      # é»˜è®¤ç”¨æˆ·å
NEO4J_PASSWORD=password                   # ä½ è®¾ç½®çš„å¯†ç 
NEO4J_DATABASE=neo4j                      # æ•°æ®åº“åï¼ˆé»˜è®¤neo4jï¼‰

# LightRAGæœ¬åœ°å­˜å‚¨é…ç½®
RAG_STORAGE_DIR=./rag_storage             # æœ¬åœ°å­˜å‚¨ç›®å½•
DOCS_DIR=./docs                           # æ–‡æ¡£ç›®å½•
CONFIDENCE_THRESHOLD=0.7                  # ç½®ä¿¡åº¦é˜ˆå€¼

# Streamlitæœ¬åœ°åº”ç”¨é…ç½®
STREAMLIT_HOST=localhost                  # æœ¬åœ°è¿è¡Œ
STREAMLIT_PORT=8501                       # Webç•Œé¢ç«¯å£

# ç³»ç»Ÿé…ç½®
DEBUG=false                               # è°ƒè¯•æ¨¡å¼
LOG_LEVEL=INFO                           # æ—¥å¿—çº§åˆ«
```

#### 4.4.1 ç½‘ç»œè¿æ¥æµ‹è¯•

åˆ›å»ºè¿æ¥æµ‹è¯•è„šæœ¬ `scripts/test_connections.py`ï¼š

```python
import os
import psycopg2
from neo4j import GraphDatabase
from dotenv import load_dotenv

def test_postgresql_connection():
    """æµ‹è¯•PostgreSQLè¿œç¨‹è¿æ¥"""
    try:
        load_dotenv()
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST"),
            port=os.getenv("POSTGRES_PORT"),
            database=os.getenv("POSTGRES_DB"),
            user=os.getenv("POSTGRES_USER"),
            password=os.getenv("POSTGRES_PASSWORD"),
            sslmode=os.getenv("POSTGRES_SSL_MODE", "prefer")
        )
    
        cursor = conn.cursor()
        cursor.execute("SELECT version();")
        version = cursor.fetchone()
    
        # æµ‹è¯•pgvectoræ‰©å±•
        cursor.execute("SELECT * FROM pg_extension WHERE extname = 'vector';")
        vector_ext = cursor.fetchone()
    
        cursor.close()
        conn.close()
    
        print("âœ… PostgreSQLè¿æ¥æˆåŠŸ")
        print(f"   ç‰ˆæœ¬ï¼š{version[0]}")
        print(f"   pgvectoræ‰©å±•ï¼š{'å·²å®‰è£…' if vector_ext else 'æœªå®‰è£…'}")
        return True
    
    except Exception as e:
        print(f"âŒ PostgreSQLè¿æ¥å¤±è´¥ï¼š{e}")
        return False

def test_neo4j_connection():
    """æµ‹è¯•Neo4jæœ¬åœ°è¿æ¥"""
    try:
        load_dotenv()
        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI"),
            auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD"))
        )
    
        with driver.session() as session:
            result = session.run("CALL dbms.components() YIELD name, versions RETURN name, versions[0] as version")
            components = list(result)
        
        driver.close()
    
        print("âœ… Neo4jè¿æ¥æˆåŠŸ")
        for component in components:
            print(f"   {component['name']}ï¼š{component['version']}")
    
        print(f"   Webç•Œé¢ï¼šhttp://localhost:7474")
        return True
    
    except Exception as e:
        print(f"âŒ Neo4jè¿æ¥å¤±è´¥ï¼š{e}")
        print("   è¯·ç¡®ä¿Neo4jå·²å¯åŠ¨å¹¶ä¸”å¯†ç æ­£ç¡®")
        return False

if __name__ == "__main__":
    print("ğŸ” æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    print()
  
    pg_ok = test_postgresql_connection()
    print()
    neo4j_ok = test_neo4j_connection()
    print()
  
    if pg_ok and neo4j_ok:
        print("ğŸ‰ æ‰€æœ‰æ•°æ®åº“è¿æ¥æ­£å¸¸ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æ•°æ®åº“è¿æ¥å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥é…ç½®")
```

### 4.4 MVPé¡¹ç›®ç›®å½•ç»“æ„

```
intelligent-qa-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ query_analysis.py      # æŸ¥è¯¢åˆ†æèŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ lightrag_retrieval.py  # LightRAGæ£€ç´¢èŠ‚ç‚¹(æ”¯æŒä¸‰ç§æ¨¡å¼)
â”‚   â”‚   â”œâ”€â”€ quality_assessment.py  # è´¨é‡è¯„ä¼°èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ web_search.py          # ç½‘ç»œæœç´¢èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ answer_generation.py   # ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ workflow.py            # LangGraphå·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ state.py               # çŠ¶æ€å®šä¹‰
â”‚   â”‚   â””â”€â”€ config.py              # ç»Ÿä¸€é…ç½®ç®¡ç†ï¼ˆè¯»å–.envæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_processor.py  # æ–‡æ¡£å¤„ç†
â”‚   â”‚   â””â”€â”€ helpers.py             # è¾…åŠ©å‡½æ•°
â”‚   â””â”€â”€ streamlit_app.py           # Streamlitå‰ç«¯ä¸»æ–‡ä»¶
â”œâ”€â”€ docs/                          # çŸ¥è¯†åº“æ–‡æ¡£ç›®å½•ï¼ˆå­˜æ”¾æ‰€æœ‰æ–‡æ¡£ï¼‰
â”œâ”€â”€ rag_storage/                   # LightRAGå­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ kv_storage/               # é”®å€¼å­˜å‚¨
â”‚   â”œâ”€â”€ vector_storage/           # å‘é‡å­˜å‚¨
â”‚   â””â”€â”€ graph_storage/            # å›¾å­˜å‚¨
â”œâ”€â”€ tests/                         # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ test_agents/
â”‚   â”œâ”€â”€ test_core/
â”‚   â””â”€â”€ test_integration/
â”œâ”€â”€ scripts/                       # è„šæœ¬å·¥å…·
â”‚   â”œâ”€â”€ setup_environment.py      # ç¯å¢ƒåˆå§‹åŒ–
â”‚   â”œâ”€â”€ ingest_documents.py       # æ–‡æ¡£å¯¼å…¥
â”‚   â”œâ”€â”€ backup_data.py            # æ•°æ®å¤‡ä»½
â”‚   â””â”€â”€ health_check.py           # ç³»ç»Ÿå¥åº·æ£€æŸ¥
â”œâ”€â”€ .env                          # ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ•æ„Ÿä¿¡æ¯ï¼‰
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–
â”œâ”€â”€ setup.py                      # å®‰è£…è„šæœ¬
â””â”€â”€ README.md                     # é¡¹ç›®è¯´æ˜
```

return {"final_answer": answer}

```

#### 2.2.2 æ¡ä»¶è¾¹é€»è¾‘

```python
def should_web_search(state: AgentState) -> str:
    """å†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢"""
    if state["need_web_search"]:
        return "web_search"
    else:
        return "answer_generation"

def route_after_web_search(state: AgentState) -> str:
    """ç½‘ç»œæœç´¢åçš„è·¯ç”±"""
    return "answer_generation"
```

## 4. æŠ€æœ¯å®ç°è¯¦æƒ…

### 4.1 LightRAG é…ç½®ä¸é›†æˆ

#### 4.1.1 åŸºç¡€é…ç½®

**LightRAGé…ç½®è¯´æ˜**ï¼š

- **æ ¸å¿ƒä½œç”¨**ï¼šLightRAGæ˜¯ç³»ç»Ÿå”¯ä¸€çš„çŸ¥è¯†æ£€ç´¢å¼•æ“ï¼Œè´Ÿè´£æ–‡æ¡£å¤„ç†ã€å‘é‡åŒ–ã€çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ£€ç´¢
- **å­˜å‚¨æ¶æ„**ï¼šä½¿ç”¨Neo4jå­˜å‚¨çŸ¥è¯†å›¾è°±ï¼ŒPostgreSQLå­˜å‚¨å‘é‡æ•°æ®
- **æ£€ç´¢æ¨¡å¼**ï¼šæ”¯æŒlocalï¼ˆå‘é‡ï¼‰ã€globalï¼ˆå›¾è°±ï¼‰ã€hybridï¼ˆæ··åˆï¼‰ä¸‰ç§æ£€ç´¢æ¨¡å¼

```python
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed
from lightrag.utils import EmbeddingFunc
import os

# åˆå§‹åŒ– LightRAG å®¢æˆ·ç«¯
def initialize_lightrag():
    """åˆå§‹åŒ–LightRAGï¼Œé…ç½®Neo4jå’ŒPostgreSQLå­˜å‚¨"""
  
    lightrag_client = LightRAG(
        working_dir="./rag_storage",
      
        # LLMé…ç½®
        llm_model_func=gpt_4o_mini_complete,
      
        # åµŒå…¥æ¨¡å‹é…ç½®
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,  # OpenAI text-embedding-3-largeç»´åº¦
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=os.getenv("OPENAI_API_KEY")
            )
        ),
      
        # å­˜å‚¨åç«¯é…ç½®
        graph_storage="Neo4JStorage",     # Neo4jå­˜å‚¨çŸ¥è¯†å›¾è°±
        vector_storage="PGVectorStorage", # PostgreSQLå­˜å‚¨å‘é‡
      
        # æ–‡æ¡£å¤„ç†é…ç½®
        chunk_token_size=1200,            # æ–‡æ¡£åˆ†å—å¤§å°
        chunk_overlap_token_size=100,     # åˆ†å—é‡å 
      
        # æ€§èƒ½é…ç½®
        max_parallel_insert=3,            # å¹¶è¡Œæ’å…¥æ•°é‡
        llm_model_max_async=12           # LLMå¼‚æ­¥å¹¶å‘æ•°
    )
  
    return lightrag_client

# å…¨å±€LightRAGå®¢æˆ·ç«¯
lightrag_client = initialize_lightrag()
```

#### 4.1.2 æ–‡æ¡£å¤„ç†å’Œç´¢å¼•

**æ–‡æ¡£å¯¼å…¥æµç¨‹**ï¼š

1. **è¯»å–æ–‡æ¡£**ï¼šæ”¯æŒå¤šç§æ ¼å¼ï¼ˆPDFã€DOCXã€TXTã€MDï¼‰
2. **æ–‡æ¡£åˆ†å—**ï¼šæŒ‰tokenå¤§å°æ™ºèƒ½åˆ†å‰²
3. **å‘é‡åŒ–**ï¼šä½¿ç”¨OpenAIåµŒå…¥æ¨¡å‹
4. **å›¾è°±æ„å»º**ï¼šLightRAGè‡ªåŠ¨æå–å®ä½“å’Œå…³ç³»
5. **å­˜å‚¨**ï¼šå‘é‡å­˜å…¥PostgreSQLï¼Œå›¾è°±å­˜å…¥Neo4j

```python
import asyncio
from pathlib import Path
from typing import List

async def ingest_documents(documents_path: str):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£å¹¶å»ºç«‹LightRAGç´¢å¼•"""
  
    print("ğŸš€ å¼€å§‹æ–‡æ¡£å¯¼å…¥...")
  
    # ç¡®ä¿å­˜å‚¨åç«¯å·²åˆå§‹åŒ–
    await lightrag_client.initialize_storages()
  
    # è¯»å–æ–‡æ¡£ç›®å½•
    documents_dir = Path(documents_path)
    supported_extensions = ['.txt', '.md', '.pdf', '.docx']
  
    doc_files = []
    for ext in supported_extensions:
        doc_files.extend(documents_dir.rglob(f'*{ext}'))
  
    if not doc_files:
        print(f"âŒ åœ¨ {documents_path} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
        return
  
    print(f"ğŸ“„ æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
  
    # æ‰¹é‡å¤„ç†æ–‡æ¡£
    batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªæ–‡æ¡£
    for i in range(0, len(doc_files), batch_size):
        batch = doc_files[i:i + batch_size]
      
        for doc_path in batch:
            try:
                print(f"ğŸ“– å¤„ç†æ–‡æ¡£: {doc_path.name}")
              
                # è¯»å–æ–‡æ¡£å†…å®¹
                content = read_document_content(doc_path)
              
                # LightRAGå¤„ç†ï¼ˆè‡ªåŠ¨åˆ†å—ã€å‘é‡åŒ–ã€å›¾è°±æ„å»ºï¼‰
                await lightrag_client.ainsert(content)
              
                print(f"âœ… å®Œæˆ: {doc_path.name}")
              
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥ {doc_path.name}: {str(e)}")
                continue
  
    print(f"ğŸ‰ æ–‡æ¡£å¯¼å…¥å®Œæˆï¼å…±å¤„ç† {len(doc_files)} ä¸ªæ–‡ä»¶")

def read_document_content(file_path: Path) -> str:
    """è¯»å–ä¸åŒæ ¼å¼çš„æ–‡æ¡£å†…å®¹"""
  
    file_ext = file_path.suffix.lower()
  
    try:
        if file_ext == '.pdf':
            import pypdf
            content = ""
            with open(file_path, 'rb') as file:
                reader = pypdf.PdfReader(file)
                for page in reader.pages:
                    content += page.extract_text() + "\n"
            return content
          
        elif file_ext == '.docx':
            from docx import Document
            doc = Document(file_path)
            content = ""
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
            return content
          
        else:  # .txt, .md
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
              
    except Exception as e:
        raise Exception(f"è¯»å–æ–‡æ¡£å¤±è´¥: {str(e)}")

# åŒæ­¥ç‰ˆæœ¬çš„æ–‡æ¡£å¯¼å…¥ï¼ˆç”¨äºè„šæœ¬è°ƒç”¨ï¼‰
def sync_ingest_documents(documents_path: str):
    """åŒæ­¥ç‰ˆæœ¬çš„æ–‡æ¡£å¯¼å…¥"""
    asyncio.run(ingest_documents(documents_path))
```

#### 4.1.3 LightRAGæ£€ç´¢æ¥å£

**ä¸‰ç§æ£€ç´¢æ¨¡å¼çš„ä½¿ç”¨åœºæ™¯**ï¼š

```python
async def query_lightrag(query: str, mode: str = "hybrid") -> dict:
    """ä½¿ç”¨LightRAGè¿›è¡Œæ£€ç´¢"""
  
    try:
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œæ£€ç´¢
        result = await lightrag_client.aquery(
            query,
            param=QueryParam(mode=mode)
        )
      
        return {
            "content": result,
            "mode": mode,
            "success": True,
            "query": query
        }
      
    except Exception as e:
        return {
            "content": "",
            "mode": mode,
            "success": False,
            "error": str(e),
            "query": query
        }

# æ£€ç´¢æ¨¡å¼è¯´æ˜
LIGHTRAG_MODES = {
    "local": {
        "description": "å‘é‡æ£€ç´¢æ¨¡å¼",
        "best_for": "äº‹å®æ€§æŸ¥è¯¢ã€å®šä¹‰æŸ¥è¯¢ã€å…·ä½“ä¿¡æ¯æŸ¥æ‰¾",
        "example": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    },
    "global": {
        "description": "å›¾æ£€ç´¢æ¨¡å¼", 
        "best_for": "å…³ç³»æ€§æŸ¥è¯¢ã€å®ä½“è”ç³»ã€å½±å“åˆ†æ",
        "example": "è°å‘æ˜äº†æœºå™¨å­¦ä¹ ï¼Ÿå®ƒä¸AIçš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ"
    },
    "hybrid": {
        "description": "æ··åˆæ£€ç´¢æ¨¡å¼",
        "best_for": "å¤æ‚æŸ¥è¯¢ã€ç»¼åˆåˆ†æã€å¤šç»´åº¦é—®é¢˜",
        "example": "æœºå™¨å­¦ä¹ çš„å‘å±•å†ç¨‹åŠå…¶å¯¹æœªæ¥çš„å½±å“"
    }
}
```

### 4.2 LangGraph å·¥ä½œæµç¼–æ’

#### 4.2.1 çŠ¶æ€å®šä¹‰

**AgentStateçŠ¶æ€ç®¡ç†**ï¼š

- **ç®€åŒ–è®¾è®¡**ï¼šç§»é™¤Graphitiç›¸å…³å­—æ®µï¼Œä¸“æ³¨äºLightRAGå·¥ä½œæµ
- **æ¸…æ™°çŠ¶æ€**ï¼šæ¯ä¸ªå­—æ®µéƒ½æœ‰æ˜ç¡®çš„ç”¨é€”å’Œç”Ÿå‘½å‘¨æœŸ

```python
from typing_extensions import TypedDict
from typing import List, Optional

class AgentState(TypedDict):
    # ç”¨æˆ·è¾“å…¥
    user_query: str
    processed_query: str
    session_id: str
  
    # æŸ¥è¯¢åˆ†æç»“æœ
    query_type: str                    # FACTUAL/RELATIONAL/ANALYTICAL
    lightrag_mode: str                 # local/global/hybrid
    key_entities: List[str]
    mode_reasoning: str
  
    # LightRAGæ£€ç´¢ç»“æœ
    lightrag_results: dict
    retrieval_score: float
    retrieval_success: bool
  
    # è´¨é‡è¯„ä¼°ç»“æœ
    confidence_score: float
    confidence_breakdown: dict
    need_web_search: bool
    confidence_threshold: float
    assessment_reason: str
  
    # ç½‘ç»œæœç´¢ç»“æœ
    web_results: Optional[List[dict]]
  
    # æœ€ç»ˆè¾“å‡º
    final_answer: str
    sources: List[dict]
    context_used: int
    lightrag_mode_used: str
    answer_confidence: float
```

#### 4.2.2 å·¥ä½œæµæ„å»º

**ç®€åŒ–çš„LangGraphå·¥ä½œæµ**ï¼š

- **ç›´çº¿å‹è®¾è®¡**ï¼šå»é™¤å¤æ‚çš„å¹¶è¡Œåˆ†æ”¯ï¼Œé‡‡ç”¨æ¸…æ™°çš„é¡ºåºæ‰§è¡Œ
- **æ™ºèƒ½å†³ç­–**ï¼šåœ¨å…³é”®èŠ‚ç‚¹ä½¿ç”¨æ¡ä»¶è¾¹è¿›è¡Œè·¯ç”±

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

def build_lightrag_workflow():
    """æ„å»ºåŸºäºLightRAGçš„æ™ºèƒ½é—®ç­”å·¥ä½œæµ"""
  
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AgentState)
  
    # æ·»åŠ æ ¸å¿ƒèŠ‚ç‚¹
    workflow.add_node("query_analysis", query_analysis_node)
    workflow.add_node("lightrag_retrieval", lightrag_retrieval_node)
    workflow.add_node("quality_assessment", quality_assessment_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("answer_generation", answer_generation_node)
  
    # è®¾ç½®å·¥ä½œæµå…¥å£
    workflow.set_entry_point("query_analysis")
  
    # çº¿æ€§è¿æ¥ä¸»è·¯å¾„
    workflow.add_edge("query_analysis", "lightrag_retrieval")
    workflow.add_edge("lightrag_retrieval", "quality_assessment")
  
    # è´¨é‡è¯„ä¼°åçš„æ¡ä»¶åˆ†æ”¯
    def route_after_quality_assessment(state: AgentState) -> str:
        """æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœå†³å®šä¸‹ä¸€æ­¥"""
        if state.get("need_web_search", False):
            return "web_search"
        else:
            return "answer_generation"
  
    workflow.add_conditional_edges(
        "quality_assessment",
        route_after_quality_assessment,
        {
            "web_search": "web_search",
            "answer_generation": "answer_generation"
        }
    )
  
    # ç½‘ç»œæœç´¢ååˆ°ç­”æ¡ˆç”Ÿæˆ
    workflow.add_edge("web_search", "answer_generation")
  
    # ç­”æ¡ˆç”Ÿæˆåç»“æŸ
    workflow.add_edge("answer_generation", END)
  
    # ç¼–è¯‘å·¥ä½œæµï¼ˆå¸¦å†…å­˜ç®¡ç†ï¼‰
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
  
    return app

# åˆå§‹åŒ–å…¨å±€å·¥ä½œæµ
workflow_app = build_lightrag_workflow()
```

#### 4.2.3 å·¥ä½œæµæ‰§è¡Œæ¥å£

**åŒæ­¥å’Œå¼‚æ­¥æ‰§è¡Œæ¥å£**ï¼š

```python
import asyncio
from typing import Dict, Any

def execute_query_sync(user_query: str, session_id: str = None) -> Dict[str, Any]:
    """åŒæ­¥æ‰§è¡ŒæŸ¥è¯¢ï¼ˆç”¨äºStreamlitï¼‰"""
  
    if not session_id:
        import uuid
        session_id = str(uuid.uuid4())
  
    # åˆå§‹çŠ¶æ€
    initial_state = {
        "user_query": user_query,
        "session_id": session_id
    }
  
    # æ‰§è¡Œå·¥ä½œæµ
    config = {"configurable": {"thread_id": session_id}}
  
    try:
        # åŒæ­¥æ‰§è¡Œ
        final_result = workflow_app.invoke(initial_state, config=config)
      
        return {
            "success": True,
            "result": final_result,
            "session_id": session_id
        }
      
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "session_id": session_id
        }

async def execute_query_async(user_query: str, session_id: str = None) -> Dict[str, Any]:
    """å¼‚æ­¥æ‰§è¡ŒæŸ¥è¯¢ï¼ˆç”¨äºAPIæœåŠ¡ï¼‰"""
  
    if not session_id:
        import uuid
        session_id = str(uuid.uuid4())
  
    initial_state = {
        "user_query": user_query,
        "session_id": session_id
    }
  
    config = {"configurable": {"thread_id": session_id}}
  
    try:
        # å¼‚æ­¥æ‰§è¡Œ
        final_result = await workflow_app.ainvoke(initial_state, config=config)
      
        return {
            "success": True,
            "result": final_result,
            "session_id": session_id
        }
      
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "session_id": session_id
        }

def stream_query_execution(user_query: str, session_id: str = None):
    """æµå¼æ‰§è¡ŒæŸ¥è¯¢ï¼ˆç”¨äºå®æ—¶æ˜¾ç¤ºï¼‰"""
  
    if not session_id:
        import uuid
        session_id = str(uuid.uuid4())
  
    initial_state = {
        "user_query": user_query,
        "session_id": session_id
    }
  
    config = {"configurable": {"thread_id": session_id}}
  
    # æµå¼æ‰§è¡Œ
    for step in workflow_app.stream(initial_state, config=config):
        yield step
```

### 3.4 Streamlit å‰ç«¯å®ç°

#### 3.4.1 ä¸»ç•Œé¢è®¾è®¡

```python
import streamlit as st
import asyncio
from datetime import datetime

st.set_page_config(
    page_title="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ¤–",
    layout="wide"
)

    st.title("ğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.markdown("åŸºäº Agentic RAG + LightRAG çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("ç³»ç»Ÿé…ç½®")
  
    # æ£€ç´¢é…ç½®
    retrieval_mode = st.selectbox(
        "æ£€ç´¢æ¨¡å¼",
        ["hybrid", "local", "global"],
        index=0
    )
  
    confidence_threshold = st.slider(
        "ç½®ä¿¡åº¦é˜ˆå€¼",
        min_value=0.0,
        max_value=1.0,
        value=0.7
    )
  
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    st.header("ç³»ç»ŸçŠ¶æ€")
    if st.button("æ£€æŸ¥è¿æ¥çŠ¶æ€"):
        check_system_status()
```

#### 3.4.2 å®æ—¶æµå¼æ˜¾ç¤º

```python
@st.fragment(run_every=0.1)
def stream_response():
    """æµå¼æ˜¾ç¤ºå“åº”è¿‡ç¨‹"""
    if "current_stream" in st.session_state:
        stream = st.session_state.current_stream
  
        progress_container = st.container()
        response_container = st.container()
  
        with progress_container:
            col1, col2, col3 = st.columns(3)
      
            with col1:
                st.info("ğŸ“Š æŸ¥è¯¢åˆ†æ")
                if stream.get("query_analysis_done"):
                    st.success("âœ… å®Œæˆ")
                else:
                    st.warning("â³ å¤„ç†ä¸­...")
      
            with col2:
                st.info("ğŸ” çŸ¥è¯†æ£€ç´¢")
                if stream.get("retrieval_done"):
                    st.success(f"âœ… æ‰¾åˆ°{stream.get('result_count', 0)}æ¡ç»“æœ")
                else:
                    st.warning("â³ æœç´¢ä¸­...")
      
            with col3:
                st.info("ğŸ§  ç­”æ¡ˆç”Ÿæˆ")
                if stream.get("generation_done"):
                    st.success("âœ… å®Œæˆ")
                else:
                    st.warning("â³ ç”Ÿæˆä¸­...")
  
        with response_container:
            if stream.get("partial_answer"):
                st.write_stream(stream["partial_answer"])

async def process_query(query: str):
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
    session_id = st.session_state.get("session_id", str(uuid.uuid4()))
  
    # åˆå§‹åŒ–æµå¼çŠ¶æ€
    st.session_state.current_stream = {
        "query_analysis_done": False,
        "retrieval_done": False,
        "generation_done": False,
        "partial_answer": "",
        "result_count": 0
    }
  
    config = {"configurable": {"thread_id": session_id}}
  
    # æµå¼æ‰§è¡Œå·¥ä½œæµ
    async for event in app.astream(
        {"user_query": query, "session_id": session_id},
        config=config
    ):
        node_name = list(event.keys())[0]
        node_output = event[node_name]
  
        # æ›´æ–°è¿›åº¦çŠ¶æ€
        if node_name == "query_analysis":
            st.session_state.current_stream["query_analysis_done"] = True
        elif node_name in ["lightrag_retrieval"]:
            st.session_state.current_stream["retrieval_done"] = True
            st.session_state.current_stream["result_count"] = len(
                node_output.get("lightrag_results", [])
            )
        elif node_name == "answer_generation":
            st.session_state.current_stream["generation_done"] = True
            st.session_state.current_stream["partial_answer"] = node_output.get("final_answer", "")
```

#### 3.4.3 äº¤äº’å¼ç»„ä»¶

```python
def main():
    """ä¸»ç•Œé¢é€»è¾‘"""
  
    # æŸ¥è¯¢è¾“å…¥
    query = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
  
    if query:
        with st.chat_message("user"):
            st.write(query)
  
        with st.chat_message("assistant"):
            # ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ
            response = asyncio.run(process_query(query))
      
            # æ˜¾ç¤ºç­”æ¡ˆ
            st.write(response.get("final_answer", ""))
      
            # æ˜¾ç¤ºæ¥æºä¿¡æ¯
            with st.expander("ğŸ“š ä¿¡æ¯æ¥æº"):
                if response.get("local_results"):
                    st.subheader("æœ¬åœ°çŸ¥è¯†åº“")
                    for result in response["local_results"]:
                        st.write(f"- {result.get('content', '')[:200]}...")
          
                if response.get("web_results"):
                    st.subheader("ç½‘ç»œæœç´¢")
                    for result in response["web_results"]:
                        st.write(f"- [{result.get('title', '')}]({result.get('url', '')})")
      
            # æ˜¾ç¤ºçŸ¥è¯†å›¾è°±
            with st.expander("ğŸ•¸ï¸ ç›¸å…³å®ä½“å…³ç³»"):
                display_knowledge_graph(response.get("graph_entities", []))

def display_knowledge_graph(entities):
    """æ˜¾ç¤ºçŸ¥è¯†å›¾è°±å¯è§†åŒ–"""
    if entities:
        # æ„å»ºå›¾æ•°æ®
        nodes = []
        edges = []
  
        for entity in entities:
            nodes.append({
                "id": entity["uuid"],
                "label": entity["name"],
                "title": entity.get("summary", "")
            })
  
        # ä½¿ç”¨streamlit-agraphæ˜¾ç¤º
        try:
            from streamlit_agraph import agraph, Node, Edge, Config
      
            config = Config(width=600, height=400, directed=True)
            agraph(nodes=nodes, edges=edges, config=config)
        except ImportError:
            st.info("å®‰è£… streamlit-agraph ä»¥æŸ¥çœ‹å›¾è°±å¯è§†åŒ–")
            for entity in entities:
                st.write(f"- **{entity['name']}**: {entity.get('summary', '')}")

if __name__ == "__main__":
    main()
```

## 4. æ•°æ®åº“è®¾è®¡

### 4.1 Neo4j å›¾æ•°æ®åº“

#### 4.1.1 èŠ‚ç‚¹ç±»å‹

```cypher
// æ–‡æ¡£èŠ‚ç‚¹
CREATE CONSTRAINT document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE;

// å®ä½“èŠ‚ç‚¹  
CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE;

// é—®ç­”è®°å½•èŠ‚ç‚¹
CREATE CONSTRAINT qa_session_id IF NOT EXISTS FOR (q:QASession) REQUIRE q.id IS UNIQUE;
```

#### 4.1.2 å…³ç³»ç±»å‹

```cypher
// æ–‡æ¡£åŒ…å«å®ä½“
(:Document)-[:CONTAINS]->(:Entity)

// å®ä½“é—´å…³ç³»
(:Entity)-[:RELATES_TO]->(:Entity)

// é—®ç­”å¼•ç”¨å®ä½“
(:QASession)-[:REFERENCES]->(:Entity)

// æ–‡æ¡£å¼•ç”¨å…³ç³»
(:Document)-[:REFERENCES]->(:Document)
```

### 4.2 PostgreSQL å‘é‡å­˜å‚¨

#### 4.2.1 è¡¨ç»“æ„è®¾è®¡

```sql
-- æ–‡æ¡£å—è¡¨
CREATE TABLE IF NOT EXISTS document_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id VARCHAR(255) NOT NULL,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    content_vector vector(3072),  -- OpenAI text-embedding-3-large
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- å®ä½“å‘é‡è¡¨
CREATE TABLE IF NOT EXISTS entity_embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_id VARCHAR(255) NOT NULL,
    entity_name VARCHAR(500) NOT NULL,
    description TEXT,
    embedding vector(3072),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºå‘é‡ç´¢å¼•
CREATE INDEX IF NOT EXISTS document_chunks_vector_idx 
ON document_chunks USING ivfflat (content_vector vector_cosine_ops) 
WITH (lists = 100);

CREATE INDEX IF NOT EXISTS entity_embeddings_vector_idx 
ON entity_embeddings USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
```

## 5. Streamlitå‰ç«¯å®ç°

### 5.1 ä¸»ç•Œé¢è®¾è®¡

**ç”¨æˆ·ä½“éªŒè®¾è®¡åŸåˆ™**ï¼š

- **ç›´è§‚æ˜“ç”¨**ï¼šæ¸…æ™°çš„è¾“å…¥æ¡†å’Œç»“æœå±•ç¤º
- **å®æ—¶åé¦ˆ**ï¼šæµå¼æ˜¾ç¤ºå¤„ç†è¿‡ç¨‹å’Œç»“æœ
- **ä¿¡æ¯é€æ˜**ï¼šæ˜¾ç¤ºä¿¡æ¯æ¥æºå’Œç½®ä¿¡åº¦
- **å“åº”å¼å¸ƒå±€**ï¼šé€‚é…ä¸åŒå±å¹•å°ºå¯¸

```python
import streamlit as st
from src.core.workflow import build_agentic_rag_workflow
from src.utils.helpers import format_sources, calculate_response_time

def main():
    st.set_page_config(
        page_title="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
  
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.markdown("åŸºäº Agentic RAG + LightRAG çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹")
  
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("ç³»ç»Ÿé…ç½®")
  
        # ç½®ä¿¡åº¦é˜ˆå€¼è°ƒæ•´
        confidence_threshold = st.slider(
            "ç½®ä¿¡åº¦é˜ˆå€¼", 
            min_value=0.1, 
            max_value=1.0, 
            value=0.7, 
            step=0.1,
            help="ä½äºæ­¤é˜ˆå€¼å°†è§¦å‘ç½‘ç»œæœç´¢"
        )
  
        # æ£€ç´¢ç»“æœæ•°é‡
        max_results = st.slider(
            "æœ€å¤§æ£€ç´¢ç»“æœæ•°", 
            min_value=5, 
            max_value=20, 
            value=10
        )
  
        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        st.subheader("ç³»ç»ŸçŠ¶æ€")
        if check_system_health():
            st.success("âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        else:
            st.error("âŒ ç³»ç»Ÿè¿æ¥å¼‚å¸¸")
  
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns([2, 1])
  
    with col1:
        # æŸ¥è¯¢è¾“å…¥
        user_query = st.text_area(
            "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š",
            height=100,
            placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿå®ƒæœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ"
        )
  
        col_submit, col_clear = st.columns([1, 1])
        with col_submit:
            submit_button = st.button("ğŸ” å¼€å§‹æŸ¥è¯¢", type="primary")
        with col_clear:
            clear_button = st.button("ğŸ—‘ï¸ æ¸…ç©ºå†…å®¹")
  
    with col2:
        # æŸ¥è¯¢å†å²
        st.subheader("æŸ¥è¯¢å†å²")
        if "query_history" not in st.session_state:
            st.session_state.query_history = []
  
        for i, hist_query in enumerate(st.session_state.query_history[-5:]):
            if st.button(f"ğŸ“ {hist_query[:30]}...", key=f"hist_{i}"):
                user_query = hist_query
                st.rerun()
  
    # å¤„ç†æŸ¥è¯¢
    if submit_button and user_query:
        process_query(user_query, confidence_threshold, max_results)
  
    if clear_button:
        st.session_state.clear()
        st.rerun()

def process_query(query: str, confidence_threshold: float, max_results: int):
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶æµå¼æ˜¾ç¤ºç»“æœ"""
  
    # æ·»åŠ åˆ°å†å²è®°å½•
    if "query_history" not in st.session_state:
        st.session_state.query_history = []
    st.session_state.query_history.append(query)
  
    # åˆ›å»ºç»“æœå®¹å™¨
    with st.container():
        st.markdown("---")
        st.subheader("ğŸ”„ å¤„ç†è¿‡ç¨‹")
  
        # è¿›åº¦æŒ‡ç¤ºå™¨
        progress_bar = st.progress(0)
        status_text = st.empty()
  
        # èŠ‚ç‚¹æ‰§è¡ŒçŠ¶æ€æ˜¾ç¤º
        col1, col2, col3 = st.columns(3)
  
        with col1:
            analysis_status = st.empty()
        with col2:
            retrieval_status = st.empty()
        with col3:
            generation_status = st.empty()
  
        # è¯¦ç»†è¿‡ç¨‹å±•ç¤º
        process_expander = st.expander("ğŸ“Š è¯¦ç»†å¤„ç†è¿‡ç¨‹", expanded=True)
  
        # æ„å»ºå·¥ä½œæµ
        workflow = build_agentic_rag_workflow()
  
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "user_query": query,
            "confidence_threshold": confidence_threshold,
            "max_results": max_results
        }
  
        # æµå¼æ‰§è¡Œå·¥ä½œæµ
        step_count = 0
        total_steps = 7  # é¢„ä¼°æ­¥éª¤æ•°
  
        for step in workflow.stream(initial_state):
            step_count += 1
            progress_bar.progress(min(step_count / total_steps, 1.0))
      
            # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
            current_node = list(step.keys())[0]
            current_data = step[current_node]
      
            status_text.text(f"æ­£åœ¨æ‰§è¡Œï¼š{get_node_display_name(current_node)}")
      
            # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
            update_node_status(current_node, analysis_status, retrieval_status, generation_status)
      
            # åœ¨è¯¦ç»†è¿‡ç¨‹ä¸­æ˜¾ç¤º
            with process_expander:
                display_step_details(current_node, current_data)
  
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        display_final_results(step)

def get_node_display_name(node_name: str) -> str:
    """è·å–èŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°"""
    name_mapping = {
        "query_analysis": "æŸ¥è¯¢åˆ†æ",
        "lightrag_retrieval": "LightRAGæ£€ç´¢",
        "quality_assessment": "è´¨é‡è¯„ä¼°",
        "web_search": "ç½‘ç»œæœç´¢",
        "answer_generation": "ç­”æ¡ˆç”Ÿæˆ"
    }
    return name_mapping.get(node_name, node_name)

def update_node_status(current_node: str, analysis_status, retrieval_status, generation_status):
    """æ›´æ–°èŠ‚ç‚¹æ‰§è¡ŒçŠ¶æ€"""
  
    # é‡ç½®æ‰€æœ‰çŠ¶æ€
    analysis_status.markdown("âšª æŸ¥è¯¢åˆ†æ")
    retrieval_status.markdown("âšª ä¿¡æ¯æ£€ç´¢") 
    generation_status.markdown("âšª ç­”æ¡ˆç”Ÿæˆ")
  
    # æ›´æ–°å½“å‰æ‰§è¡ŒçŠ¶æ€
    if current_node == "query_analysis":
        analysis_status.markdown("ğŸ”µ æŸ¥è¯¢åˆ†æ (è¿›è¡Œä¸­)")
    elif current_node in ["lightrag_retrieval", "quality_assessment", "web_search"]:
        analysis_status.markdown("âœ… æŸ¥è¯¢åˆ†æ (å®Œæˆ)")
        retrieval_status.markdown("ğŸ”µ ä¿¡æ¯æ£€ç´¢ (è¿›è¡Œä¸­)")
    else:
        analysis_status.markdown("âœ… æŸ¥è¯¢åˆ†æ (å®Œæˆ)")
        retrieval_status.markdown("âœ… ä¿¡æ¯æ£€ç´¢ (å®Œæˆ)")
        generation_status.markdown("ğŸ”µ ç­”æ¡ˆç”Ÿæˆ (è¿›è¡Œä¸­)")

def display_step_details(node_name: str, data: dict):
    """æ˜¾ç¤ºæ­¥éª¤è¯¦ç»†ä¿¡æ¯"""
  
    with st.container():
        st.markdown(f"**{get_node_display_name(node_name)}**")
  
        if node_name == "query_analysis":
            st.json({
                "æŸ¥è¯¢ç±»å‹": data.get("query_type", ""),
                "å…³é”®å®ä½“": data.get("key_entities", []),
                "å¤„ç†åæŸ¥è¯¢": data.get("processed_query", "")
            })
      
        elif node_name == "quality_assessment":
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ç½®ä¿¡åº¦åˆ†æ•°", f"{data.get('confidence_score', 0):.2f}")
            with col2:
                need_search = data.get('need_web_search', False)
                st.metric("éœ€è¦ç½‘ç»œæœç´¢", "æ˜¯" if need_search else "å¦")
          
        elif node_name == "answer_generation":
            st.metric("ä¿¡æ¯æ¥æºæ•°é‡", data.get('context_used', 0))
      
        st.markdown("---")

def display_final_results(final_step: dict):
    """æ˜¾ç¤ºæœ€ç»ˆç»“æœ"""
  
    st.markdown("---")
    st.subheader("ğŸ’¡ æŸ¥è¯¢ç»“æœ")
  
    # è·å–æœ€ç»ˆæ•°æ®
    final_data = list(final_step.values())[0]
  
    # ä¸»è¦ç­”æ¡ˆ
    if "final_answer" in final_data:
        st.markdown("### ğŸ“ ç­”æ¡ˆ")
        st.markdown(final_data["final_answer"])
  
    # ä¿¡æ¯æ¥æº
    if "sources" in final_data:
        with st.expander("ğŸ“š ä¿¡æ¯æ¥æº", expanded=False):
            sources = final_data["sources"]
            for i, source in enumerate(sources, 1):
                if source["type"] == "local_knowledge":
                    st.markdown(f"**{i}. æœ¬åœ°çŸ¥è¯†åº“** (ç½®ä¿¡åº¦: {source.get('confidence', 0):.2f})")
                elif source["type"] == "knowledge_graph":
                    st.markdown(f"**{i}. çŸ¥è¯†å›¾è°±** (å®ä½“æ•°: {source.get('entities', 0)})")
                elif source["type"] == "web_search":
                    st.markdown(f"**{i}. ç½‘ç»œæœç´¢**: [{source['title']}]({source['url']})")
  
    # ç³»ç»ŸæŒ‡æ ‡
    col1, col2, col3 = st.columns(3)
  
    with col1:
        confidence = final_data.get("answer_confidence", 0)
        st.metric(
            "ç­”æ¡ˆç½®ä¿¡åº¦", 
            f"{confidence:.2f}",
            delta=f"{'é«˜' if confidence > 0.7 else 'ä¸­' if confidence > 0.5 else 'ä½'}"
        )
  
    with col2:
        source_count = len(final_data.get("sources", []))
        st.metric("ä¿¡æ¯æ¥æºæ•°", source_count)
  
    with col3:
        context_used = final_data.get("context_used", 0)
        st.metric("ä½¿ç”¨ä¿¡æ¯æº", context_used)

def check_system_health() -> bool:
    """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        # æ£€æŸ¥Neo4jè¿æ¥
        # æ£€æŸ¥PostgreSQLè¿æ¥  
        # æ£€æŸ¥APIå¯ç”¨æ€§
        return True
    except:
        return False

if __name__ == "__main__":
    main()
```

### 5.2 å®æ—¶æµå¼æ˜¾ç¤º

**LangGraphä¸Streamlité›†æˆçš„æ ¸å¿ƒæŠ€æœ¯**ï¼š

```python
def stream_workflow_execution(workflow, initial_state):
    """æµå¼æ‰§è¡Œå·¥ä½œæµå¹¶å®æ—¶æ›´æ–°ç•Œé¢"""
  
    # åˆ›å»ºæµå¼å®¹å™¨
    container = st.container()
  
    # çŠ¶æ€è·Ÿè¸ª
    execution_state = {
        "current_step": 0,
        "total_steps": 7,
        "step_details": {},
        "timeline": []
    }
  
    # é€æ­¥æ‰§è¡Œå¹¶æ›´æ–°
    for step_output in workflow.stream(initial_state):
        node_name = list(step_output.keys())[0]
        node_data = step_output[node_name]
  
        # æ›´æ–°æ‰§è¡ŒçŠ¶æ€
        execution_state["current_step"] += 1
        execution_state["step_details"][node_name] = node_data
        execution_state["timeline"].append({
            "node": node_name,
            "timestamp": datetime.now(),
            "status": "completed"
        })
  
        # å®æ—¶æ›´æ–°ç•Œé¢
        with container:
            update_progress_display(execution_state)
      
        # çŸ­æš‚åœé¡¿ä»¥ä¾¿ç”¨æˆ·è§‚å¯Ÿ
        time.sleep(0.5)
  
    return execution_state["step_details"]

@st.fragment
def update_progress_display(execution_state):
    """æ›´æ–°è¿›åº¦æ˜¾ç¤ºï¼ˆä½¿ç”¨Streamlit fragmentå®ç°å®æ—¶æ›´æ–°ï¼‰"""
  
    # è¿›åº¦æ¡
    progress = execution_state["current_step"] / execution_state["total_steps"]
    st.progress(progress)
  
    # æ—¶é—´çº¿æ˜¾ç¤º
    for timeline_item in execution_state["timeline"]:
        node_name = timeline_item["node"]
        timestamp = timeline_item["timestamp"]
  
        col1, col2, col3 = st.columns([2, 3, 2])
  
        with col1:
            st.markdown(f"âœ… {get_node_display_name(node_name)}")
        with col2:
            if node_name in execution_state["step_details"]:
                show_node_summary(execution_state["step_details"][node_name])
        with col3:
            st.caption(timestamp.strftime("%H:%M:%S"))

def show_node_summary(node_data):
    """æ˜¾ç¤ºèŠ‚ç‚¹æ‰§è¡Œæ‘˜è¦"""
  
    if "confidence_score" in node_data:
        st.caption(f"ç½®ä¿¡åº¦: {node_data['confidence_score']:.2f}")
    elif "local_results" in node_data:
        st.caption("æ£€ç´¢å®Œæˆ")
    elif "final_answer" in node_data:
        st.caption("ç­”æ¡ˆå·²ç”Ÿæˆ")
    else:
        st.caption("å¤„ç†å®Œæˆ")
```

## 6. å¼€å‘æŒ‡å—

### 6.1 MVPé¡¹ç›®ç»“æ„ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

```
intelligent-qa-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                    # LangGraphèŠ‚ç‚¹å®ç°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ query_analysis.py      # æŸ¥è¯¢åˆ†æèŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ lightrag_retrieval.py  # LightRAGæ£€ç´¢èŠ‚ç‚¹(æ”¯æŒä¸‰ç§æ¨¡å¼)
â”‚   â”‚   â”œâ”€â”€ quality_assessment.py  # è´¨é‡è¯„ä¼°èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ web_search.py          # ç½‘ç»œæœç´¢èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ answer_generation.py   # ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹
â”‚   â”œâ”€â”€ core/                      # æ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ workflow.py            # LangGraphå·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ state.py               # çŠ¶æ€å®šä¹‰
â”‚   â”‚   â””â”€â”€ config.py              # ç»Ÿä¸€é…ç½®ç®¡ç†ï¼ˆè¯»å–.envæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_processor.py  # æ–‡æ¡£å¤„ç†
â”‚   â”‚   â””â”€â”€ helpers.py             # è¾…åŠ©å‡½æ•°
â”‚   â””â”€â”€ streamlit_app.py           # Streamlitå‰ç«¯ä¸»æ–‡ä»¶
â”œâ”€â”€ docs/                          # çŸ¥è¯†åº“æ–‡æ¡£ç›®å½•ï¼ˆå­˜æ”¾æ‰€æœ‰æ–‡æ¡£ï¼‰
â”œâ”€â”€ rag_storage/                   # LightRAGå­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ kv_storage/               # é”®å€¼å­˜å‚¨
â”‚   â”œâ”€â”€ vector_storage/           # å‘é‡å­˜å‚¨
â”‚   â””â”€â”€ graph_storage/            # å›¾å­˜å‚¨
â”œâ”€â”€ tests/                         # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ test_agents/
â”‚   â”œâ”€â”€ test_core/
â”‚   â””â”€â”€ test_integration/
â”œâ”€â”€ scripts/                       # è„šæœ¬å·¥å…·
â”‚   â”œâ”€â”€ setup_environment.py      # ç¯å¢ƒåˆå§‹åŒ–
â”‚   â”œâ”€â”€ ingest_documents.py       # æ–‡æ¡£å¯¼å…¥
â”‚   â”œâ”€â”€ backup_data.py            # æ•°æ®å¤‡ä»½
â”‚   â””â”€â”€ health_check.py           # ç³»ç»Ÿå¥åº·æ£€æŸ¥
â”œâ”€â”€ .env                          # ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ•æ„Ÿä¿¡æ¯ï¼‰
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–
â”œâ”€â”€ setup.py                      # å®‰è£…è„šæœ¬
â””â”€â”€ README.md                     # é¡¹ç›®è¯´æ˜
```

### 6.2 æ··åˆéƒ¨ç½²å¯åŠ¨æµç¨‹

#### 6.2.1 åˆå§‹åŒ–æœ¬åœ°ç¯å¢ƒ

```bash
# 1. åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir intelligent-qa-system
cd intelligent-qa-system

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv qa_env
source qa_env/bin/activate  # Linux/macOS
# qa_env\Scripts\activate   # Windows

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
# - OPENAI_API_KEY=your_openai_api_key
# - TAVILY_API_KEY=your_tavily_api_key
# - POSTGRES_HOST=117.72.54.192
# - POSTGRES_DB=searchforrag
# - POSTGRES_USER=searchforrag 
# - POSTGRES_PASSWORD=searchforrag
# - NEO4J_PASSWORD=your_neo4j_password
```

#### 6.2.2 å¯åŠ¨Neo4jæœ¬åœ°æœåŠ¡

```bash
# æ–¹å¼1ï¼šNeo4j Desktop
# - æ‰“å¼€Neo4j Desktop
# - å¯åŠ¨"qa-knowledge-graph"æ•°æ®åº“
# - ç¡®è®¤çŠ¶æ€æ˜¾ç¤ºä¸º"Active"

# æ–¹å¼2ï¼šå‘½ä»¤è¡Œå¯åŠ¨
sudo systemctl start neo4j  # Linux
# æˆ–è€…é€šè¿‡æœåŠ¡ç®¡ç†å™¨å¯åŠ¨ Windows

# éªŒè¯Neo4jå¯åŠ¨
# æµè§ˆå™¨è®¿é—®ï¼šhttp://localhost:7474
# ç”¨æˆ·åï¼šneo4jï¼Œå¯†ç ï¼šä½ è®¾ç½®çš„å¯†ç 
```

#### 6.2.3 æµ‹è¯•æ•°æ®åº“è¿æ¥

```bash
# 5. æµ‹è¯•æ•°æ®åº“è¿æ¥
python scripts/test_connections.py
# ç¡®ä¿è¾“å‡ºæ˜¾ç¤ºï¼š
# âœ… PostgreSQLè¿æ¥æˆåŠŸ
# âœ… Neo4jè¿æ¥æˆåŠŸ
```

#### 6.2.4 åˆå§‹åŒ–ç³»ç»Ÿå’Œå¯¼å…¥æ•°æ®

```bash
# 6. åˆå§‹åŒ–LightRAGç¯å¢ƒ
python scripts/setup_environment.py

# 7. å¯¼å…¥çŸ¥è¯†åº“æ–‡æ¡£
python scripts/ingest_documents.py --path ./docs

# 8. ç³»ç»Ÿå¥åº·æ£€æŸ¥
python scripts/health_check.py
```

#### 6.2.5 å¯åŠ¨åº”ç”¨

```bash
# 9. å¯åŠ¨Streamlitåº”ç”¨
streamlit run src/streamlit_app.py

# 10. è®¿é—®åº”ç”¨
# æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:8501
```

#### 6.2.6 è®¿é—®ç®¡ç†ç•Œé¢

**åº”ç”¨è®¿é—®åœ°å€**ï¼š

- **ä¸»åº”ç”¨ç•Œé¢**ï¼šhttp://localhost:8501
- **Neo4jæ•°æ®åº“ç®¡ç†**ï¼šhttp://localhost:7474
- **PostgreSQLç®¡ç†**ï¼šé€šè¿‡å®å¡”é¢æ¿æˆ–phpPgAdmin

**å¸¸ç”¨ç®¡ç†æ“ä½œ**ï¼š

```bash
# æŸ¥çœ‹Streamlitæ—¥å¿—
tail -f logs/streamlit.log

# é‡å¯åº”ç”¨
Ctrl+C  # åœæ­¢åº”ç”¨
streamlit run src/streamlit_app.py  # é‡æ–°å¯åŠ¨

# æ£€æŸ¥Neo4jçŠ¶æ€
# è®¿é—®ï¼šhttp://localhost:7474/browser/
# è¿è¡ŒæŸ¥è¯¢ï¼šMATCH (n) RETURN count(n) as node_count

# æ£€æŸ¥PostgreSQLè¿æ¥
python scripts/test_connections.py
```

### 6.3 ç®€åŒ–é…ç½®ç®¡ç†

#### 6.3.1 é…ç½®æ–‡ä»¶æ¶æ„

**é…ç½®åˆ†ç¦»åŸåˆ™**ï¼š

- `.env` æ–‡ä»¶ï¼šå­˜æ”¾æ•æ„Ÿä¿¡æ¯ï¼ˆAPIå¯†é’¥ã€æ•°æ®åº“å¯†ç ï¼‰
- `config.py` æ–‡ä»¶ï¼šè¯»å–.envå¹¶ç®¡ç†æ‰€æœ‰åº”ç”¨é…ç½®

#### 6.3.2 ç»Ÿä¸€é…ç½®ç®¡ç† (src/core/config.py)

```python
import os
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class Config:
    """ç»Ÿä¸€é…ç½®ç®¡ç†ç±»"""
  
    # ç³»ç»ŸåŸºæœ¬é…ç½®
    SYSTEM_NAME = "æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
    VERSION = "1.0.0"
    DEBUG = os.getenv("DEBUG", "false").lower() == "true"
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
  
    # APIé…ç½®ï¼ˆä».envè¯»å–æ•æ„Ÿä¿¡æ¯ï¼‰
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4")
    TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
  
    # LLMé…ç½®
    LLM_TEMPERATURE = 0.1
    LLM_MAX_TOKENS = 2000
  
    # æ•°æ®åº“é…ç½®ï¼ˆä».envè¯»å–è¿æ¥ä¿¡æ¯ï¼‰
    # PostgreSQL
    POSTGRES_HOST = os.getenv("POSTGRES_HOST", "117.72.54.192")
    POSTGRES_PORT = int(os.getenv("POSTGRES_PORT", "5432"))
    POSTGRES_DB = os.getenv("POSTGRES_DB", "searchforrag")
    POSTGRES_USER = os.getenv("POSTGRES_USER", "searchforrag")
    POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "searchforrag")
    POSTGRES_SSL_MODE = os.getenv("POSTGRES_SSL_MODE", "prefer")
  
    # Neo4j
    NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    NEO4J_USERNAME = os.getenv("NEO4J_USERNAME", "neo4j")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password")
    NEO4J_DATABASE = os.getenv("NEO4J_DATABASE", "neo4j")
  
    # LightRAGé…ç½®
    RAG_STORAGE_DIR = Path(os.getenv("RAG_STORAGE_DIR", "./rag_storage"))
    DOCS_DIR = Path(os.getenv("DOCS_DIR", "./docs"))
    CHUNK_SIZE = 1200
    CHUNK_OVERLAP = 100
  
    # æ£€ç´¢é…ç½®
    CONFIDENCE_THRESHOLD = float(os.getenv("CONFIDENCE_THRESHOLD", "0.7"))
    MAX_LOCAL_RESULTS = 10
    MAX_WEB_RESULTS = 5
    VECTOR_SIMILARITY_THRESHOLD = 0.75
  
    # ç½‘ç»œæœç´¢é…ç½®
    WEB_SEARCH_TIMEOUT = 30
    WEB_SEARCH_MAX_RETRIES = 3
  
    # Streamlité…ç½®
    STREAMLIT_HOST = os.getenv("STREAMLIT_HOST", "localhost")
    STREAMLIT_PORT = int(os.getenv("STREAMLIT_PORT", "8501"))
    STREAMLIT_THEME = "light"
    SHOW_DETAILS = True
  
    @property
    def postgres_url(self):
        """æ„å»ºPostgreSQLè¿æ¥URL"""
        return f"postgresql://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_HOST}:{self.POSTGRES_PORT}/{self.POSTGRES_DB}?sslmode={self.POSTGRES_SSL_MODE}"
  
    @property
    def neo4j_config(self):
        """è·å–Neo4jè¿æ¥é…ç½®"""
        return {
            "uri": self.NEO4J_URI,
            "auth": (self.NEO4J_USERNAME, self.NEO4J_PASSWORD),
            "database": self.NEO4J_DATABASE
        }

# å…¨å±€é…ç½®å®ä¾‹
config = Config()
```

#### 6.3.3 é…ç½®ä½¿ç”¨ç¤ºä¾‹

```python
# åœ¨å…¶ä»–æ¨¡å—ä¸­ä½¿ç”¨é…ç½®
from src.core.config import config

# ä½¿ç”¨APIé…ç½®
openai_client = OpenAI(api_key=config.OPENAI_API_KEY)

# ä½¿ç”¨æ•°æ®åº“é…ç½®
conn = psycopg2.connect(config.postgres_url)

# ä½¿ç”¨Neo4jé…ç½®
driver = GraphDatabase.driver(**config.neo4j_config)

# ä½¿ç”¨åº”ç”¨é…ç½®
if config.DEBUG:
    print("è°ƒè¯•æ¨¡å¼å¼€å¯")
```

#### 6.3.4 æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ (scripts/setup_environment.py)

```python
import psycopg2
from neo4j import GraphDatabase
from pathlib import Path
import sys

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('src')
from core.config import config

def setup_postgresql():
    """åˆå§‹åŒ–PostgreSQLæ•°æ®åº“"""
    try:
        conn = psycopg2.connect(config.postgres_url)
        cursor = conn.cursor()
    
        # åˆ›å»ºpgvectoræ‰©å±•
        cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")
    
        # åˆ›å»ºå‘é‡è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS document_embeddings (
                id SERIAL PRIMARY KEY,
                document_id VARCHAR(255) NOT NULL,
                chunk_id VARCHAR(255) NOT NULL,
                content TEXT NOT NULL,
                embedding vector(1536),  -- OpenAI embeddingç»´åº¦
                metadata JSONB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
    
        # åˆ›å»ºç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_document_embeddings_vector 
            ON document_embeddings USING ivfflat (embedding vector_cosine_ops);
        """)
    
        conn.commit()
        cursor.close()
        conn.close()
    
        print("âœ… PostgreSQLæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ PostgreSQLåˆå§‹åŒ–å¤±è´¥ï¼š{e}")

def setup_neo4j():
    """åˆå§‹åŒ–Neo4jæ•°æ®åº“"""
    try:
        driver = GraphDatabase.driver(**config.neo4j_config)
    
        with driver.session() as session:
            # åˆ›å»ºçº¦æŸå’Œç´¢å¼•
            session.run("CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE")
            session.run("CREATE CONSTRAINT relationship_id IF NOT EXISTS FOR (r:Relationship) REQUIRE r.id IS UNIQUE")
            session.run("CREATE INDEX entity_name IF NOT EXISTS FOR (e:Entity) ON (e.name)")
            session.run("CREATE INDEX relationship_type IF NOT EXISTS FOR (r:Relationship) ON (r.type)")
    
        driver.close()
        print("âœ… Neo4jæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ Neo4jåˆå§‹åŒ–å¤±è´¥ï¼š{e}")

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    config.RAG_STORAGE_DIR.mkdir(parents=True, exist_ok=True)
    config.DOCS_DIR.mkdir(parents=True, exist_ok=True)
  
    # åˆ›å»ºå­ç›®å½•
    (config.RAG_STORAGE_DIR / "kv_storage").mkdir(exist_ok=True)
    (config.RAG_STORAGE_DIR / "vector_storage").mkdir(exist_ok=True)
    (config.RAG_STORAGE_DIR / "graph_storage").mkdir(exist_ok=True)
  
    print("âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–ç¯å¢ƒ...")
  
    setup_directories()
    setup_postgresql()
    setup_neo4j()
  
    print("ğŸ‰ ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    main()
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.1
**æœ€åæ›´æ–°**: 2024-01-15
**æŠ€æœ¯æ ˆ**: LightRAG + LangGraph + Neo4j + PostgreSQL + Streamlit
**éƒ¨ç½²æ–¹å¼**: æ··åˆéƒ¨ç½²ï¼ˆæœ¬åœ°+è¿œç¨‹ï¼‰
